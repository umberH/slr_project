"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"JSRJ3JQX","webpage","","","Detecting Covariate Drift with Explanations | SpringerLink","","","","","https://link.springer.com/chapter/10.1007/978-3-030-88483-3_24","","","2023-05-24 14:13:03","2023-05-24 14:23:12","2023-05-24 14:13:03","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\9C2ZGZYG\978-3-030-88483-3_24.html; ","notion://www.notion.so/Detecting-Covariate-Drift-with-Explanations-SpringerLink-n-d-519b4c816f414359952f948ef3ecfb45","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2MQW6Z8D","webpage","","","</> Keyword Surfer - Content","","","","","https://surferseo.com/iframe/keyword-surfer-body/","","","2023-05-24 14:08:32","2023-05-24 14:23:02","2023-05-24 14:08:32","","","","","","","","","","","","","","en","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\D3CB445S\keyword-surfer-body.html","notion://www.notion.so/cite-noauthor__nodate-0c2b6acde72640b99e1045c2bc0b1663","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZNHCCZDX","journalArticle","1995","Andrews, Robert; Diederich, Joachim; Tickle, Alan B.","Survey and critique of techniques for extracting rules from trained artificial neural networks","Knowledge-Based Systems","","0950-7051","10.1016/0950-7051(96)81920-4","https://www.sciencedirect.com/science/article/pii/0950705196819204","It is becoming increasingly apparent that, without some form of explanation capability, the full potential of trained artificial neural networks (ANNs) may not be realised. This survey gives an overview of techniques developed to redress this situation. Specifically, the survey focuses on mechanisms, procedures, and algorithms designed to insert knowledge into ANNs (knowledge initialisation), extract rules from trained ANNs (rule extraction), and utilise ANNs to refine existing rule bases (rule refinement). The survey also introduces a new taxonomy for classifying the various techniques, discusses their modus operandi, and delineates criteria for evaluating their efficacy.","1995-12-01","2023-04-07 22:31:38","2023-05-24 14:22:57","2023-04-07 22:31:38","373-389","","6","8","","Knowledge-Based Systems","","Knowledge-based neural networks","","","","","","en","","","","","ScienceDirect","","726 citations (Crossref) [2023-04-08]","","C:\Users\ambreen.hanif\Zotero\storage\B49K6PUG\Andrews et al_1995_Survey and critique of techniques for extracting rules from trained artificial.pdf; ; C:\Users\ambreen.hanif\Zotero\storage\ZK96UF8I\0950705196819204.html","notion://www.notion.so/cite-andrews_survey_1995-1f37a41bf169484b95c1a33dcaa9eb75","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"995Y7E2A","preprint","2019","Arik, Sercan O.; Pfister, Tomas","ProtoAttend: Attention-Based Prototypical Learning","","","","10.48550/arXiv.1902.06292","http://arxiv.org/abs/1902.06292","We propose a novel inherently interpretable machine learning method that bases decisions on few relevant examples that we call prototypes. Our method, ProtoAttend, can be integrated into a wide range of neural network architectures including pre-trained models. It utilizes an attention mechanism that relates the encoded representations to samples in order to determine prototypes. The resulting model outperforms state of the art in three high impact problems without sacrificing accuracy of the original model: (1) it enables high-quality interpretability that outputs samples most relevant to the decision-making (i.e. a sample-based interpretability method); (2) it achieves state of the art confidence estimation by quantifying the mismatch across prototype labels; and (3) it obtains state of the art in distribution mismatch detection. All this can be achieved with minimal additional test time and a practically viable training time computational cost.","2019-09-25","2022-08-18 01:42:00","2023-05-24 14:22:55","2022-08-18 01:42:00","","","","","","","ProtoAttend","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1902.06292 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\MK8FHTZT\Arik_Pfister_2019_ProtoAttend.pdf; C:\Users\ambreen.hanif\Zotero\storage\6X472DY9\1902.html; ","notion://www.notion.so/cite-arik_protoattend_2019-7dab68ca681f47cc8859fcd2fd13e6ef","notion","","","","","","","","","","","","","","","","","","","","arXiv:1902.06292","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9A7JYJD9","conferencePaper","2014","Zeiler, Matthew D.; Fergus, Rob","Visualizing and Understanding Convolutional Networks","Computer Vision – ECCV 2014","978-3-319-10590-1","","10.1007/978-3-319-10590-1_53","","Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.","2014","2023-04-10 23:20:20","2023-05-24 14:22:53","","818-833","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","3991 citations (Crossref) [2023-04-11]","","; C:\Users\ambreen.hanif\Zotero\storage\3E8L2IWZ\Zeiler_Fergus_2014_Visualizing and Understanding Convolutional Networks.pdf","notion://www.notion.so/cite-zeiler_visualizing_2014-562b6ae5746a4a78b66b51ac5b4e33c7","notion","","Fleet, David; Pajdla, Tomas; Schiele, Bernt; Tuytelaars, Tinne","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZJIAEQWK","journalArticle","2022","Zhao, Zhenge; Xu, Panpan; Scheidegger, Carlos; Ren, Liu","Human-in-the-loop Extraction of Interpretable Concepts in Deep Learning Models","IEEE Transactions on Visualization and Computer Graphics","","1941-0506","10.1109/TVCG.2021.3114837","","The interpretation of deep neural networks (DNNs) has become a key topic as more and more people apply them to solve various problems and making critical decisions. Concept-based explanations have recently become a popular approach for post-hoc interpretation of DNNs. However, identifying human-understandable visual concepts that affect model decisions is a challenging task that is not easily addressed with automatic approaches. We present a novel human-in-the-Ioop approach to generate user-defined concepts for model interpretation and diagnostics. Central to our proposal is the use of active learning, where human knowledge and feedback are combined to train a concept extractor with very little human labeling effort. We integrate this process into an interactive system, ConceptExtract. Through two case studies, we show how our approach helps analyze model behavior and extract human-friendly concepts for different machine learning tasks and datasets and how to use these concepts to understand the predictions, compare model performance and make suggestions for model refinement. Quantitative experiments show that our active learning approach can accurately extract meaningful visual concepts. More importantly, by identifying visual concepts that negatively affect model performance, we develop the corresponding data augmentation strategy that consistently improves model performance.","2022-01","2023-05-22 12:50:20","2023-05-24 14:22:50","","780-790","","1","28","","","","","","","","","","","","","","","IEEE Xplore","","8 citations (Crossref) [2023-05-23] Conference Name: IEEE Transactions on Visualization and Computer Graphics","","C:\Users\ambreen.hanif\Zotero\storage\CUI37AB4\9552218.html; ; C:\Users\ambreen.hanif\Zotero\storage\AYTA8VJI\Zhao et al. - 2022 - Human-in-the-loop Extraction of Interpretable Conc.pdf","notion://www.notion.so/cite-zhao_human-loop_2022-9cd3ca09ddb34066aaffe00904dd01e1","notion","Analytical models; Computational modeling; Data models; Deep learning; Deep Neural Network; Explainable AI; Model Interpretation; Predictive models; Task analysis; Visual Data Exploration; Visualization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UXG7LIEB","preprint","2018","Bellamy, Rachel K. E.; Dey, Kuntal; Hind, Michael; Hoffman, Samuel C.; Houde, Stephanie; Kannan, Kalapriya; Lohia, Pranay; Martino, Jacquelyn; Mehta, Sameep; Mojsilovic, Aleksandra; Nagar, Seema; Ramamurthy, Karthikeyan Natesan; Richards, John; Saha, Diptikalyan; Sattigeri, Prasanna; Singh, Moninder; Varshney, Kush R.; Zhang, Yunfeng","AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias","","","","10.48550/arXiv.1810.01943","http://arxiv.org/abs/1810.01943","Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license {https://github.com/ibm/aif360). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms. The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience (https://aif360.mybluemix.net) that provides a gentle introduction to the concepts and capabilities for line-of-business users, as well as extensive documentation, usage guidance, and industry-specific tutorials to enable data scientists and practitioners to incorporate the most appropriate tool for their problem into their work products. The architecture of the package has been engineered to conform to a standard paradigm used in data science, thereby further improving usability for practitioners. Such architectural design and abstractions enable researchers and developers to extend the toolkit with their new algorithms and improvements, and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.","2018-10-03","2023-04-03 04:31:01","2023-05-24 14:22:48","2023-04-03 04:31:01","","","","","","","AI Fairness 360","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1810.01943 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\J7FAZ6KM\1810.html; C:\Users\ambreen.hanif\Zotero\storage\QZQDZ3PA\Bellamy et al_2018_AI Fairness 360.pdf; ","notion://www.notion.so/cite-bellamy_ai_2018-72f99a6cc507485c860a37d52a964d8b","notion","","","","","","","","","","","","","","","","","","","","arXiv:1810.01943","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"59R8J73U","preprint","2016","Binder, Alexander; Montavon, Grégoire; Bach, Sebastian; Müller, Klaus-Robert; Samek, Wojciech","Layer-wise Relevance Propagation for Neural Networks with Local Renormalization Layers","","","","10.48550/arXiv.1604.00825","http://arxiv.org/abs/1604.00825","Layer-wise relevance propagation is a framework which allows to decompose the prediction of a deep neural network computed over a sample, e.g. an image, down to relevance scores for the single input dimensions of the sample such as subpixels of an image. While this approach can be applied directly to generalized linear mappings, product type non-linearities are not covered. This paper proposes an approach to extend layer-wise relevance propagation to neural networks with local renormalization layers, which is a very common product-type non-linearity in convolutional neural networks. We evaluate the proposed method for local renormalization layers on the CIFAR-10, Imagenet and MIT Places datasets.","2016-04-04","2022-08-18 02:36:28","2023-05-24 14:22:41","2022-08-18 02:36:28","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1604.00825 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\NSPQTQML\1604.html; C:\Users\ambreen.hanif\Zotero\storage\X5RVJ7EG\Binder et al_2016_Layer-wise Relevance Propagation for Neural Networks with Local Renormalization.pdf; ","notion://www.notion.so/cite-binder_layer-wise_2016-57d473932d604cd5865780701e3f7ba1","notion","","","","","","","","","","","","","","","","","","","","arXiv:1604.00825","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PSW86ZBQ","journalArticle","2019","Spinner, Thilo; Schlegel, Udo; Schäfer, Hanna; El-Assady, Mennatallah","explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning","IEEE Transactions on Visualization and Computer Graphics","","1077-2626, 1941-0506, 2160-9306","10.1109/TVCG.2019.2934629","http://arxiv.org/abs/1908.00087","We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.","2019","2023-04-04 00:00:29","2023-05-24 14:22:38","2023-04-04 00:00:29","1-1","","","","","IEEE Trans. Visual. Comput. Graphics","explAIner","","","","","","","","","","","","arXiv.org","","51 citations (Crossref) [2023-04-04] arXiv:1908.00087 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\M6MRIGJ4\1908.html; ; C:\Users\ambreen.hanif\Zotero\storage\VTT9QBXF\Spinner et al_2019_explAIner.pdf","notion://www.notion.so/cite-spinner_explainer_2019-cf7bae69bd9c477f8981b38c8e904228","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FS6GAX69","preprint","2018","Sugimura, Peter; Hartl, Florian","Building a Reproducible Machine Learning Pipeline","","","","10.48550/arXiv.1810.04570","http://arxiv.org/abs/1810.04570","Reproducibility of modeling is a problem that exists for any machine learning practitioner, whether in industry or academia. The consequences of an irreproducible model can include significant financial costs, lost time, and even loss of personal reputation (if results prove unable to be replicated). This paper will first discuss the problems we have encountered while building a variety of machine learning models, and subsequently describe the framework we built to tackle the problem of model reproducibility. The framework is comprised of four main components (data, feature, scoring, and evaluation layers), which are themselves comprised of well defined transformations. This enables us to not only exactly replicate a model, but also to reuse the transformations across different models. As a result, the platform has dramatically increased the speed of both offline and online experimentation while also ensuring model reproducibility.","2018-10-09","2023-04-27 04:27:55","2023-05-24 14:22:36","2023-04-27 04:27:55","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1810.04570 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\GDGNEPNF\1810.html; ; C:\Users\ambreen.hanif\Zotero\storage\QD69H2FR\Sugimura_Hartl_2018_Building a Reproducible Machine Learning Pipeline.pdf","notion://www.notion.so/cite-sugimura_building_2018-119461477f6341b4b86c40ff41414113","notion","","","","","","","","","","","","","","","","","","","","arXiv:1810.04570","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JQDL5MJY","journalArticle","","Mueller, Shane T; Hoffman, Robert R; Clancey, William; Emrey, Abigail","Explanation in Human-AI Systems: A Literature Meta-Review Synopsis of Key Ideas and Publications and Bibliography for Explainable AI","","","","","","","","2023-04-06 20:14:14","2023-05-24 14:22:34","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\Y829FWPV\Mueller et al. - Explanation in Human-AI Systems A Literature Meta.pdf; ","notion://www.notion.so/cite-mueller_explanation_nodate-f132aee58f5140a9a1eaa2c703e2cf97","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XID7DU5K","webpage","","","Trusted-AI/AIF360: A comprehensive set of fairness metrics for datasets and machine learning models, explanations for these metrics, and algorithms to mitigate bias in datasets and models.","","","","","https://github.com/Trusted-AI/AIF360","","","2023-04-03 04:09:26","2023-05-24 14:02:11","2023-04-03 04:09:26","","","","","","","","","","","","","","","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\7FCDIJTZ\AIF360.html","notion://www.notion.so/cite-noauthor_trusted-aiaif360_nodate-7a33c15d1e2043c7858b79452da8464d","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4DS2YESR","webpage","","","Word2Vec | Natural Language Engineering | Cambridge Core","","","","","https://www.cambridge.org/core/journals/natural-language-engineering/article/word2vec/B84AE4446BD47F48847B4904F0B36E0B","","","2021-02-05 02:00:05","2023-05-24 14:02:10","2021-02-05 02:00:04","","","","","","","","","","","","","","","","","","","","","","","","notion://www.notion.so/cite-noauthor_word2vec_nodate-49a9338fb14147098cebe7e540993d69","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CNKR9GTF","preprint","2022","Fel, Thomas; Hervier, Lucas; Vigouroux, David; Poche, Antonin; Plakoo, Justin; Cadene, Remi; Chalvidal, Mathieu; Colin, Julien; Boissin, Thibaut; Bethune, Louis; Picard, Agustin; Nicodeme, Claire; Gardes, Laurent; Flandin, Gregory; Serre, Thomas","Xplique: A Deep Learning Explainability Toolbox","","","","10.48550/arXiv.2206.04394","http://arxiv.org/abs/2206.04394","Today's most advanced machine-learning models are hardly scrutable. The key challenge for explainability methods is to help assisting researchers in opening up these black boxes, by revealing the strategy that led to a given decision, by characterizing their internal states or by studying the underlying data representation. To address this challenge, we have developed Xplique: a software library for explainability which includes representative explainability methods as well as associated evaluation metrics. It interfaces with one of the most popular learning libraries: Tensorflow as well as other libraries including PyTorch, scikit-learn and Theano. The code is licensed under the MIT license and is freely available at github.com/deel-ai/xplique.","2022-06-09","2023-05-22 23:08:31","2023-05-24 14:02:09","2023-05-22 23:08:30","","","","","","","Xplique","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2206.04394 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\JZXJNW49\2206.html; C:\Users\ambreen.hanif\Zotero\storage\7CAR5LNU\Fel et al_2022_Xplique.pdf; ","notion://www.notion.so/cite-fel_xplique_2022-68d250940d104079893f67207c3a935b","notion","","","","","","","","","","","","","","","","","","","","arXiv:2206.04394","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2CLJI5TM","webpage","2020","","XAI for Graphs: Explaining Graph Neural Network Predictions by Identifying Relevant Walks","DeepAI","","","","https://deepai.org/publication/xai-for-graphs-explaining-graph-neural-network-predictions-by-identifying-relevant-walks","06/05/20 - Graph Neural Networks (GNNs) are a popular approach for predicting graph structured data. As GNNs tightly entangle the input graph...","2020-06-05","2022-08-25 06:00:37","2023-05-24 14:02:09","2022-08-25 06:00:37","","","","","","","XAI for Graphs","","","","","","","","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\ZEMZUSWM\xai-for-graphs-explaining-graph-neural-network-predictions-by-identifying-relevant-walks.html","notion://www.notion.so/cite-noauthor_xai_2020-21a4488115694552819d0a9eda9c2e7e","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LXX8G9UL","conferencePaper","2018","Ahmad, Muhammad Aurangzeb; Eckert, Carly; Teredesai, Ankur","Interpretable Machine Learning in Healthcare","Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics","978-1-4503-5794-4","","10.1145/3233547.3233667","https://dl.acm.org/doi/10.1145/3233547.3233667","This tutorial extensively covers the definitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare.","2018-08-15","2023-04-06 06:06:35","2023-05-24 14:02:08","2023-04-05","559–560","","","","","","","BCB '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","131 citations (Crossref) [2023-04-06]","","C:\Users\ambreen.hanif\Zotero\storage\RZF6AQRF\Ahmad et al_2018_Interpretable Machine Learning in Healthcare.pdf; ","notion://www.notion.so/Ahmad-et-al-2018-f1eeda430b834b04bcf670acaa35f4b8","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WFA35PD8","conferencePaper","2018","Abdul, Ashraf; Vermeulen, Jo; Wang, Danding; Lim, Brian Y.; Kankanhalli, Mohan","Trends and Trajectories for Explainable, Accountable and Intelligible Systems: An HCI Research Agenda","Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems","978-1-4503-5620-6","","10.1145/3173574.3174156","https://dl.acm.org/doi/10.1145/3173574.3174156","Advances in artificial intelligence, sensors and big data management have far-reaching societal impacts. As these systems augment our everyday lives, it becomes increasing-ly important for people to understand them and remain in control. We investigate how HCI researchers can help to develop accountable systems by performing a literature analysis of 289 core papers on explanations and explaina-ble systems, as well as 12,412 citing papers. Using topic modeling, co-occurrence and network analysis, we mapped the research space from diverse domains, such as algorith-mic accountability, interpretable machine learning, context-awareness, cognitive psychology, and software learnability. We reveal fading and burgeoning trends in explainable systems, and identify domains that are closely connected or mostly isolated. The time is ripe for the HCI community to ensure that the powerful new autonomous systems have intelligible interfaces built-in. From our results, we propose several implications and directions for future research to-wards this goal.","2018-04-21","2023-04-11 04:01:37","2023-05-24 14:02:07","2023-04-10","1–18","","","","","","Trends and Trajectories for Explainable, Accountable and Intelligible Systems","CHI '18","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","269 citations (Crossref) [2023-04-11]","","C:\Users\ambreen.hanif\Zotero\storage\8X2U49GX\Abdul et al_2018_Trends and Trajectories for Explainable, Accountable and Intelligible Systems.pdf; ","notion://www.notion.so/Abdul-et-al-2018-4e7d78d30ebb42769f53dfa2f743b7fb","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MNTTWYS2","journalArticle","2019","Baniecki, Hubert; Biecek, Przemyslaw","modelStudio: Interactive Studio with Explanations for ML Predictive Models","Journal of Open Source Software","","2475-9066","10.21105/joss.01798","https://joss.theoj.org/papers/10.21105/joss.01798","","2019-11-05","2023-05-22 03:53:56","2023-05-24 14:02:05","2023-05-22 03:53:56","1798","","43","4","","JOSS","modelStudio","","","","","","","en","","","","","DOI.org (Crossref)","","8 citations (Crossref) [2023-05-22]","","C:\Users\ambreen.hanif\Zotero\storage\6FLFFQI9\Baniecki and Biecek - 2019 - modelStudio Interactive Studio with Explanations .pdf; ","notion://www.notion.so/Baniecki-Biecek-2019-ebcd99cecd724758b432ad344b02532e","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TN3G8YSW","book","2019","El-Assady, Mennatallah; Jentner, Wolfgang; Kehlbeck, Rebecca; Schlegel, Udo; Sevastjanova, Rita; Sperrle, Fabian; Spinner, Thilo; Keim, Daniel","Towards XAI: Structuring the Processes of Explanations","","","","","","Explainable Artificial Intelligence describes a process to reveal the logical propagation of operations that transform a given input to a certain output. In this paper, we investigate the design space of explanation processes based on factors gathered from six research areas, namely, Pedagogy, Story-telling, Argumentation, Programming, Trust-Building, and Gamification. We contribute a conceptual model describing the building blocks of explanation processes, including a comprehensive overview of explanation and verification phases, pathways, mediums, and strategies. We further argue for the importance of studying effective methods of explainable machine learning, and discuss open research challenges and opportunities. Figure 1: The proposed explanation process model. On the highest level, each explanation consists of different phases that structure the whole process into defined elements. Each phase contains explanation blocks, i.e., self-contained units to explain one phenomenon based on a selected strategy and medium. At the end of each explanation phase, an optional verification block ensures the understanding of the explained aspects. Lastly, to transition between phases and building blocks, different pathways are utilized.","2019-05-04","2023-04-10 23:22:48","2023-05-24 14:02:04","","","","","","","","Towards XAI","","","","","","","","","","","","ResearchGate","","","","; ","notion://www.notion.so/El-Assady-et-al-2019-c2b4fc63b5704413a549f4ffd38c0bcd; https://www.researchgate.net/profile/Mennatallah-El-Assady/publication/332802468_Towards_XAI_Structuring_the_Processes_of_Explanations/links/5ccad56b92851c8d22146613/Towards-XAI-Structuring-the-Processes-of-Explanations.pdf","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TN8EK6XR","conferencePaper","2018","Bellamy, Rachel K. E.; Dey, Kuntal; Hind, Michael; Hoffman, Samuel C.; Houde, Stephanie; Kannan, Kalapriya; Lohia, Pranay; Martino, Jacquelyn; Mehta, Sameep; Mojsilovic, Aleksandra; Nagar, Seema; Ramamurthy, Karthikeyan Natesan; Richards, John; Saha, Diptikalyan; Sattigeri, Prasanna; Singh, Moninder; Varshney, Kush R.; Zhang, Yunfeng","AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias","","","","","https://arxiv.org/abs/1810.01943","","2018-10","2023-04-03 04:30:12","2023-05-24 14:02:03","","","","","","","","","","","","","","","","","","","","","","Citation Key: aif360-oct-2018","","","notion://www.notion.so/Bellamy-et-al-2018-2771d46c5dd94dd28283421036a74dcc","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9H2BTL8L","journalArticle","2018","Biecek, Przemyslaw","DALEX: Explainers for Complex Predictive Models in R","Journal of Machine Learning Research","","1533-7928","","http://jmlr.org/papers/v19/18-416.html","Predictive modeling is invaded by elastic, yet complex methods such as neural networks or ensembles (model stacking, boosting or bagging). Such methods are usually described by a large number of parameters or hyper parameters - a price that one needs to pay for elasticity. The very number of parameters makes models hard to understand. This paper describes a consistent collection of explainers for predictive models, a.k.a. black boxes. Each explainer is a technique for exploration of a black box model. Presented approaches are model-agnostic, what means that they extract useful information from any predictive method irrespective of its internal structure. Each explainer is linked with a specific aspect of a model. Some are useful in decomposing predictions, some serve better in understanding performance, while others are useful in understanding importance and conditional responses of a particular variable. Every explainer presented here works for a single model or for a collection of models. In the latter case, models can be compared against each other. Such comparison helps to find strengths and weaknesses of different models and gives additional tools for model validation. Presented explainers are implemented in the DALEX package for R. They are based on a uniform standardized grammar of model exploration which may be easily extended.","2018","2023-05-22 03:50:04","2023-05-24 14:02:02","2023-05-22 03:50:04","1-5","","84","19","","","DALEX","","","","","","","","","","","","jmlr.org","","","","C:\Users\ambreen.hanif\Zotero\storage\LETSE7JY\Biecek_2018_DALEX.pdf; ","notion://www.notion.so/Biecek-2018-71dccfe4e04e4fb5b93a08ab4e335e8e","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ETSNG2NR","conferencePaper","2021","Paleja, Rohan; Ghuy, Muyleng; Ranawaka Arachchige, Nadun; Jensen, Reed; Gombolay, Matthew","The Utility of Explainable AI in Ad Hoc Human-Machine Teaming","Advances in Neural Information Processing Systems","","","","https://proceedings.neurips.cc/paper_files/paper/2021/hash/05d74c48b5b30514d8e9bd60320fc8f6-Abstract.html","","2021","2023-05-24 02:45:55","2023-05-24 12:27:47","2023-05-24 02:45:54","610–623","","","34","","","","","","","","Curran Associates, Inc.","","","","","","","Neural Information Processing Systems","","","","; C:\Users\ambreen.hanif\Zotero\storage\F9DKJKKG\Paleja et al_2021_The Utility of Explainable AI in Ad Hoc Human-Machine Teaming.pdf","notion://www.notion.so/Paleja-et-al-2021-287902505127433594aceb570c72726d","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9677UQCP","journalArticle","","Mueller, Shane T; Hoffman, Robert R; Clancey, William; Emrey, Abigail","Explanation in Human-AI Systems: A Literature Meta-Review Synopsis of Key Ideas and Publications and Bibliography for Explainable AI","","","","","","","","2023-05-24 02:49:04","2023-05-24 12:27:46","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\ZI2QKS8K\Mueller et al. - Explanation in Human-AI Systems A Literature Meta.pdf; ","notion://www.notion.so/Mueller-et-al-n-d-18da9585eb3b42c593485e69758ee631","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VCF7J9JC","webpage","","","O'Reilly Machine Learning for High-Risk Applications: Techniques for Responsible AI","Dataiku","","","","https://content.dataiku.com/oreilly-responsible-ai","Machine Learning for High-Risk Applications will arm practitioners with a solid understanding of model governance processes and a new way to use common Python tools for training interpretable models and debugging them for performance, safety, fairness, security and privacy issues.","","2023-05-24 06:26:22","2023-05-24 06:26:22","2023-05-24 06:26:22","","","","","","","O'Reilly Machine Learning for High-Risk Applications","","","","","","","en","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\Y8Q9SGDM\oreilly-responsible-ai.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ULMK45SV","journalArticle","2021","Taylor, J. Eric T.; Taylor, Graham W.","Artificial cognition: How experimental psychology can help generate explainable artificial intelligence","Psychonomic Bulletin & Review","","1531-5320","10.3758/s13423-020-01825-5","https://doi.org/10.3758/s13423-020-01825-5","Artificial intelligence powered by deep neural networks has reached a level of complexity where it can be difficult or impossible to express how a model makes its decisions. This black-box problem is especially concerning when the model makes decisions with consequences for human well-being. In response, an emerging field called explainable artificial intelligence (XAI) aims to increase the interpretability, fairness, and transparency of machine learning. In this paper, we describe how cognitive psychologists can make contributions to XAI. The human mind is also a black box, and cognitive psychologists have over 150 years of experience modeling it through experimentation. We ought to translate the methods and rigor of cognitive psychology to the study of artificial black boxes in the service of explainability. We provide a review of XAI for psychologists, arguing that current methods possess a blind spot that can be complemented by the experimental cognitive tradition. We also provide a framework for research in XAI, highlight exemplary cases of experimentation within XAI inspired by psychological science, and provide a tutorial on experimenting with machines. We end by noting the advantages of an experimental approach and invite other psychologists to conduct research in this exciting new field.","2021-04-01","2023-05-24 02:51:51","2023-05-24 02:52:23","2023-05-24 02:51:46","454-475","","2","28","","Psychon Bull Rev","Artificial cognition","","","","","","","en","","","","","Springer Link","","20 citations (Crossref) [2023-05-24]","","; C:\Users\ambreen.hanif\Zotero\storage\EPRYH84K\Taylor_Taylor_2021_Artificial cognition.pdf","notion://www.notion.so/Taylor-Taylor-2021-6b13fb1882334f6e909834e6253fe0c8","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FR5G5QFX","journalArticle","2021","Alufaisan, Yasmeen; Marusich, Laura R.; Bakdash, Jonathan Z.; Zhou, Yan; Kantarcioglu, Murat","Does Explainable Artificial Intelligence Improve Human Decision-Making?","Proceedings of the AAAI Conference on Artificial Intelligence","","2374-3468","10.1609/aaai.v35i8.16819","https://ojs.aaai.org/index.php/AAAI/article/view/16819","Explainable AI provides insights to users into the why for model predictions, offering potential for users to better understand and trust a model, and to recognize and correct AI predictions that are incorrect. Prior research on human and explainable AI interactions has focused on measures such as interpretability, trust, and usability of the explanation. There are mixed findings whether explainable AI can improve actual human decision-making and the ability to identify the problems with the underlying model. Using real datasets, we compare objective human decision accuracy without AI (control), with an AI prediction (no explanation), and AI prediction with explanation. We find providing any kind of AI prediction tends to improve user decision accuracy, but no conclusive evidence that explainable AI has a meaningful impact. Moreover, we observed the strongest predictor for human decision accuracy was AI accuracy and that users were somewhat able to detect when the AI was correct vs. incorrect, but this was not significantly affected by including an explanation. Our results indicate that, at least in some situations, the why information provided in explainable AI may not enhance user decision-making, and further research may be needed to understand how to integrate explainable AI into real systems.","2021-05-18","2023-05-24 02:49:00","2023-05-24 02:49:39","2023-05-24 02:49:00","6618-6626","","8","35","","","","","","","","","","en","Copyright (c) 2021 Association for the Advancement of Artificial Intelligence","","","","ojs.aaai.org","","12 citations (Crossref) [2023-05-24] Number: 8","","C:\Users\ambreen.hanif\Zotero\storage\4NL35ANG\Alufaisan et al_2021_Does Explainable Artificial Intelligence Improve Human Decision-Making.pdf; ","notion://www.notion.so/Alufaisan-et-al-2021-271408d8c34e4d83a14c8515587686c4","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZIE8DXVD","journalArticle","2023","Leichtmann, Benedikt; Humer, Christina; Hinterreiter, Andreas; Streit, Marc; Mara, Martina","Effects of Explainable Artificial Intelligence on trust and human behavior in a high-risk decision task","Computers in Human Behavior","","0747-5632","10.1016/j.chb.2022.107539","https://www.sciencedirect.com/science/article/pii/S0747563222003594","Understanding the recommendations of an artificial intelligence (AI) based assistant for decision-making is especially important in high-risk tasks, such as deciding whether a mushroom is edible or poisonous. To foster user understanding and appropriate trust in such systems, we assessed the effects of explainable artificial intelligence (XAI) methods and an educational intervention on AI-assisted decision-making behavior in a 2 × 2 between subjects online experiment with N=410 participants. We developed a novel use case in which users go on a virtual mushroom hunt and are tasked with picking edible and leaving poisonous mushrooms. Users were provided with an AI-based app that showed classification results of mushroom images. To manipulate explainability, one subgroup additionally received attribution-based and example-based explanations of the AI’s predictions; for the educational intervention one subgroup received additional information on how the AI worked. We found that the group that received explanations outperformed that which did not and showed better calibrated trust levels. Contrary to our expectations, we found that the educational intervention, domain-specific (i.e., mushroom) knowledge, and AI knowledge had no effect on performance. We discuss practical implications and introduce the mushroom-picking task as a promising use case for XAI research.","2023-02-01","2023-05-24 02:46:40","2023-05-24 02:48:12","2023-05-24 02:46:40","107539","","","139","","Computers in Human Behavior","","","","","","","","en","","","","","ScienceDirect","","2 citations (Crossref) [2023-05-24]","","C:\Users\ambreen.hanif\Zotero\storage\2PMDFGAA\Leichtmann et al_2023_Effects of Explainable Artificial Intelligence on trust and human behavior in a.pdf; ; C:\Users\ambreen.hanif\Zotero\storage\TTWFP8CG\S0747563222003594.html","notion://www.notion.so/Leichtmann-et-al-2023-1869d13325214615834ef699f4020235","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6KUSGKCF","journalArticle","2023","Silva, Andrew; Schrum, Mariah; Hedlund-Botti, Erin; Gopalan, Nakul; Gombolay, Matthew","Explainable Artificial Intelligence: Evaluating the Objective and Subjective Impacts of xAI on Human-Agent Interaction","International Journal of Human–Computer Interaction","","1044-7318","10.1080/10447318.2022.2101698","https://doi.org/10.1080/10447318.2022.2101698","Intelligent agents must be able to communicate intentions and explain their decision-making processes to build trust, foster confidence, and improve human-agent team dynamics. Recognizing this need, academia and industry are rapidly proposing new ideas, methods, and frameworks to aid in the design of more explainable AI. Yet, there remains no standardized metric or experimental protocol for benchmarking new methods, leaving researchers to rely on their own intuition or ad hoc methods for assessing new concepts. In this work, we present the first comprehensive (n = 286) user study testing a wide range of approaches for explainable machine learning, including feature importance, probability scores, decision trees, counterfactual reasoning, natural language explanations, and case-based reasoning, as well as a baseline condition with no explanations. We provide the first large-scale empirical evidence of the effects of explainability on human-agent teaming. Our results will help to guide the future of explainability research by highlighting the benefits of counterfactual explanations and the shortcomings of confidence scores for explainability. We also propose a novel questionnaire to measure explainability with human participants, inspired by relevant prior work and correlated with human-agent teaming metrics.","2023-04-21","2023-05-24 02:45:40","2023-05-24 02:46:48","2023-05-24 02:45:40","1390-1404","","7","39","","","Explainable Artificial Intelligence","","","","","","","","","","","","Taylor and Francis+NEJM","","3 citations (Crossref) [2023-05-24] Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/10447318.2022.2101698","","; C:\Users\ambreen.hanif\Zotero\storage\6Y99SVD8\Silva et al_2023_Explainable Artificial Intelligence.pdf","notion://www.notion.so/Silva-et-al-2023-147204d7765040f39451e21a8510d383","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7Z47ZQX2","preprint","2023","Kandul, Serhiy; Micheli, Vincent; Beck, Juliane; Kneer, Markus; Burri, Thomas; Fleuret, François; Christen, Markus","Explainable AI: A Review of the Empirical Literature","","","","10.2139/ssrn.4325219","https://papers.ssrn.com/abstract=4325219","AI deep learning models have become more capable, but also more complex and less explainable. To address this development, new research on so-called explainable AI (XAI) has proliferated. This paper surveys the empirical literature on XAI based on human-subject experiments. It classifies extant work across different technical and experimental dimensions. Our findings suggest that explainable AI improves self-reported understanding and trust in AI. However, this rarely translates into improved performance of humans in incentivized tasks with AI support. We list several implications of these findings concerning the use of explainable AI in human-computer interaction.","2023-01-16","2023-05-24 02:43:33","2023-05-24 02:43:37","2023-05-24 02:43:33","","","","","","","Explainable AI","","","","","","Rochester, NY","en","","SSRN Scholarly Paper","","","Social Science Research Network","","0 citations (Crossref) [2023-05-24]","","C:\Users\ambreen.hanif\Zotero\storage\DMULH5U6\Kandul et al_2023_Explainable AI.pdf","","","","","","","","","","","","","","","","","","","","","","4325219","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EB7YXTJX","preprint","2018","Simmons, Andrew J.; Barnett, Scott; Vajda, Simon; Vasa, Rajesh","Data Provenance for Sport","","","","10.48550/arXiv.1812.05804","http://arxiv.org/abs/1812.05804","Data analysts often discover irregularities in their underlying dataset, which need to be traced back to the original source and corrected. Standards for representing data provenance (i.e. the origins of the data), such as the W3C PROV standard, can assist with this process, however require a mapping between abstract provenance concepts and the domain of use in order to apply them effectively. We propose a custom notation for expressing provenance of information in the sport performance analysis domain, and map our notation to concepts in the W3C PROV standard where possible. We evaluate the functionality of W3C PROV (without specialisations) and the VisTrails workflow manager (without extensions), and find that as is, neither are able to fully capture sport performance analysis workflows, notably due to limitations surrounding capture of automated and manual activities respectively. Furthermore, their notations suffer from ineffective use of visual design space, and present potential usability issues as their terminology is unlikely to match that of sport practitioners. Our findings suggest that one-size-fits-all provenance and workflow systems are a poor fit in practice, and that their notation and functionality need to be optimised for the domain of use.","2018-12-14","2023-05-24 02:38:18","2023-05-24 02:39:03","2023-05-24 02:38:18","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1812.05804 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\K3ZHIQ88\1812.html; ; C:\Users\ambreen.hanif\Zotero\storage\XI97G3H9\Simmons et al_2018_Data Provenance for Sport.pdf","notion://www.notion.so/Simmons-et-al-2018-64eb87ccebf54c618a17732548823595","notion","","","","","","","","","","","","","","","","","","","","arXiv:1812.05804","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SSPTWKAG","webpage","","","[1812.05804] Data Provenance for Sport","","","","","https://arxiv.org/abs/1812.05804","","","2023-05-24 02:37:48","2023-05-24 02:38:07","2023-05-24 02:37:48","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\EVXVU4BP\1812.html; ","notion://www.notion.so/1812-05804-Data-Provenance-for-Sport-n-d-6cbdaed7fe7d4bd5b1591d1335a05823","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4UXJHFWL","book","2021","Ehsan, Upol; Liao, Vera; Muller, Michael; Riedl, Mark; Weisz, Justin","Expanding Explainability: Towards Social Transparency in AI systems","","","","","","As AI-powered systems increasingly mediate consequential decision-making, their explainability is critical for end-users to take informed and accountable actions. Explanations in human-human interactions are socially-situated. AI systems are often socio-organizationally embedded. However, Explainable AI (XAI) approaches have been predominantly algorithm-centered. We take a developmental step towards socially-situated XAI by introducing and exploring Social Transparency (ST), a sociotechnically informed perspective that incorporates the socio-organizational context into explaining AI-mediated decision-making. To explore ST conceptually, we conducted interviews with 29 AI users and practitioners grounded in a speculative design scenario. We suggested constitutive design elements of ST and developed a conceptual framework to unpack ST's effect and implications at the technical, decision-making, and organizational level. The framework showcases how ST can potentially calibrate trust in AI, improve decision-making, facilitate organizational collective actions, and cultivate holistic explainability. Our work contributes to the discourse of Human-Centered XAI by expanding the design space of XAI.","2021-05-08","2023-05-23 03:21:30","2023-05-23 03:21:46","","","","","","","","Expanding Explainability","","","","","","","","","","","","ResearchGate","","DOI: 10.1145/3411764.3445188","","C:\Users\ambreen.hanif\Zotero\storage\JB5UQUIT\Ehsan et al_2021_Expanding Explainability.pdf; ; ","notion://www.notion.so/Ehsan-et-al-2021-51db9c23acf34dcda86f8bb52afd9905; https://www.researchgate.net/publication/348443845_Expanding_Explainability_Towards_Social_Transparency_in_AI_systems","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JIUWK398","book","2019","Ehsan, Upol; Tambwekar, Pradyumna; Chan, Larry; Harrison, Brent; Riedl, Mark","Automated Rationale Generation: A Technique for Explainable AI and its Effects on Human Perceptions","","","","","","Automated rationale generation is an approach for real-time explanation generation whereby a computational model learns to translate an autonomous agent's internal state and action data representations into natural language. Training on human explanation data can enable agents to learn to generate human-like explanations for their behavior. In this paper, using the context of an agent that plays Frogger, we describe (a) how to collect a corpus of explanations, (b) how to train a neural rationale generator to produce different styles of rationales, and (c) how people perceive these rationales. We conducted two user studies. The first study establishes the plausibility of each type of generated rationale and situates their user perceptions along the dimensions of confidence, humanlike-ness, adequate justification, and understandability. The second study further explores user preferences between the generated rationales with regard to confidence in the autonomous agent, communicating failure and unexpected behavior. Overall, we find alignment between the intended differences in features of the generated rationales and the perceived differences by users. Moreover, context permitting, participants preferred detailed rationales to form a stable mental model of the agent's behavior.","2019-01-11","2023-05-23 03:10:45","2023-05-23 03:11:04","","","","","","","","Automated Rationale Generation","","","","","","","","","","","","ResearchGate","","","","C:\Users\ambreen.hanif\Zotero\storage\GXRVPA3J\Ehsan et al_2019_Automated Rationale Generation.pdf; ; ","notion://www.notion.so/Ehsan-et-al-2019-f65b26b121144fa4a7f2e680fba2a767; https://www.researchgate.net/publication/330383037_Automated_Rationale_Generation_A_Technique_for_Explainable_AI_and_its_Effects_on_Human_Perceptions","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M5G4SQ27","book","2018","Ehsan, Upol; Harrison, Brent; Chan, Larry; Riedl, Mark","Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations","","","","","","We introduce AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had performed the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of an autonomous agent into natural language. We evaluate our technique in the Frogger game environment, training an autonomous game playing agent to rationalize its action choices using natural language. A natural language training corpus is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation and show the results of two experiments evaluating the effectiveness of rationalization. Results of these evaluations show that neural machine translation is able to accurately generate rationalizations that describe agent behavior, and that rationalizations are more satisfying to humans than other alternative methods of explanation.","2018-02-02","2023-05-23 03:06:17","2023-05-23 03:06:55","","","","","","","","Rationalization","","","","","","","","","","","","ResearchGate","","DOI: 10.1145/3278721.3278736","","C:\Users\ambreen.hanif\Zotero\storage\2QDWVD47\Ehsan et al_2018_Rationalization.pdf; ; ","notion://www.notion.so/Ehsan-et-al-2018-4b0cca7da58c4abb80651e5ec14bace0; https://www.researchgate.net/publication/314092665_Rationalization_A_Neural_Machine_Translation_Approach_to_Generating_Natural_Language_Explanations","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F48769TT","webpage","","","(12) (PDF) Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations","","","","","https://www.researchgate.net/publication/314092665_Rationalization_A_Neural_Machine_Translation_Approach_to_Generating_Natural_Language_Explanations","","","2023-05-23 03:06:02","2023-05-23 03:06:08","2023-05-23 03:06:02","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\SBCTST5W\314092665_Rationalization_A_Neural_Machine_Translation_Approach_to_Generating_Natural_Language_.html; ","notion://www.notion.so/12-PDF-Rationalization-A-Neural-Machine-Translation-Approach-to-Generating-Natural-Language-Expl-3afdc608bb264d0faea333c0bdce6de4","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R928F5KV","webpage","2021","","Upol Ehsan on Human-Centered Explainable AI and Social Transparency","The Gradient","","","","https://thegradient.pub/upol-ehsan-interview/","An interview with Upol Ehsan on Human-Centered Explainable AI and Expanding Explainability for AI","2021-12-03","2023-05-23 02:58:27","2023-05-23 02:58:36","2023-05-23 02:58:27","","","","","","","","","","","","","","en","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\VLHN4SRB\upol-ehsan-interview.html","notion://www.notion.so/Upol-Ehsan-on-Human-Centered-Explainable-AI-and-Social-Transparency-2021-754b21b9dbf24b658d6c681ef2f2eef0","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"84DLTXH2","book","2022","Ehsan, Upol; Riedl, Mark","Social Construction of XAI: Do We Need One Definition to Rule Them All?","","","","","","There is a growing frustration amongst researchers and developers in Explainable AI (XAI) around the lack of consensus around what is meant by 'explainability'. Do we need one definition of explainability to rule them all? In this paper, we argue why a singular definition of XAI is neither feasible nor desirable at this stage of XAI's development. We view XAI through the lenses of Social Construction of Technology (SCOT) to explicate how diverse stakeholders (relevant social groups) have different interpretations (interpretative flexibility) that shape the meaning of XAI. Forcing a standardization (closure) on the pluralistic interpretations too early can stifle innovation and lead to premature conclusions. We share how we can leverage the pluralism to make progress in XAI without having to wait for a definitional consensus.","2022-11-11","2023-05-23 02:58:02","2023-05-23 02:58:13","","","","","","","","Social Construction of XAI","","","","","","","","","","","","ResearchGate","","DOI: 10.48550/arXiv.2211.06499","","C:\Users\ambreen.hanif\Zotero\storage\MJQV2F4F\Ehsan_Riedl_2022_Social Construction of XAI.pdf; ; ","notion://www.notion.so/Ehsan-Riedl-2022-46db136ceb894e0399ecc5a1f030b8a5; https://www.researchgate.net/publication/365372367_Social_Construction_of_XAI_Do_We_Need_One_Definition_to_Rule_Them_All","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SLMIPRYF","book","2022","Ehsan, Upol; Liao, Vera; Passi, Samir; Riedl, Mark; Daume, III","Seamful XAI: Operationalizing Seamful Design in Explainable AI","","","","","","Mistakes in AI systems are inevitable, arising from both technical limitations and sociotechnical gaps. While black-boxing AI systems can make the user experience seamless, hiding the seams risks disempowering users to mitigate fallouts from AI mistakes. While Explainable AI (XAI) has predominantly tackled algorithmic opaqueness, we propose that seamful design can foster Humancentered XAI by strategically revealing sociotechnical and infrastructural mismatches. We introduce the notion of Seamful XAI by (1) conceptually transferring ""seams"" to the AI context and (2) developing a design process that helps stakeholders design with seams, thereby augmenting explainability and user agency. We explore this process with 43 AI practitioners and users, using a scenario-based co-design activity informed by real-world use cases. We share empirical insights, implications, and critical reflections on how this process can help practitioners anticipate and craft seams in AI, how seamfulness can improve explainability, empower end-users, and facilitate Responsible AI.","2022-11-12","2023-05-23 02:56:55","2023-05-23 02:57:19","","","","","","","","Seamful XAI","","","","","","","","","","","","ResearchGate","","DOI: 10.48550/arXiv.2211.06753","","C:\Users\ambreen.hanif\Zotero\storage\WJZ996C4\Ehsan et al_2022_Seamful XAI.pdf; ; ","notion://www.notion.so/Ehsan-et-al-2022-123af3fe43804a27b4b246d50849de3d; https://www.researchgate.net/publication/365372009_Seamful_XAI_Operationalizing_Seamful_Design_in_Explainable_AI","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TYAIR7BM","book","2023","Ehsan, Upol; Saha, Koustuv; Choudhury, Munmun; Riedl, Mark","Charting the Sociotechnical Gap in Explainable AI: A Framework to Address the Gap in XAI","","","","","","Explainable AI (XAI) systems are sociotechnical in nature; thus, they are subject to the sociotechnical gap--divide between the technical affordances and the social needs. However, charting this gap is challenging. In the context of XAI, we argue that charting the gap improves our problem understanding, which can reflexively provide actionable insights to improve explainability. Utilizing two case studies in distinct domains, we empirically derive a framework that facilitates systematic charting of the sociotechnical gap by connecting AI guidelines in the context of XAI and elucidating how to use them to address the gap. We apply the framework to a third case in a new domain, showcasing its affordances. Finally, we discuss conceptual implications of the framework, share practical considerations in its operationalization, and offer guidance on transferring it to new contexts. By making conceptual and practical contributions to understanding the sociotechnical gap in XAI, the framework expands the XAI design space.","2023-02-01","2023-05-23 02:56:28","2023-05-23 02:56:28","","","","","","","","Charting the Sociotechnical Gap in Explainable AI","","","","","","","","","","","","ResearchGate","","DOI: 10.48550/arXiv.2302.00799","","C:\Users\ambreen.hanif\Zotero\storage\QCEBZCHN\Ehsan et al_2023_Charting the Sociotechnical Gap in Explainable AI.pdf; ; ","notion://www.notion.so/Ehsan-et-al-2023-be437aa47b284ab499fde91d468d6200; https://www.researchgate.net/publication/368159860_Charting_the_Sociotechnical_Gap_in_Explainable_AI_A_Framework_to_Address_the_Gap_in_XAI","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WMZDDURH","journalArticle","2018","Lipton, Zachary C.","The mythos of model interpretability","Communications of the ACM","","0001-0782, 1557-7317","10.1145/3233231","https://dl.acm.org/doi/10.1145/3233231","In machine learning, the concept of interpretability is both important and slippery.","2018-09-26","2022-04-27 02:34:07","2023-05-23 01:36:12","2022-04-27 02:34:07","36-43","","10","61","","Commun. ACM","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\ambreen.hanif\Zotero\storage\242474FV\1606.html; C:\Users\ambreen.hanif\Zotero\storage\TP42SHVD\Lipton - 2018 - The mythos of model interpretability.pdf; C:\Users\ambreen.hanif\Zotero\storage\XC9Y85KX\Lipton_2016_The Mythos of Model Interpretability.pdf; C:\Users\ambreen.hanif\Zotero\storage\8G47PVTR\Lipton_2017_The Mythos of Model Interpretability.pdf; ","notion://www.notion.so/cite-lipton_mythos_2018-1012d08eef964457a08c693e1562e870","notion","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HW5THURX","webpage","2019","Alexandra Samuel","How to Use Zotero and Scrivener for Research-Driven Writing","JSTOR Daily","","","","https://daily.jstor.org/how-to-use-zotero-and-scrivener-for-research-driven-writing/","This month, I’m doing something a little different with my column: I’m sharing the system I use to write it, so that you can use or adapt my system.","2019-12-17","2021-02-02 02:03:57","2023-05-23 01:36:09","2021-02-02 02:03:57","","","","","","","","","","","","","","en-US","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\JL2Q3WDM\how-to-use-zotero-and-scrivener-for-research-driven-writing.html","notion://www.notion.so/cite-alexandra_samuel_how_2019-d4e7d6888a1741819ec7ee9562290006","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F45Y93S9","webpage","","","ZotFile - Advanced PDF management for Zotero","","","","","http://zotfile.com/","","","2021-02-02 02:04:08","2023-05-23 01:36:08","2021-02-02 02:04:08","","","","","","","","","","","","","","","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\936N2FUA\zotfile.com.html","notion://www.notion.so/cite-noauthor_zotfile_nodate-88fbc3233f054bcf98dbd89ab8f9055f","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B9GFF6T7","conferencePaper","2021","Sanjana Rao, G. P.; Aditya Shastry, K.; Sathyashree, S. R.; Sahu, Shivani","Machine Learning based Restaurant Revenue Prediction","Evolutionary Computing and Mobile Sustainable Networks","9789811552588","","10.1007/978-981-15-5258-8_35","","Food industry has a crucial part in enhancing the financial progress of a country. This is very true for metropolitan cities than any small towns of our country. Despite the contribution of food industry to the economy, the revenue prediction of the restaurant has been limited. The agenda of this work is basically to detect the revenue for any upcoming setting of restaurant. There are three types of restaurant which have been encountered. They are inline, food court, and mobile. In our proposed solution, we take into consideration the various features of the datasets for the prediction. The input features were ordered based on their impact on the target attribute which was the restaurant revenue. Various other pre-processing techniques like Principal Component Analysis (PCA), feature selection and label encoding have been used. Without the proper analysis of Kaggle datasets pre-processing cannot be done. Algorithms are then evaluated on the test data after being trained on the training datasets. Random Forest (RF) was found to be the best performing model for revenue prediction when compared to linear regression model. The model accuracy does make a difference before pre-processing and after pre-processing. The accuracy increases after the applied methods of pre-processing.","2021","2021-02-02 02:06:42","2023-05-23 01:36:04","","363-371","","","","","","","Lecture Notes on Data Engineering and Communications Technologies","","","","Springer","Singapore","en","","","","","Springer Link","","","","; C:\Users\ambreen.hanif\Zotero\storage\NK8NDGZC\Sanjana Rao et al. - 2021 - Machine Learning based Restaurant Revenue Predicti.pdf","notion://www.notion.so/cite-sanjana_rao_machine_2021-7e9f4a15c46041308189444f0407130f","notion","Machine learning; Pre-processing; Prediction; Restaurant; Revenue","Suma, V.; Bouhmala, Noureddine; Wang, Haoxiang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7LS58FRE","preprint","2014","Maleki, Sasan; Tran-Thanh, Long; Hines, Greg; Rahwan, Talal; Rogers, Alex","Bounding the Estimation Error of Sampling-based Shapley Value Approximation","","","","","http://arxiv.org/abs/1306.4265","The Shapley value is arguably the most central normative solution concept in cooperative game theory. It specifies a unique way in which the reward from cooperation can be ""fairly"" divided among players. While it has a wide range of real world applications, its use is in many cases hampered by the hardness of its computation. A number of researchers have tackled this problem by (i) focusing on classes of games where the Shapley value can be computed efficiently, or (ii) proposing representation formalisms that facilitate such efficient computation, or (iii) approximating the Shapley value in certain classes of games. For the classical \textit{characteristic function} representation, the only attempt to approximate the Shapley value for the general class of games is due to Castro \textit{et al.} \cite{castro}. While this algorithm provides a bound on the approximation error, this bound is \textit{asymptotic}, meaning that it only holds when the number of samples increases to infinity. On the other hand, when a finite number of samples is drawn, an unquantifiable error is introduced, meaning that the bound no longer holds. With this in mind, we provide non-asymptotic bounds on the estimation error for two cases: where (i) the \textit{variance}, and (ii) the \textit{range}, of the players' marginal contributions is known. Furthermore, for the second case, we show that when the range is significantly large relative to the Shapley value, the bound can be improved (from $O(\frac{r}{m})$ to $O(\sqrt{\frac{r}{m}})$). Finally, we propose, and demonstrate the effectiveness of using stratified sampling for improving the bounds further.","2014-02-12","2023-05-22 12:39:04","2023-05-23 01:35:49","2023-05-22 12:39:04","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1306.4265 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\DNLYY22M\1306.html; C:\Users\ambreen.hanif\Zotero\storage\V9UCQC78\Maleki et al. - 2014 - Bounding the Estimation Error of Sampling-based Sh.pdf; ","notion://www.notion.so/Maleki-et-al-2014-498a9f30b5d74f67a26e6c917b86611d","notion","Computer Science - Computer Science and Game Theory","","","","","","","","","","","","","","","","","","","arXiv:1306.4265","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TGLFNY9E","journalArticle","2023","Kothadiya, Deep R.; Bhatt, Chintan M.; Rehman, Amjad; Alamri, Faten S.; Saba, Tanzila","SignExplainer: An Explainable AI-Enabled Framework for Sign Language Recognition with Ensemble Learning","IEEE Access","","2169-3536","10.1109/ACCESS.2023.3274851","","Deep learning has significantly aided current advancements in artificial intelligence. Deep learning techniques have significantly outperformed more than typical machine learning approaches, in various fields like Computer Vision, Natural Language Processing (NLP), Robotics Science, and Human-Computer Interaction (HCI). Deep learning models are somewhat ineffective in outlining their fundamental mechanism. That’s the reason the deep learning model mainly consider as Black-Box. To establish confidence and responsibility, deep learning applications need to explain the model’s decision in addition to the prediction of results. The explainable AI (XAI) research has created methods that offer these interpretations for already trained neural networks. It’s highly recommended for computer vision tasks relevant to medical science, defense system, and many more. The proposed study is associated with XAI for Sign Language Recognition, the methodology uses an attention-based ensemble learning approach to create a prediction model more accurate. Methodology uses ReSNet50 and Self Attention model to design ensemble learning architecture. The proposed ensemble learning approach has achieved remarkable accuracy at 98.20%. Interpret ensemble learning prediction, the author has proposed SignExplainer to explain the relevancy (in percentage) of predicted results. SignExplainer has illustrated excellent results, compare to other conventional Explainable AI models.","2023","2023-05-18 13:02:47","2023-05-23 01:35:39","","1-1","","","","","","SignExplainer","","","","","","","","","","","","IEEE Xplore","","0 citations (Crossref) [2023-05-22] Conference Name: IEEE Access","","C:\Users\ambreen.hanif\Zotero\storage\WLERAQ72\Kothadiya et al. - 2023 - SignExplainer An Explainable AI-Enabled Framework.pdf; C:\Users\ambreen.hanif\Zotero\storage\Q9BJ4XQ9\10122570.html; ","notion://www.notion.so/Kothadiya-et-al-2023-1fbbe4d6be9042bc98b8d02bc4ac910b","notion","Computational modeling; Deep learning; Explainable AI; Predictive models; Artificial intelligence; Assistive technologies; Classification; Computer vision; Computer Vision; Deep Learning; Gesture recognition; Sign Language; SignExplainer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WS93LVEB","journalArticle","2022","Tiddi, Ilaria; Schlobach, Stefan","Knowledge graphs as tools for explainable machine learning: A survey","Artificial Intelligence","","0004-3702","10.1016/j.artint.2021.103627","https://www.sciencedirect.com/science/article/pii/S0004370221001788","This paper provides an extensive overview of the use of knowledge graphs in the context of Explainable Machine Learning. As of late, explainable AI has become a very active field of research by addressing the limitations of the latest machine learning solutions that often provide highly accurate, but hardly scrutable and interpretable decisions. An increasing interest has also been shown in the integration of Knowledge Representation techniques in Machine Learning applications, mostly motivated by the complementary strengths and weaknesses that could lead to a new generation of hybrid intelligent systems. Following this idea, we hypothesise that knowledge graphs, which naturally provide domain background knowledge in a machine-readable format, could be integrated in Explainable Machine Learning approaches to help them provide more meaningful, insightful and trustworthy explanations. Using a systematic literature review methodology we designed an analytical framework to explore the current landscape of Explainable Machine Learning. We focus particularly on the integration with structured knowledge at large scale, and use our framework to analyse a variety of Machine Learning domains, identifying the main characteristics of such knowledge-based, explainable systems from different perspectives. We then summarise the strengths of such hybrid systems, such as improved understandability, reactivity, and accuracy, as well as their limitations, e.g. in handling noise or extracting knowledge efficiently. We conclude by discussing a list of open challenges left for future research.","2022-01-01","2023-05-08 19:25:23","2023-05-23 01:35:37","2023-05-08 19:25:09","103627","","","302","","Artificial Intelligence","Knowledge graphs as tools for explainable machine learning","","","","","","","en","","","","","ScienceDirect","","30 citations (Crossref) [2023-05-09]","","","notion://www.notion.so/Tiddi-Schlobach-2022-5661111314124379b369a5a8b531870e","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RN9W4H25","preprint","2020","Danilevsky, Marina; Qian, Kun; Aharonov, Ranit; Katsis, Yannis; Kawas, Ban; Sen, Prithviraj","A Survey of the State of Explainable AI for Natural Language Processing","","","","","http://arxiv.org/abs/2010.00711","Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.","2020-10-01","2023-05-08 18:54:13","2023-05-23 01:35:36","2023-05-08 18:54:07","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2010.00711 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\7VP867WL\2010.html; C:\Users\ambreen.hanif\Zotero\storage\PDHKB8YU\Danilevsky et al_2020_A Survey of the State of Explainable AI for Natural Language Processing.pdf; ","notion://www.notion.so/Danilevsky-et-al-2020-ed6a112161624e1e8f2539051592ab8c","notion","","","","","","","","","","","","","","","","","","","","arXiv:2010.00711","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9A84PZ6A","conferencePaper","2019","Xu, Feiyu; Uszkoreit, Hans; Du, Yangzhou; Fan, Wei; Zhao, Dongyan; Zhu, Jun","Explainable AI: A Brief Survey on History, Research Areas, Approaches and Challenges","","978-3-030-32235-9 978-3-030-32236-6","","10.1007/978-3-030-32236-6_51","http://link.springer.com/10.1007/978-3-030-32236-6_51","Deep learning has made significant contribution to the recent progress in artificial intelligence. In comparison to traditional machine learning methods such as decision trees and support vector machines, deep learning methods have achieved substantial improvement in various prediction tasks. However, deep neural networks (DNNs) are comparably weak in explaining their inference processes and final results, and they are typically treated as a black-box by both developers and users. Some people even consider DNNs (deep neural networks) in the current stage rather as alchemy, than as real science. In many real-world applications such as business decision, process optimization, medical diagnosis and investment recommendation, explainability and transparency of our AI systems become particularly essential for their users, for the people who are affected by AI decisions, and furthermore, for the researchers and developers who create the AI solutions. In recent years, the explainability and explainable AI have received increasing attention by both research community and industry. This paper first introduces the history of Explainable AI, starting from expert systems and traditional machine learning approaches to the latest progress in the context of modern deep learning, and then describes the major research areas and the state-of-art approaches in recent years. The paper ends with a discussion on the challenges and future directions.","2019","2023-05-08 18:51:42","2023-05-23 01:35:33","2023-05-08 18:51:41","563-574","","","11839","","","Explainable AI","","","","","Springer International Publishing","Cham","en","","","","","Semantic Scholar","","94 citations (Crossref) [2023-05-09] Book Title: Natural Language Processing and Chinese Computing Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-32236-6_51","","; ","notion://www.notion.so/Xu-et-al-2019-15f67c41c0964abaa0fb3b539bf53dc1; https://www.semanticscholar.org/paper/Explainable-AI%3A-A-Brief-Survey-on-History%2C-Research-Xu-Uszkoreit/62635461b44d9ff7901fbce982a5c1bbccfaf2b0","notion","","Tang, Jie; Kan, Min-Yen; Zhao, Dongyan; Li, Sujian; Zan, Hongying","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FP455TIX","journalArticle","2023","Wu, Jian-Lin; Chang, Pei-Chen; Wang, Chao; Wang, Ko-Chih","ATICVis: A Visual Analytics System for Asymmetric Transformer Models Interpretation and Comparison","Applied Sciences","","2076-3417","10.3390/app13031595","https://www.mdpi.com/2076-3417/13/3/1595","In recent years, natural language processing (NLP) technology has made great progress. Models based on transformers have performed well in various natural language processing problems. However, a natural language task can be carried out by multiple different models with slightly different architectures, such as different numbers of layers and attention heads. In addition to quantitative indicators such as the basis for selecting models, many users also consider the language understanding ability of the model and the computing resources it requires. However, comparing and deeply analyzing two transformer-based models with different numbers of layers and attention heads are not easy because it lacks the inherent one-to-one match between models, so comparing models with different architectures is a crucial and challenging task when users train, select, or improve models for their NLP tasks. In this paper, we develop a visual analysis system to help machine learning experts deeply interpret and compare the pros and cons of asymmetric transformer-based models when the models are applied to a user’s target NLP task. We propose metrics to evaluate the similarity between layers or attention heads to help users to identify valuable layers and attention head combinations to compare. Our visual tool provides an interactive overview-to-detail framework for users to explore when and why models behave differently. In the use cases, users use our visual tool to find out and explain why a large model does not significantly outperform a small model and understand the linguistic features captured by layers and attention heads. The use cases and user feedback show that our tool can help people gain insight and facilitate model comparison tasks.","2023-01","2023-05-08 18:50:22","2023-05-23 01:35:31","2023-05-08 18:50:22","1595","","3","13","","","ATICVis","","","","","","","en","http://creativecommons.org/licenses/by/3.0/","","","","www.mdpi.com","","0 citations (Crossref) [2023-05-09] Number: 3 Publisher: Multidisciplinary Digital Publishing Institute","","; C:\Users\ambreen.hanif\Zotero\storage\RQV74E5X\Wu et al_2023_ATICVis.pdf","notion://www.notion.so/Wu-et-al-2023-2bacaa5d6e4a42f484271030e0f26637","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MDLL4NMG","webpage","","","Papers with Code - Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond","","","","","https://paperswithcode.com/paper/harnessing-the-power-of-llms-in-practice-a","Implemented in one code library.","","2023-05-08 18:41:54","2023-05-23 01:35:27","2023-05-08 18:41:54","","","","","","","Papers with Code - Harnessing the Power of LLMs in Practice","","","","","","","en","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\4WUJUDDE\harnessing-the-power-of-llms-in-practice-a.html","notion://www.notion.so/Papers-with-Code-Harnessing-the-Power-of-LLMs-in-Practice-n-d-a973766878da4eef882763ffa07149a2","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CD2R7R7P","conferencePaper","2021","Qian, Kun; Danilevsky, Marina; Katsis, Yannis; Kawas, Ban; Oduor, Erick; Popa, Lucian; Li, Yunyao","XNLP: A Living Survey for XAI Research in Natural Language Processing","26th International Conference on Intelligent User Interfaces - Companion","978-1-4503-8018-8","","10.1145/3397482.3450728","https://dl.acm.org/doi/10.1145/3397482.3450728","We present XNLP: an interactive browser-based system embodying a living survey of recent state-of-the-art research in the field of Explainable AI (XAI) within the domain of Natural Language Processing (NLP). The system visually organizes and illustrates XAI-NLP publications and distills their content to allow users to gain insights, generate ideas, and explore the field. We hope that XNLP can become a leading demonstrative example of a living survey, balancing the depth and quality of a traditional well-constructed survey paper with the collaborative dynamism of a widely available interactive tool. XNLP can be accessed at: https://xainlp2020.github.io/xainlp.","2021-04-14","2023-05-08 18:37:58","2023-05-23 01:35:25","2023-05-08","78–80","","","","","","XNLP","IUI '21 Companion","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","7 citations (Crossref) [2023-05-09]","","; C:\Users\ambreen.hanif\Zotero\storage\F2MVJPDD\Qian et al_2021_XNLP.pdf","notion://www.notion.so/Qian-et-al-2021-c93042f3a3d847718ad7a7dd31a944d8","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4MCU77HR","book","","","","","","","","","","","2023-05-08 18:32:33","2023-05-23 01:35:21","","","","","","","","","","","","","","","","","","","","","","","","","notion://www.notion.so/n-d-49e0784f37ed4816a73885bc83214019","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PJG6U5NE","conferencePaper","2022","Jha, Sumit; Velasquez, Alvaro; Ewetz, Rickard; Pullum, Laura; Jha, Susmit","ExplainIt!: A Tool for Computing Robust Attributions of DNNs","Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence","978-1-956792-00-3","","10.24963/ijcai.2022/853","https://www.ijcai.org/proceedings/2022/853","Responsible integration of deep neural networks into the design of trustworthy systems requires the ability to explain decisions made by these models. Explainability and transparency are critical for system analysis, certification, and human-machine teaming. We have recently demonstrated that neural stochastic differential equations (SDEs) present an explanation-friendly DNN architecture. In this paper, we present ExplainIt, an online tool for explaining AI decisions that uses neural SDEs to create visually sharper and more robust attributions than traditional residual neural networks. Our tool shows that the injection of noise in every layer of a residual network often leads to less noisy and less fragile integrated gradient attributions. The discrete neural stochastic differential equation model is trained on the ImageNet data set with a million images, and the demonstration produces robust attributions on images in the ImageNet validation library and on a variety of images in the wild. Our online tool is hosted publicly for educational purposes.","2022-07","2023-05-08 18:31:51","2023-05-23 01:35:20","2023-05-08 18:31:51","5916-5919","","","","","","ExplainIt!","","","","","International Joint Conferences on Artificial Intelligence Organization","Vienna, Austria","en","","","","","DOI.org (Crossref)","","1 citations (Crossref) [2023-05-09]","","C:\Users\ambreen.hanif\Zotero\storage\FTJSBNBM\Jha et al. - 2022 - ExplainIt! A Tool for Computing Robust Attributio.pdf; ","notion://www.notion.so/Jha-et-al-2022-fd312c4b98e9448f8a4a813b6b2339ad","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Thirty-First International Joint Conference on Artificial Intelligence {IJCAI-22}","","","","","","","","","","","","","","",""
"42L933VZ","webpage","","","Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence | IJCAI","","","","","https://www.ijcai.org/proceedings/2022/","","","2023-05-08 18:29:32","2023-05-23 01:35:17","2023-05-08 18:29:32","","","","","","","","","","","","","","","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\V3FC5GKX\2022.html","notion://www.notion.so/Proceedings-of-the-Thirty-First-International-Joint-Conference-on-Artificial-Intelligence-IJCAI-n-98658547d97847dd8614515b2d4a6043","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4AREML96","conferencePaper","2022","Amgoud, Leila; Ben-Naim, Jonathan","Axiomatic Foundations of Explainability","Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence","978-1-956792-00-3","","10.24963/ijcai.2022/90","https://www.ijcai.org/proceedings/2022/90","Improving trust in decisions made by classiﬁcation models is becoming crucial for the acceptance of automated systems, and an important way of doing that is by providing explanations for the behaviour of the models. Different explainers have been proposed in the recent literature for that purpose, however their formal properties are under-studied.","2022-07","2023-05-08 18:29:06","2023-05-23 01:35:15","2023-05-08 18:29:06","636-642","","","","","","","","","","","International Joint Conferences on Artificial Intelligence Organization","Vienna, Austria","en","","","","","DOI.org (Crossref)","","2 citations (Crossref) [2023-05-09]","","C:\Users\ambreen.hanif\Zotero\storage\PRZIR75M\Amgoud and Ben-Naim - 2022 - Axiomatic Foundations of Explainability.pdf; ","notion://www.notion.so/Amgoud-Ben-Naim-2022-4b44be305168415d8dababe6f9a29627","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Thirty-First International Joint Conference on Artificial Intelligence {IJCAI-22}","","","","","","","","","","","","","","",""
"HWVQF34E","conferencePaper","2022","Malandri, Lorenzo; Mercorio, Fabio; Mezzanzanica, Mario; Nobani, Navid; Seveso, Andrea","The Good, the Bad, and the Explainer: A Tool for Contrastive Explanations of Text Classifiers","Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence","978-1-956792-00-3","","10.24963/ijcai.2022/858","https://www.ijcai.org/proceedings/2022/858","In the last few years, we have been witnessing the increasing deployment of machine learningbased systems, which act as black boxes whose behaviour is hidden to end-users. As a side-effect, this contributes to increasing the need for explainable methods and tools to support the coordination between humans and ML models towards collaborative decision-making. In this paper, we demonstrate ContrXT, a novel tool that computes the differences in the classification logic of two distinct trained models, reasoning on their symbolic representation through Binary Decision Diagrams. ContrXT is available as a pip package and API.","2022-07","2023-05-08 18:28:55","2023-05-23 01:35:13","2023-05-08 18:28:55","5936-5939","","","","","","The Good, the Bad, and the Explainer","","","","","International Joint Conferences on Artificial Intelligence Organization","Vienna, Austria","en","","","","","DOI.org (Crossref)","","1 citations (Crossref) [2023-05-09]","","C:\Users\ambreen.hanif\Zotero\storage\Y5GR5L6Y\Malandri et al. - 2022 - The Good, the Bad, and the Explainer A Tool for C.pdf; ","notion://www.notion.so/Malandri-et-al-2022-499fa87c02c64412b2cf1d5f64b9227e","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Thirty-First International Joint Conference on Artificial Intelligence {IJCAI-22}","","","","","","","","","","","","","","",""
"Q9Y3IUAQ","conferencePaper","2022","Malandri, Lorenzo; Mercorio, Fabio; Mezzanzanica, Mario; Nobani, Navid; Seveso, Andrea","The Good, the Bad, and the Explainer: A Tool for Contrastive Explanations of Text Classifiers","Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence","978-1-956792-00-3","","10.24963/ijcai.2022/858","https://www.ijcai.org/proceedings/2022/858","In the last few years, we have been witnessing the increasing deployment of machine learningbased systems, which act as black boxes whose behaviour is hidden to end-users. As a side-effect, this contributes to increasing the need for explainable methods and tools to support the coordination between humans and ML models towards collaborative decision-making. In this paper, we demonstrate ContrXT, a novel tool that computes the differences in the classification logic of two distinct trained models, reasoning on their symbolic representation through Binary Decision Diagrams. ContrXT is available as a pip package and API.","2022-07","2023-05-08 18:28:32","2023-05-23 01:35:12","2023-05-08 18:28:32","5936-5939","","","","","","The Good, the Bad, and the Explainer","","","","","International Joint Conferences on Artificial Intelligence Organization","Vienna, Austria","en","","","","","DOI.org (Crossref)","","1 citations (Crossref) [2023-05-09]","","C:\Users\ambreen.hanif\Zotero\storage\HEFAKP6F\Malandri et al. - 2022 - The Good, the Bad, and the Explainer A Tool for C.pdf; ","notion://www.notion.so/Malandri-et-al-2022-2c7e0fa818e54e8594ae44789d277cce","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Thirty-First International Joint Conference on Artificial Intelligence {IJCAI-22}","","","","","","","","","","","","","","",""
"P848LSNZ","journalArticle","2022","Liu, Zhifeng; Zhong, Xianzhan; Zhou, Conghua","Personalized Relationships-Based Knowledge Graph for Recommender Systems with Dual-View Items","Symmetry","","2073-8994","10.3390/sym14112386","https://www.mdpi.com/2073-8994/14/11/2386","The knowledge graph has received a lot of interest in the field of recommender systems as side information because it can address the sparsity and cold start issues associated with collaborative filtering-based recommender systems. However, when incorporating entities from a knowledge graph to represent semantic information, most current KG-based recommendation methods are unaware of the relationships between these users and items. As such, the learned semantic information representation of users and items cannot fully reflect the connectivity between users and items. In this paper, we present the PRKG-DI symmetry model, a Personalized Relationships-based Knowledge Graph for recommender systems with Dual-view Items that explores user-item relatedness by mining associated entities in the KG from user-oriented entity view and item-oriented entity view to augment item semantic information. Specifically, PRKG-DI utilizes a heterogeneous propagation strategy to gather information on higher-order user-item interactions and an attention mechanism to generate the weighted representation of entities. Moreover, PRKG-DI provides a score feature as a filter for individualized relationships to evaluate users’ potential interests. The empirical results demonstrate that our approach significantly outperforms several state-of-the-art baselines by 1.6%, 2.1%, and 0.8% on AUC, and 1.8%, 2.3%, and 0.8% on F1 when applied to three real-world scenarios for music, movie, and book recommendations, respectively.","2022-11","2023-05-08 07:34:39","2023-05-23 01:35:10","2023-05-08 07:34:39","2386","","11","14","","","","","","","","","","en","http://creativecommons.org/licenses/by/3.0/","","","","www.mdpi.com","","0 citations (Crossref) [2023-05-08] Number: 11 Publisher: Multidisciplinary Digital Publishing Institute","","C:\Users\ambreen.hanif\Zotero\storage\4FU7UEWJ\Liu et al_2022_Personalized Relationships-Based Knowledge Graph for Recommender Systems with.pdf; ","notion://www.notion.so/Liu-et-al-2022-bb8961d91cd14a5ab5ef19d9bdfe1405","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2ZB46KDV","journalArticle","2018","Gyrard, Amelie; Gaur, Manas; Shekarpour, Saeedeh; Thirunarayan, Krishnaprasad; Sheth, Amit","Personalized Health Knowledge Graph","CEUR workshop proceedings","","1613-0073","","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8532078/","Our current health applications do not adequately take into account contextual and personalized knowledge about patients. In order to design “Personalized Coach for Healthcare” applications to manage chronic diseases, there is a need to create a Personalized Healthcare Knowledge Graph (PHKG) that takes into consideration a patient’s health condition (personalized knowledge) and enriches that with contextualized knowledge from environmental sensors and Web of Data (e.g., symptoms and treatments for diseases). To develop PHKG, aggregating knowledge from various heterogeneous sources such as the Internet of Things (IoT) devices, clinical notes, and Electronic Medical Records (EMRs) is necessary. In this paper, we explain the challenges of collecting, managing, analyzing, and integrating patients’ health data from various sources in order to synthesize and deduce meaningful information embodying the vision of the Data, Information, Knowledge, and Wisdom (DIKW) pyramid. Furthermore, we sketch a solution that combines: 1) IoT data analytics, and 2) explicit knowledge and illustrate it using three chronic disease use cases – asthma, obesity, and Parkinson’s.","2018-10","2023-05-08 07:33:13","2023-05-23 01:35:08","2023-05-08 07:33:13","5","","","2317","","CEUR Workshop Proc","","","","","","","","","","","","","PubMed Central","","PMID: 34690624 PMCID: PMC8532078","","C:\Users\ambreen.hanif\Zotero\storage\AFAEZ8X3\Gyrard et al_2018_Personalized Health Knowledge Graph.pdf; ; ","notion://www.notion.so/Gyrard-et-al-2018-a7d13a8cab4c437e948338d0fd84da05; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8532078/","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PMCVGP9C","journalArticle","2022","Alicioglu, Gulsum; Sun, Bo","A survey of visual analytics for Explainable Artificial Intelligence methods","Computers & Graphics","","0097-8493","10.1016/j.cag.2021.09.002","https://www.sciencedirect.com/science/article/pii/S0097849321001886","Deep learning (DL) models have achieved impressive performance in various domains such as medicine, finance, and autonomous vehicle systems with advances in computing power and technologies. However, due to the black-box structure of DL models, the decisions of these learning models often need to be explained to end-users. Explainable Artificial Intelligence (XAI) provides explanations of black-box models to reveal the behavior and underlying decision-making mechanisms of the models through tools, techniques, and algorithms. Visualization techniques help to present model and prediction explanations in a more understandable, explainable, and interpretable way. This survey paper aims to review current trends and challenges of visual analytics in interpreting DL models by adopting XAI methods and present future research directions in this area. We reviewed literature based on two different aspects, model usage and visual approaches. We addressed several research questions based on our findings and then discussed missing points, research gaps, and potential future research directions. This survey provides guidelines to develop a better interpretation of neural networks through XAI methods in the field of visual analytics.","2022-02-01","2023-05-05 06:58:08","2023-05-23 01:35:04","2023-05-05 06:58:05","502-520","","","102","","Computers & Graphics","","","","","","","","en","","","","","ScienceDirect","","28 citations (Crossref) [2023-05-05]","","; C:\Users\ambreen.hanif\Zotero\storage\B9ICFY8D\S0097849321001886.html","notion://www.notion.so/Alicioglu-Sun-2022-021ece1bc46d4e4394a7bd6f97c72bd2","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KDS5RT7V","journalArticle","","Ahmed, Mobyen Uddin; Barua, Shaibal; Begum, Shahina; Islam, Mir Riyanul; Weber, Rosina O","When a CBR in Hand is Better than Twins in the Bush","","","","","","AI methods referred to as interpretable are often discredited as inaccurate by supporters of the existence of a trade-off between interpretability and accuracy. In many problem contexts however this trade-off does not hold. This paper discusses a regression problem context to predict flight take-off delays where the most accurate data regression model was trained via the XGBoost implementation of gradient boosted decision trees. While building an XGB-CBR Twin and converting the XGBoost feature importance into global weights in the CBR model, the resultant CBR model alone provides the most accurate local prediction, maintains the global importance to provide a global explanation of the model, and offers the most interpretable representation for local explanations. This resultant CBR model becomes a benchmark of accuracy and interpretability for this problem context, and hence it is used to evaluate the two additive feature attribute methods SHAP and LIME to explain the XGBoost regression model. The results with respect to local accuracy and feature attribution lead to potentially valuable future work.","","2023-05-05 06:34:52","2023-05-23 01:35:02","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\JIJH4K95\Ahmed et al. - When a CBR in Hand is Better than Twins in the Bus.pdf; ","notion://www.notion.so/Ahmed-et-al-n-d-2c8b4b3a72184416baffa61b3e559be0","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6QMK648Z","journalArticle","2022","Ahmed, Imran; Jeon, Gwanggil; Piccialli, Francesco","From Artificial Intelligence to Explainable Artificial Intelligence in Industry 4.0: A Survey on What, How, and Where","IEEE Transactions on Industrial Informatics","","1941-0050","10.1109/TII.2022.3146552","","Nowadays, Industry 4.0 can be considered a reality, a paradigm integrating modern technologies and innovations. Artificial intelligence (AI) can be considered the leading component of the industrial transformation enabling intelligent machines to execute tasks autonomously such as self-monitoring, interpretation, diagnosis, and analysis. AI-based methodologies (especially machine learning and deep learning support manufacturers and industries in predicting their maintenance needs and reducing downtime. Explainable artificial intelligence (XAI) studies and designs approaches, algorithms and tools producing human-understandable explanations of AI-based systems information and decisions. This article presents a comprehensive survey of AI and XAI-based methods adopted in the Industry 4.0 scenario. First, we briefly discuss different technologies enabling Industry 4.0. Then, we present an in-depth investigation of the main methods used in the literature: we also provide the details of what, how, why, and where these methods have been applied for Industry 4.0. Furthermore, we illustrate the opportunities and challenges that elicit future research directions toward responsible or human-centric AI and XAI systems, essential for adopting high-stakes industry applications.","2022-08","2023-05-05 06:36:09","2023-05-23 01:35:01","","5031-5042","","8","18","","","From Artificial Intelligence to Explainable Artificial Intelligence in Industry 4.0","","","","","","","","","","","","IEEE Xplore","","54 citations (Crossref) [2023-05-05] Conference Name: IEEE Transactions on Industrial Informatics","","C:\Users\ambreen.hanif\Zotero\storage\ZRQ3EXW9\9695219.html; ","notion://www.notion.so/Ahmed-et-al-2022-6cf3956daa564c42a99a7479546c65e4","notion","Artificial intelligence; Artificial intelligence (AI); cloud computing; cyber-physical system; explainable artificial intelligence (XAI); Fourth Industrial Revolution; Hidden Markov models; Industries; Industry 4.0; Internet of Things (IoT); Manufacturing; Robots; Service robots","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NW2BZYMQ","journalArticle","2021","Sheth, Amit; Gaur, Manas; Roy, Kaushik; Faldu, Keyur","Knowledge-Intensive Language Understanding for Explainable AI","IEEE Internet Computing","","1941-0131","10.1109/MIC.2021.3101919","","AI systems have seen significant adoption in various domains. At the same time, further adoption in some domains is hindered by the inability to fully trust an AI system that it will not harm a human. Besides, fairness, privacy, transparency, and explainability are vital to developing trust in AI systems. As stated in Describing Trustworthy AI,aa.https://www.ibm.com/watson/trustworthy-ai. “Trust comes through understanding. How AI-led decisions are made and what determining factors were included are crucial to understand.” The subarea of explaining AI systems has come to be known as XAI. Multiple aspects of an AI system can be explained; these include biases that the data might have, lack of data points in a particular region of the example space, fairness of gathering the data, feature importances, etc. However, besides these, it is critical to have human-centered explanations directly related to decision-making, similar to how a domain expert makes decisions based on “domain knowledge,” including well-established, peer-validated explicit guidelines. To understand and validate an AI system's outcomes (such as classification, recommendations, predictions) that lead to developing trust in the AI system, it is necessary to involve explicit domain knowledge that humans understand and use. Contemporary XAI methods are yet addressed explanations that enable decision-making similar to an expert. Figure 1 shows the stages of adoption of an AI system into the real world.","2021-09","2023-05-05 00:45:43","2023-05-23 01:34:58","","19-24","","5","25","","","","","","","","","","","","","","","IEEE Xplore","","6 citations (Crossref) [2023-05-05] Conference Name: IEEE Internet Computing","","C:\Users\ambreen.hanif\Zotero\storage\BQEDXQA5\9514440.html; ; C:\Users\ambreen.hanif\Zotero\storage\DHVK3CJI\Sheth et al. - 2021 - Knowledge-Intensive Language Understanding for Exp.pdf","notion://www.notion.so/Sheth-et-al-2021-7b52d567ee0149c2a6ee59e99060c794","notion","Computational modeling; Deep learning; Task analysis; Artificial intelligence; Decision making; Human computer interaction; Knowledge discovery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8P26QYQC","preprint","2019","Gilpin, Leilani H.; Bau, David; Yuan, Ben Z.; Bajwa, Ayesha; Specter, Michael; Kagal, Lalana","Explaining Explanations: An Overview of Interpretability of Machine Learning","","","","","http://arxiv.org/abs/1806.00069","There has recently been a surge of work in explanatory artiﬁcial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our deﬁnition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufﬁcient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artiﬁcial intelligence.","2019-02-03","2023-05-04 06:03:29","2023-05-23 01:34:55","2023-05-04 06:03:29","","","","","","","Explaining Explanations","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1806.00069 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\2SX6X88L\Gilpin et al. - 2019 - Explaining Explanations An Overview of Interpreta.pdf; ","notion://www.notion.so/Gilpin-et-al-2019-1179f196eb9d4a4e95b02a0beb6ff506","notion","","","","","","","","","","","","","","","","","","","","arXiv:1806.00069","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZVIYRZAV","preprint","2020","Chari, Shruthi; Seneviratne, Oshani; Gruen, Daniel M.; Foreman, Morgan A.; Das, Amar K.; McGuinness, Deborah L.","Explanation Ontology: A Model of Explanations for User-Centered AI","","","","10.48550/arXiv.2010.01479","http://arxiv.org/abs/2010.01479","Explainability has been a goal for Artificial Intelligence (AI) systems since their conception, with the need for explainability growing as more complex AI models are increasingly used in critical, high-stakes settings such as healthcare. Explanations have often added to an AI system in a non-principled, post-hoc manner. With greater adoption of these systems and emphasis on user-centric explainability, there is a need for a structured representation that treats explainability as a primary consideration, mapping end user needs to specific explanation types and the system's AI capabilities. We design an explanation ontology to model both the role of explanations, accounting for the system and user attributes in the process, and the range of different literature-derived explanation types. We indicate how the ontology can support user requirements for explanations in the domain of healthcare. We evaluate our ontology with a set of competency questions geared towards a system designer who might use our ontology to decide which explanation types to include, given a combination of users' needs and a system's capabilities, both in system design settings and in real-time operations. Through the use of this ontology, system designers will be able to make informed choices on which explanations AI systems can and should provide.","2020-10-03","2023-05-04 06:02:17","2023-05-23 01:34:52","2023-05-04 06:02:17","","","","","","","Explanation Ontology","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2010.01479 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\PL2JZD96\2010.html; C:\Users\ambreen.hanif\Zotero\storage\NDVH4X8F\Chari et al_2020_Explanation Ontology.pdf; ","notion://www.notion.so/Chari-et-al-2020-16b48f3cfa214608b4eac88e3681c0e2","notion","","","","","","","","","","","","","","","","","","","","arXiv:2010.01479","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SGU7FA6T","presentation","","Weber, Rosina","XAI ROADBLOCKS, TRENDS, AND DIRECTIONS","","","","","","","","2023-05-04 02:06:50","2023-05-23 01:34:31","","","","","","","","","","","","","","","en","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\2YYCWD7Y\Weber - XAI ROADBLOCKS, TRENDS, AND DIRECTIONS.pdf","notion://www.notion.so/Weber-n-d-51fd3153dcbf42d2bd16c3fb38c28fb2","notion; direction; read; trends; xai","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C6UFSQ33","webpage","","","Principal component analysis","Prince","","","","https://maxhalford.github.io/prince/pca/","Resources Principal component analysis by Hervé Abdi and Lynne J. Williams is excellent at explaining PCA interpretation. It also covers some extensions to PCA. A Tutorial on Principal Component Analysis by Jonathon Shlens goes into more detail on the intuition behind PCA, while also discussing its applicability and limits. I learnt PCA from these lecture notes from Xavier Gendre. He provides a comprehensive walkthrough using a dataset of Skyrim bows. Data PCA assumes you have a dataframe consisting of numerical variables.","","2023-05-04 00:32:27","2023-05-23 01:34:29","2023-05-04 00:32:27","","","","","","","","","","","","","","en-us","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\WZV9DD7J\pca.html","notion://www.notion.so/Principal-Component-Analysis-n-d-180d1167f87e47ed90d6b333b0858ef7","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JTVZJC62","journalArticle","2010","Abdi, Hervé; Williams, Lynne J.","Principal component analysis: Principal component analysis","Wiley Interdisciplinary Reviews: Computational Statistics","","19395108","10.1002/wics.101","https://onlinelibrary.wiley.com/doi/10.1002/wics.101","","2010-07","2023-05-04 00:07:53","2023-05-23 01:34:27","2023-05-04 00:07:53","433-459","","4","2","","WIREs Comp Stat","Principal component analysis","","","","","","","en","","","","","DOI.org (Crossref)","","5275 citations (Crossref) [2023-05-04]","","C:\Users\ambreen.hanif\Zotero\storage\WQS4HEJZ\Abdi and Williams - 2010 - Principal component analysis Principal component .pdf; ","notion://www.notion.so/Abdi-Williams-2010-b31ebdb299d741d3aadfac030215bda7","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"284QYTXW","preprint","2014","Shlens, Jonathon","A Tutorial on Principal Component Analysis","","","","","http://arxiv.org/abs/1404.1100","Principal component analysis (PCA) is a mainstay of modern data analysis - a black box that is widely used but (sometimes) poorly understood. The goal of this paper is to dispel the magic behind this black box. This manuscript focuses on building a solid intuition for how and why principal component analysis works. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind PCA. This tutorial does not shy away from explaining the ideas informally, nor does it shy away from the mathematics. The hope is that by addressing both aspects, readers of all levels will be able to gain a better understanding of PCA as well as the when, the how and the why of applying this technique.","2014-04-03","2023-05-03 23:22:13","2023-05-23 01:34:24","2023-05-03 23:22:13","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1404.1100 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\GM4JBHC4\1404.html; ; C:\Users\ambreen.hanif\Zotero\storage\GN3A6QXD\Shlens_2014_A Tutorial on Principal Component Analysis.pdf","notion://www.notion.so/Shlens-2014-90e55979c77044a78e4cc52f843448fa","notion","","","","","","","","","","","","","","","","","","","","arXiv:1404.1100","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EJPVMADZ","journalArticle","","Shlens, Jonathon","A Tutorial on Principal Component Analysis","","","","","","","","2023-05-03 06:06:22","2023-05-23 01:34:22","","","","","","","","","","","","","","","en","","","","","Zotero","","","","; C:\Users\ambreen.hanif\Zotero\storage\YXIUGDAW\Shlens - A Tutorial on Principal Component Analysis.pdf","notion://www.notion.so/Shlens-n-d-03ee726a4b4343b5a23d80c608225c33","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZV2PXBMJ","journalArticle","2021","Wang, Jianyong; Chen, Nan; Guo, Jixiang; Xu, Xiuyuan; Liu, Lunxu; Yi, Zhang","SurvNet: A Novel Deep Neural Network for Lung Cancer Survival Analysis With Missing Values","Frontiers in Oncology","","2234-943X","","https://www.frontiersin.org/articles/10.3389/fonc.2020.588990","Survival analysis is important for guiding further treatment and improving lung cancer prognosis. It is a challenging task because of the poor distinguishability of features and the missing values in practice. A novel multi-task based neural network, SurvNet, is proposed in this paper. The proposed SurvNet model is trained in a multi-task learning framework to jointly learn across three related tasks: input reconstruction, survival classification, and Cox regression. It uses an input reconstruction mechanism cooperating with incomplete-aware reconstruction loss for latent feature learning of incomplete data with missing values. Besides, the SurvNet model introduces a context gating mechanism to bridge the gap between survival classification and Cox regression. A new real-world dataset of 1,137 patients with IB-IIA stage non-small cell lung cancer is collected to evaluate the performance of the SurvNet model. The proposed SurvNet achieves a higher concordance index than the traditional Cox model and Cox-Net. The difference between high-risk and low-risk groups obtained by SurvNet is more significant than that of high-risk and low-risk groups obtained by the other models. Moreover, the SurvNet outperforms the other models even though the input data is randomly cropped and it achieves better generalization performance on the Surveillance, Epidemiology, and End Results Program (SEER) dataset.","2021","2023-05-02 23:57:50","2023-05-23 01:34:18","2023-05-02 23:57:50","","","","10","","","SurvNet","","","","","","","","","","","","Frontiers","","","","; C:\Users\ambreen.hanif\Zotero\storage\5XW98YHM\Wang et al_2021_SurvNet.pdf","notion://www.notion.so/Wang-et-al-2021-22eb90d67f2e4cf3884fb651fbe85f10","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FC6UKPFR","book","","Sestelo, Marta","A short course on Survival Analysis applied to the Financial Industry","","","","","https://bookdown.org/sestelo/sa_financial/intro-censor.html","This is a short course on survival analysis applied to the financial field.","","2023-05-02 02:18:40","2023-05-23 01:34:15","2023-05-02 02:18:40","","","","","","","","","","","","","","","","","","","bookdown.org","","","","; C:\Users\ambreen.hanif\Zotero\storage\EW9LJS48\intro-censor.html","notion://www.notion.so/Sestelo-n-d-6d28b865639645e9beefacf3af803233","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FHM5BVDJ","preprint","2023","Jacovi, Alon","Trends in Explainable AI (XAI) Literature","","","","","http://arxiv.org/abs/2301.05433","The XAI literature is decentralized, both in terminology and in publication venues, but recent years saw the community converge around keywords that make it possible to more reliably discover papers automatically. We use keyword search using the SemanticScholar API and manual curation to collect a well-formatted and reasonably comprehensive set of 5199 XAI papers, available at https://github.com/alonjacovi/XAI-Scholar . We use this collection to clarify and visualize trends about the size and scope of the literature, citation trends, cross-field trends, and collaboration trends. Overall, XAI is becoming increasingly multidisciplinary, with relative growth in papers belonging to increasingly diverse (non-CS) scientific fields, increasing cross-field collaborative authorship, increasing cross-field citation activity. The collection can additionally be used as a paper discovery engine, by retrieving XAI literature which is cited according to specific constraints (for example, papers that are influential outside of their field, or influential to non-XAI research).","2023-01-13","2023-05-02 01:40:32","2023-05-23 01:34:13","2023-05-02 01:40:32","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2301.05433 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\FZJSXMG6\2301.html; C:\Users\ambreen.hanif\Zotero\storage\Y48T58EF\Jacovi_2023_Trends in Explainable AI (XAI) Literature.pdf; ","notion://www.notion.so/Jacovi-2023-7f69f71f74494b05b6defec03cd4755a","notion","","","","","","","","","","","","","","","","","","","","arXiv:2301.05433","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WCQT62T5","journalArticle","2022","Meske, Christian; Bunde, Enrico; Schneider, Johannes; Gersch, Martin","Explainable Artificial Intelligence: Objectives, Stakeholders, and Future Research Opportunities","Information Systems Management","","1058-0530","10.1080/10580530.2020.1849465","https://doi.org/10.1080/10580530.2020.1849465","Artificial Intelligence (AI) has diffused into many areas of our private and professional life. In this research note, we describe exemplary risks of black-box AI, the consequent need for explainability, and previous research on Explainable AI (XAI) in information systems research. Moreover, we discuss the origin of the term XAI, generalized XAI objectives, and stakeholder groups, as well as quality criteria of personalized explanations. We conclude with an outlook to future research on XAI.","2022-01-02","2023-05-02 01:38:04","2023-05-23 01:34:11","2023-05-02 01:38:04","53-63","","1","39","","","Explainable Artificial Intelligence","","","","","","","","","","","","Taylor and Francis+NEJM","","72 citations (Crossref) [2023-05-02] Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/10580530.2020.1849465","","C:\Users\ambreen.hanif\Zotero\storage\CIPWR93M\Meske et al_2022_Explainable Artificial Intelligence.pdf; ","notion://www.notion.so/Meske-et-al-2022-899895756f4844c0b3be128bc679fde1","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H978ICW4","journalArticle","2021","Confalonieri, Roberto; Coba, Ludovik; Wagner, Benedikt; Besold, Tarek R.","A historical perspective of explainable Artificial Intelligence","WIREs Data Mining and Knowledge Discovery","","1942-4795","10.1002/widm.1391","https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1391","Explainability in Artificial Intelligence (AI) has been revived as a topic of active research by the need of conveying safety and trust to users in the “how” and “why” of automated decision-making in different applications such as autonomous driving, medical diagnosis, or banking and finance. While explainability in AI has recently received significant attention, the origins of this line of work go back several decades to when AI systems were mainly developed as (knowledge-based) expert systems. Since then, the definition, understanding, and implementation of explainability have been picked up in several lines of research work, namely, expert systems, machine learning, recommender systems, and in approaches to neural-symbolic learning and reasoning, mostly happening during different periods of AI history. In this article, we present a historical perspective of Explainable Artificial Intelligence. We discuss how explainability was mainly conceived in the past, how it is understood in the present and, how it might be understood in the future. We conclude the article by proposing criteria for explanations that we believe will play a crucial role in the development of human-understandable explainable systems. This article is categorized under: Fundamental Concepts of Data and Knowledge > Explainable AI Technologies > Artificial Intelligence","2021","2023-05-01 06:15:04","2023-05-23 01:34:10","2023-05-01 06:15:04","e1391","","1","11","","","","","","","","","","en","","","","","Wiley Online Library","","57 citations (Crossref) [2023-05-01] _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1391","","C:\Users\ambreen.hanif\Zotero\storage\8MAPHC3H\Confalonieri et al_2021_A historical perspective of explainable Artificial Intelligence.pdf; ; C:\Users\ambreen.hanif\Zotero\storage\7P2729P2\widm.html","notion://www.notion.so/Confalonieri-et-al-2021-6b8b63e05cfd42f6bf685e29ab0964f1","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7R7G2GSR","journalArticle","2023","Yue, Thomas; Au, David; Au, Chi Chung; Iu, Kwan Yuen","Democratizing Financial Knowledge with ChatGPT by OpenAI: Unleashing the Power of Technology","SSRN Electronic Journal","","1556-5068","10.2139/ssrn.4346152","https://www.ssrn.com/abstract=4346152","","2023","2023-04-28 00:33:32","2023-05-23 01:34:08","2023-04-28 00:33:31","","","","","","SSRN Journal","Democratizing Financial Knowledge with ChatGPT by OpenAI","","","","","","","en","","","","","DOI.org (Crossref)","","1 citations (Crossref) [2023-04-28]","","","notion://www.notion.so/Yue-et-al-2023-0c5d77dcb63f4220a25d7224063fb664","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XP4UIPX4","journalArticle","","Schelter, Sebastian; Böse, Joos-Hendrik; Kirschnick, Johannes; Klein, Thoralf; Seufert, Stephan","Automatically Tracking Metadata and Provenance of Machine Learning Experiments","","","","","","We present a lightweight system to extract, store and manage metadata and provenance information of common artifacts in machine learning (ML) experiments: datasets, models, predictions, evaluations and training runs. Our system accelerates users in their ML workﬂow, and provides a basis for comparability and repeatability of ML experiments. We achieve this by tracking the lineage of produced artifacts and automatically extracting metadata such as hyperparameters of models, schemas of datasets or layouts of deep neural networks. Our system provides a general declarative representation of said ML artifacts, is integrated with popular frameworks such as MXNet, SparkML and scikit-learn, and meets the demands of various production use cases at Amazon.","","2023-04-26 06:12:14","2023-05-23 01:34:07","","","","","","","","","","","","","","","en","","","","","Zotero","","","","; C:\Users\ambreen.hanif\Zotero\storage\9PL8LHDE\Schelter et al. - Automatically Tracking Metadata and Provenance of .pdf","notion://www.notion.so/Schelter-et-al-n-d-cd120663651248dfa265357efc06f9a5","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"62E5B437","webpage","","","Automatically tracking metadata and provenance of machine learning experiments","Amazon Science","","","","https://www.amazon.science/publications/automatically-tracking-metadata-and-provenance-of-machine-learning-experiments","We present a lightweight system to extract, store and manage metadata and provenance information of common artifacts in machine learning (ML) experiments: datasets, models, predictions, evaluations and training runs. Our system accelerates users in their ML workflow, and provides a basis for…","","2023-04-26 06:11:54","2023-05-23 01:34:05","2023-04-26 06:11:54","","","","","","","","","","","","","","en","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\AI39WJTB\automatically-tracking-metadata-and-provenance-of-machine-learning-experiments.html","notion://www.notion.so/Automatically-Tracking-Metadata-and-Provenance-of-Machine-Learning-Experiments-n-d-56d504e1cbf84170a447ff84e158b4e5","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XBLKA86W","preprint","2023","Xu, Mingxue; Li, Xiang-Yang","Data Origin Inference in Machine Learning","","","","10.48550/arXiv.2211.13416","http://arxiv.org/abs/2211.13416","It is a growing direction to utilize unintended memorization in ML models to benefit real-world applications, with recent efforts like user auditing, dataset ownership inference and forgotten data measurement. Standing on the point of ML model development, we introduce a process named data origin inference, to assist ML developers in locating missed or faulty data origin in training set without maintaining strenuous metadata. We formally define the data origin and the data origin inference task in the development of the ML model (mainly neural networks). Then we propose a novel inference strategy combining embedded-space multiple instance classification and shadow training. Diverse use cases cover language, visual and structured data, with various kinds of data origin (e.g. business, county, movie, mobile user, text author). A comprehensive performance analysis of our proposed strategy contains referenced target model layers, available testing data for each origin, and in shadow training, the implementations of feature extraction as well as shadow models. Our best inference accuracy achieves 98.96% in the language use case when the target model is a transformer-based deep neural network. Furthermore, we give a statistical analysis of different kinds of data origin to investigate what kind of origin is probably to be inferred correctly.","2023-01-29","2023-04-26 06:07:33","2023-05-23 01:34:03","2023-04-26 06:07:33","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2211.13416 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\JSGUF654\2211.html; ; C:\Users\ambreen.hanif\Zotero\storage\6VRL6MBP\Xu_Li_2023_Data Origin Inference in Machine Learning.pdf","notion://www.notion.so/Xu-Li-2023-deacc7790af3464685ffafd4b3503c17","notion","","","","","","","","","","","","","","","","","","","","arXiv:2211.13416","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VG8UPNZA","preprint","2017","Ross, Andrew Slavin; Hughes, Michael C.; Doshi-Velez, Finale","Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations","","","","10.48550/arXiv.1703.03717","http://arxiv.org/abs/1703.03717","Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.","2017-05-25","2023-04-26 04:44:07","2023-05-23 01:34:01","2023-04-26 04:44:07","","","","","","","Right for the Right Reasons","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1703.03717 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\DTQ3PFW5\1703.html; ; C:\Users\ambreen.hanif\Zotero\storage\7TXNWK96\Ross et al_2017_Right for the Right Reasons.pdf","notion://www.notion.so/Ross-et-al-2017-a8049ae70bc24846856b21aef97e81ba","notion","","","","","","","","","","","","","","","","","","","","arXiv:1703.03717","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MSN2EB5J","preprint","2017","Alvarez-Melis, David; Jaakkola, Tommi S.","A causal framework for explaining the predictions of black-box sequence-to-sequence models","","","","10.48550/arXiv.1707.01943","http://arxiv.org/abs/1707.01943","We interpret the predictions of any black-box structured input-structured output model around a specific input-output pair. Our method returns an ""explanation"" consisting of groups of input-output tokens that are causally related. These dependencies are inferred by querying the black-box model with perturbed inputs, generating a graph over tokens from the responses, and solving a partitioning problem to select the most relevant components. We focus the general approach on sequence-to-sequence problems, adopting a variational autoencoder to yield meaningful input perturbations. We test our method across several NLP sequence generation tasks.","2017-11-14","2023-04-26 04:22:57","2023-05-23 01:33:58","2023-04-26 04:22:57","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1707.01943 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\A5TA5UL2\Alvarez-Melis_Jaakkola_2017_A causal framework for explaining the predictions of black-box.pdf; C:\Users\ambreen.hanif\Zotero\storage\APY4D6K7\1707.html; ","notion://www.notion.so/Alvarez-Melis-Jaakkola-2017-3cc12ec51b574f2a9e319f6f7b239cf5","notion","","","","","","","","","","","","","","","","","","","","arXiv:1707.01943","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JL6XAPBB","preprint","2018","Lee, Guang-He; Alvarez-Melis, David; Jaakkola, Tommi S.","Game-Theoretic Interpretability for Temporal Modeling","","","","10.48550/arXiv.1807.00130","http://arxiv.org/abs/1807.00130","Interpretability has arisen as a key desideratum of machine learning models alongside performance. Approaches so far have been primarily concerned with fixed dimensional inputs emphasizing feature relevance or selection. In contrast, we focus on temporal modeling and the problem of tailoring the predictor, functionally, towards an interpretable family. To this end, we propose a co-operative game between the predictor and an explainer without any a priori restrictions on the functional class of the predictor. The goal of the explainer is to highlight, locally, how well the predictor conforms to the chosen interpretable family of temporal models. Our co-operative game is setup asymmetrically in terms of information sets for efficiency reasons. We develop and illustrate the framework in the context of temporal sequence models with examples.","2018-06-30","2023-04-26 04:22:48","2023-05-23 01:33:56","2023-04-26 04:22:48","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1807.00130 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\6FIXAAC7\1807.html; C:\Users\ambreen.hanif\Zotero\storage\Z4PW2UI4\Lee et al_2018_Game-Theoretic Interpretability for Temporal Modeling.pdf; ","notion://www.notion.so/Lee-et-al-2018-08e65b57f50d4a44b49f54e5c652f5a0","notion","","","","","","","","","","","","","","","","","","","","arXiv:1807.00130","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4Y5SDNVA","preprint","2019","Lee, Guang-He; Jin, Wengong; Alvarez-Melis, David; Jaakkola, Tommi S.","Functional Transparency for Structured Data: a Game-Theoretic Approach","","","","10.48550/arXiv.1902.09737","http://arxiv.org/abs/1902.09737","We provide a new approach to training neural models to exhibit transparency in a well-defined, functional manner. Our approach naturally operates over structured data and tailors the predictor, functionally, towards a chosen family of (local) witnesses. The estimation problem is setup as a co-operative game between an unrestricted predictor such as a neural network, and a set of witnesses chosen from the desired transparent family. The goal of the witnesses is to highlight, locally, how well the predictor conforms to the chosen family of functions, while the predictor is trained to minimize the highlighted discrepancy. We emphasize that the predictor remains globally powerful as it is only encouraged to agree locally with locally adapted witnesses. We analyze the effect of the proposed approach, provide example formulations in the context of deep graph and sequence models, and empirically illustrate the idea in chemical property prediction, temporal modeling, and molecule representation learning.","2019-02-26","2023-04-26 04:22:02","2023-05-23 01:33:54","2023-04-26 04:22:01","","","","","","","Functional Transparency for Structured Data","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1902.09737 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\43TYW8K5\1902.html; C:\Users\ambreen.hanif\Zotero\storage\2YNWDFN4\Lee et al_2019_Functional Transparency for Structured Data.pdf; ","notion://www.notion.so/Lee-et-al-2019-efd8ed718e614977a0fda76d032e5d3d","notion","","","","","","","","","","","","","","","","","","","","arXiv:1902.09737","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"375BU6UC","preprint","2019","Lee, Guang-He; Alvarez-Melis, David; Jaakkola, Tommi S.","Towards Robust, Locally Linear Deep Networks","","","","10.48550/arXiv.1907.03207","http://arxiv.org/abs/1907.03207","Deep networks realize complex mappings that are often understood by their locally linear behavior at or around points of interest. For example, we use the derivative of the mapping with respect to its inputs for sensitivity analysis, or to explain (obtain coordinate relevance for) a prediction. One key challenge is that such derivatives are themselves inherently unstable. In this paper, we propose a new learning problem to encourage deep networks to have stable derivatives over larger regions. While the problem is challenging in general, we focus on networks with piecewise linear activation functions. Our algorithm consists of an inference step that identifies a region around a point where linear approximation is provably stable, and an optimization step to expand such regions. We propose a novel relaxation to scale the algorithm to realistic models. We illustrate our method with residual and recurrent networks on image and sequence datasets.","2019-07-06","2023-04-26 04:21:54","2023-05-23 01:33:53","2023-04-26 04:21:54","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1907.03207 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\SSZHDX2C\1907.html; C:\Users\ambreen.hanif\Zotero\storage\TLX8QRRH\Lee et al_2019_Towards Robust, Locally Linear Deep Networks.pdf; ","notion://www.notion.so/Lee-et-al-2019-9335cd8e29bd493a891a75505b218777","notion","","","","","","","","","","","","","","","","","","","","arXiv:1907.03207","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MWPD3RNJ","preprint","2019","Alvarez-Melis, David; Daumé III, Hal; Vaughan, Jennifer Wortman; Wallach, Hanna","Weight of Evidence as a Basis for Human-Oriented Explanations","","","","10.48550/arXiv.1910.13503","http://arxiv.org/abs/1910.13503","Interpretability is an elusive but highly sought-after characteristic of modern machine learning methods. Recent work has focused on interpretability via $\textit{explanations}$, which justify individual model predictions. In this work, we take a step towards reconciling machine explanations with those that humans produce and prefer by taking inspiration from the study of explanation in philosophy, cognitive science, and the social sciences. We identify key aspects in which these human explanations differ from current machine explanations, distill them into a list of desiderata, and formalize them into a framework via the notion of $\textit{weight of evidence}$ from information theory. Finally, we instantiate this framework in two simple applications and show it produces intuitive and comprehensible explanations.","2019-10-29","2023-04-26 04:21:47","2023-05-23 01:33:51","2023-04-26 04:21:47","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1910.13503 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\69BFTPR3\Alvarez-Melis et al_2019_Weight of Evidence as a Basis for Human-Oriented Explanations.pdf; C:\Users\ambreen.hanif\Zotero\storage\2IY495BP\1910.html; ","notion://www.notion.so/Alvarez-Melis-et-al-2019-930592cc0e14496d85ef49acadb7a501","notion","","","","","","","","","","","","","","","","","","","","arXiv:1910.13503","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HHQ9HKPI","preprint","2018","Alvarez-Melis, David; Jaakkola, Tommi S.","On the Robustness of Interpretability Methods","","","","10.48550/arXiv.1806.08049","http://arxiv.org/abs/1806.08049","We argue that robustness of explanations---i.e., that similar inputs should give rise to similar explanations---is a key desideratum for interpretability. We introduce metrics to quantify robustness and demonstrate that current methods do not perform well according to these metrics. Finally, we propose ways that robustness can be enforced on existing interpretability approaches.","2018-06-20","2023-04-26 04:20:04","2023-05-23 01:33:49","2023-04-26 04:20:04","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1806.08049 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\MWRNZ6AM\Alvarez-Melis_Jaakkola_2018_On the Robustness of Interpretability Methods.pdf; C:\Users\ambreen.hanif\Zotero\storage\32BAZWA9\1806.html; ","notion://www.notion.so/Alvarez-Melis-Jaakkola-2018-0d80ca7a65b5411983ead2c7116c1985","notion","","","","","","","","","","","","","","","","","","","","arXiv:1806.08049","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HDEKHQ2B","webpage","","","Thomson and Schoenherr (2020) - KITT Paper.pdf","Google Docs","","","","https://drive.google.com/file/d/17zvjVyb8oxhXAVIwiU7PRDzOEyWLCvlU/view?usp=embed_facebook","","","2023-04-26 03:09:54","2023-05-23 01:33:46","2023-04-26 03:09:54","","","","","","","","","","","","","","","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\GTF3RWIG\view.html; C:\Users\ambreen.hanif\Downloads\Thomson and Schoenherr (2020) - KITT Paper.pdf","notion://www.notion.so/Thomson-and-Schoenherr-2020-KITT-Paper-Pdf-n-d-865605b4686546908270f5f0c997654d","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NMCJDTNL","conferencePaper","2017","Wang, Yu-Xuan; Sun, QiHui; Chien, Ting-Ying; Huang, Po-Chun","Using data mining and machine learning techniques for system design space exploration and automatized optimization","2017 International Conference on Applied System Innovation (ICASI)","","","10.1109/ICASI.2017.7988179","","Recently, the significance of data mining and machine learning have been highlighted in diversified application scenarios. Various data mining and machine learning techniques are often used to analyze the gigantic amount of data to create more commercial values in high-end enterprise systems. However, the advancement of technologies has made data mining and machine learning possible on low-end systems, such as personal computers or embedded systems. While researchers have proposed excellent work on the management de-signs of different components of the system, most of the work are built upon the characteristics of the system, which may change from time to time. This makes it impossible to optimize the system performance with stat-ic, or statically adaptive, system designs. In this work, we propose to embed the supports of data mining and machine learning to the design of operating system, so as to discover a new, automatized way to adaptively optimize the system without using complex algorithms. To validate the proposed ideas, we choose the cache design as a case study, where the replacement of cached contents is automatically controlled by a decision maker. The decision maker then replies on a data miner, which analyzes the data collected by the system monitor. The efficacy of the considered case is verified by a series of experiments, where the results are quite encouraging.","2017-05","2023-04-21 05:57:32","2023-05-23 01:33:45","","1079-1082","","","","","","","","","","","","","","","","","","IEEE Xplore","","4 citations (Crossref) [2023-04-21]","","C:\Users\ambreen.hanif\Zotero\storage\ZVWIGZPM\7988179.html; ; C:\Users\ambreen.hanif\Zotero\storage\3JLRXZPR\Wang et al_2017_Using data mining and machine learning techniques for system design space.pdf","notion://www.notion.so/Wang-et-al-2017-e658470a7fbc44f8803f0429f094e42e","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2017 International Conference on Applied System Innovation (ICASI)","","","","","","","","","","","","","","",""
"U93VPWRB","journalArticle","2021","Linardatos, Pantelis; Papastefanopoulos, Vasilis; Kotsiantis, Sotiris","Explainable AI: A Review of Machine Learning Interpretability Methods","Entropy","","1099-4300","10.3390/e23010018","https://www.mdpi.com/1099-4300/23/1/18","Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.","2021-01","2023-04-21 05:55:32","2023-05-23 01:33:37","2023-04-21 05:55:32","18","","1","23","","","Explainable AI","","","","","","","en","http://creativecommons.org/licenses/by/3.0/","","","","www.mdpi.com","","462 citations (Crossref) [2023-04-21] Number: 1 Publisher: Multidisciplinary Digital Publishing Institute","","C:\Users\ambreen.hanif\Zotero\storage\54LBAXBQ\Linardatos et al_2021_Explainable AI.pdf; ","notion://www.notion.so/Linardatos-et-al-2021-3a85d70de4f64eb683e210148d0d5a64","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A54GBBZL","journalArticle","1994","Fu, LiMin","Rule generation from neural networks","IEEE Transactions on Systems, Man, and Cybernetics","","2168-2909","10.1109/21.299696","","The neural network approach has proven useful for the development of artificial intelligence systems. However, a disadvantage with this approach is that the knowledge embedded in the neural network is opaque. In this paper, we show how to interpret neural network knowledge in symbolic form. We lay dawn required definitions for this treatment, formulate the interpretation algorithm, and formally verify its soundness. The main result is a formalized relationship between a neural network and a rule-based system. In addition, it has been demonstrated that the neural network generates rules of better performance than the decision tree approach in noisy conditions.<>","1994-08","2023-04-21 05:51:10","2023-05-23 01:33:31","","1114-1124","","8","24","","","","","","","","","","","","","","","IEEE Xplore","","182 citations (Crossref) [2023-04-21] Conference Name: IEEE Transactions on Systems, Man, and Cybernetics","","C:\Users\ambreen.hanif\Zotero\storage\EPPQ6LJE\Fu_1994_Rule generation from neural networks.pdf; C:\Users\ambreen.hanif\Zotero\storage\GUKLBTU4\299696.html; ","notion://www.notion.so/Fu-1994-32fe1fac09114205b95c7d557edd2d7c","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PSRJQK4U","conferencePaper","2021","Jacovi, Alon; Marasović, Ana; Miller, Tim; Goldberg, Yoav","Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI","Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency","978-1-4503-8309-7","","10.1145/3442188.3445923","https://dl.acm.org/doi/10.1145/3442188.3445923","Trust is a central component of the interaction between people and AI, in that 'incorrect' levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in AI? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we promote them, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, interpersonal trust (i.e., trust between people) as defined by sociologists. This model rests on two key properties: the vulnerability of the user; and the ability to anticipate the impact of the AI model's decisions. We incorporate a formalization of 'contractual trust', such that trust between a user and an AI model is trust that some implicit or explicit contract will hold, and a formalization of 'trustworthiness' (that detaches from the notion of trustworthiness in sociology), and with it concepts of 'warranted' and 'unwarranted' trust. We present the possible causes of warranted trust as intrinsic reasoning and extrinsic behavior, and discuss how to design trustworthy AI, how to evaluate whether trust has manifested, and whether it is warranted. Finally, we elucidate the connection between trust and XAI using our formalization.","2021-03-01","2023-04-21 05:49:34","2023-05-23 01:33:26","2023-04-20","624–635","","","","","","Formalizing Trust in Artificial Intelligence","FAccT '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","85 citations (Crossref) [2023-04-21]","","C:\Users\ambreen.hanif\Zotero\storage\9BALPE2A\Jacovi et al_2021_Formalizing Trust in Artificial Intelligence.pdf; ","notion://www.notion.so/Jacovi-et-al-2021-05fb4431c27b42ff9cae6278a3b47fd9","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WQS59PQ5","webpage","","","Formalizing Trust in Artificial Intelligence | Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency","","","","","https://dl-acm-org.simsrad.net.ocs.mq.edu.au/doi/abs/10.1145/3442188.3445923","","","2023-04-21 05:49:13","2023-05-23 01:33:09","2023-04-21 05:49:13","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\Z34YLXMS\3442188.html; ","notion://www.notion.so/Formalizing-Trust-in-Artificial-Intelligence-Proceedings-of-the-2021-ACM-Conference-on-Fairness-A-83200ee4a84c45d493da4990f78be08a","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6V36EEUJ","journalArticle","2021","Bento, Vitor; Kohler, Manoela; Diaz, Pedro; Mendoza, Leonardo; Pacheco, Marco Aurelio","Improving deep learning performance by using Explainable Artificial Intelligence (XAI) approaches","Discover Artificial Intelligence","","2731-0809","10.1007/s44163-021-00008-y","https://doi.org/10.1007/s44163-021-00008-y","In this work we propose a workflow to deal with overlaid images—images with superimposed text and company logos—, which is very common in underwater monitoring videos and surveillance camera footage. It is demonstrated that it is possible to use Explaining Artificial Intelligence to improve deep learning models performance for image classification tasks in general. A deep learning model trained to classify metal surface defect, which previously had a low performance, is then evaluated with Layer-wise relevance propagation—an Explaining Artificial Intelligence technique—to identify problems in a dataset that hinder the training of deep learning models in a wide range of applications. Thereafter, it is possible to remove this unwanted information from the dataset—using different approaches: from cutting part of the images to training a Generative Inpainting neural network model—and retrain the model with the new preprocessed images. This proposed methodology improved F1 score in 20% when compared to the original trained dataset, validating the proposed workflow.","2021-10-06","2023-04-21 05:48:53","2023-05-23 01:33:07","2023-04-21 05:48:53","9","","1","1","","Discov Artif Intell","","","","","","","","en","","","","","Springer Link","","4 citations (Crossref) [2023-04-21]","","C:\Users\ambreen.hanif\Zotero\storage\N9GTMKWD\Bento et al_2021_Improving deep learning performance by using Explainable Artificial.pdf; ","notion://www.notion.so/Bento-et-al-2021-bd49fd790bf448a4be6de92f48f41ada","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MC66KF3K","preprint","2021","Hanif, Ambreen","Towards Explainable Artificial Intelligence in Banking and Financial Services","","","","10.48550/arXiv.2112.08441","http://arxiv.org/abs/2112.08441","Artificial intelligence (AI) enables machines to learn from human experience, adjust to new inputs, and perform human-like tasks. AI is progressing rapidly and is transforming the way businesses operate, from process automation to cognitive augmentation of tasks and intelligent process/data analytics. However, the main challenge for human users would be to understand and appropriately trust the result of AI algorithms and methods. In this paper, to address this challenge, we study and analyze the recent work done in Explainable Artificial Intelligence (XAI) methods and tools. We introduce a novel XAI process, which facilitates producing explainable models while maintaining a high level of learning performance. We present an interactive evidence-based approach to assist human users in comprehending and trusting the results and output created by AI-enabled algorithms. We adopt a typical scenario in the Banking domain for analyzing customer transactions. We develop a digital dashboard to facilitate interacting with the algorithm results and discuss how the proposed XAI method can significantly improve the confidence of data scientists in understanding the result of AI-enabled algorithms.","2021-12-14","2023-04-21 05:46:54","2023-05-23 01:33:03","2023-04-21 05:46:54","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2112.08441 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\A6D8CV3F\2112.html; C:\Users\ambreen.hanif\Zotero\storage\7JIB7XWM\Hanif_2021_Towards Explainable Artificial Intelligence in Banking and Financial Services.pdf; ","notion://www.notion.so/Hanif-2021-118735262a7449ca865ff877b3dfb2e7","notion","","","","","","","","","","","","","","","","","","","","arXiv:2112.08441","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C3AU88ND","preprint","2021","Luss, Ronny; Chen, Pin-Yu; Dhurandhar, Amit; Sattigeri, Prasanna; Zhang, Yunfeng; Shanmugam, Karthikeyan; Tu, Chun-Chen","Leveraging Latent Features for Local Explanations","","","","10.48550/arXiv.1905.12698","http://arxiv.org/abs/1905.12698","As the application of deep neural networks proliferates in numerous areas such as medical imaging, video surveillance, and self driving cars, the need for explaining the decisions of these models has become a hot research topic, both at the global and local level. Locally, most explanation methods have focused on identifying relevance of features, limiting the types of explanations possible. In this paper, we investigate a new direction by leveraging latent features to generate contrastive explanations; predictions are explained not only by highlighting aspects that are in themselves sufficient to justify the classification, but also by new aspects which if added will change the classification. The key contribution of this paper lies in how we add features to rich data in a formal yet humanly interpretable way that leads to meaningful results. Our new definition of ""addition"" uses latent features to move beyond the limitations of previous explanations and resolve an open question laid out in Dhurandhar, et. al. (2018), which creates local contrastive explanations but is limited to simple datasets such as grayscale images. The strength of our approach in creating intuitive explanations that are also quantitatively superior to other methods is demonstrated on three diverse image datasets (skin lesions, faces, and fashion apparel). A user study with 200 participants further exemplifies the benefits of contrastive information, which can be viewed as complementary to other state-of-the-art interpretability methods.","2021-05-29","2023-04-21 05:46:05","2023-05-23 01:33:00","2023-04-21 05:46:05","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1905.12698 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\GY4HB7NL\1905.html; C:\Users\ambreen.hanif\Zotero\storage\VPITQPFC\Luss et al_2021_Leveraging Latent Features for Local Explanations.pdf; ","notion://www.notion.so/Luss-et-al-2021-b467ee105631437588a2856153344a6c","notion","","","","","","","","","","","","","","","","","","","","arXiv:1905.12698","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"47MKFJFH","preprint","2021","Yang, Zebin; Zhang, Aijun; Sudjianto, Agus","GAMI-Net: An Explainable Neural Network based on Generalized Additive Models with Structured Interactions","","","","10.48550/arXiv.2003.07132","http://arxiv.org/abs/2003.07132","The lack of interpretability is an inevitable problem when using neural network models in real applications. In this paper, an explainable neural network based on generalized additive models with structured interactions (GAMI-Net) is proposed to pursue a good balance between prediction accuracy and model interpretability. GAMI-Net is a disentangled feedforward network with multiple additive subnetworks; each subnetwork consists of multiple hidden layers and is designed for capturing one main effect or one pairwise interaction. Three interpretability aspects are further considered, including a) sparsity, to select the most significant effects for parsimonious representations; b) heredity, a pairwise interaction could only be included when at least one of its parent main effects exists; and c) marginal clarity, to make main effects and pairwise interactions mutually distinguishable. An adaptive training algorithm is developed, where main effects are first trained and then pairwise interactions are fitted to the residuals. Numerical experiments on both synthetic functions and real-world datasets show that the proposed model enjoys superior interpretability and it maintains competitive prediction accuracy in comparison to the explainable boosting machine and other classic machine learning models.","2021-06-02","2023-04-21 05:43:41","2023-05-23 01:32:59","2023-04-21 05:43:40","","","","","","","GAMI-Net","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2003.07132 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\EL9K4JLT\2003.html; ; C:\Users\ambreen.hanif\Zotero\storage\PQDFWJ9R\Yang et al_2021_GAMI-Net.pdf","notion://www.notion.so/Yang-et-al-2021-fa4c55a9f7164fd396b56bbe9c49ea51","notion","","","","","","","","","","","","","","","","","","","","arXiv:2003.07132","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YXARE5VN","preprint","2021","Sudjianto, Agus; Zhang, Aijun","Designing Inherently Interpretable Machine Learning Models","","","","10.48550/arXiv.2111.01743","http://arxiv.org/abs/2111.01743","Interpretable machine learning (IML) becomes increasingly important in highly regulated industry sectors related to the health and safety or fundamental rights of human beings. In general, the inherently IML models should be adopted because of their transparency and explainability, while black-box models with model-agnostic explainability can be more difficult to defend under regulatory scrutiny. For assessing inherent interpretability of a machine learning model, we propose a qualitative template based on feature effects and model architecture constraints. It provides the design principles for high-performance IML model development, with examples given by reviewing our recent works on ExNN, GAMI-Net, SIMTree, and the Aletheia toolkit for local linear interpretability of deep ReLU networks. We further demonstrate how to design an interpretable ReLU DNN model with evaluation of conceptual soundness for a real case study of predicting credit default in home lending. We hope that this work will provide a practical guide of developing inherently IML models in high risk applications in banking industry, as well as other sectors.","2021-11-02","2023-04-21 05:43:11","2023-05-23 01:32:58","2023-04-21 05:43:11","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2111.01743 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\LQP8G5GH\2111.html; ; C:\Users\ambreen.hanif\Zotero\storage\IXVALRCH\Sudjianto_Zhang_2021_Designing Inherently Interpretable Machine Learning Models.pdf","notion://www.notion.so/Sudjianto-Zhang-2021-d478539465c149dc92836f29e67c7533","notion","","","","","","","","","","","","","","","","","","","","arXiv:2111.01743","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q7EATIB9","webpage","","","[2111.01743] Designing Inherently Interpretable Machine Learning Models","","","","","https://arxiv.org/abs/2111.01743","","","2023-04-21 05:42:57","2023-05-23 01:32:54","2023-04-21 05:42:57","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\DZMKUHIV\2111.html; ","notion://www.notion.so/2111-01743-Designing-Inherently-Interpretable-Machine-Learning-Models-n-d-db97c22a079d4726a97cc9068487d727","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YGJ5AFP2","preprint","2020","Sudjianto, Agus; Knauth, William; Singh, Rahul; Yang, Zebin; Zhang, Aijun","Unwrapping The Black Box of Deep ReLU Networks: Interpretability, Diagnostics, and Simplification","","","","10.48550/arXiv.2011.04041","http://arxiv.org/abs/2011.04041","The deep neural networks (DNNs) have achieved great success in learning complex patterns with strong predictive power, but they are often thought of as ""black box"" models without a sufficient level of transparency and interpretability. It is important to demystify the DNNs with rigorous mathematics and practical tools, especially when they are used for mission-critical applications. This paper aims to unwrap the black box of deep ReLU networks through local linear representation, which utilizes the activation pattern and disentangles the complex network into an equivalent set of local linear models (LLMs). We develop a convenient LLM-based toolkit for interpretability, diagnostics, and simplification of a pre-trained deep ReLU network. We propose the local linear profile plot and other visualization methods for interpretation and diagnostics, and an effective merging strategy for network simplification. The proposed methods are demonstrated by simulation examples, benchmark datasets, and a real case study in home lending credit risk assessment.","2020-11-08","2023-04-21 05:41:20","2023-05-23 01:32:52","2023-04-21 05:41:20","","","","","","","Unwrapping The Black Box of Deep ReLU Networks","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2011.04041 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\KJN7NAYP\2011.html; ; C:\Users\ambreen.hanif\Zotero\storage\W2MXM2L8\Sudjianto et al_2020_Unwrapping The Black Box of Deep ReLU Networks.pdf","notion://www.notion.so/Sudjianto-et-al-2020-dc51747d9e9441cea343910587a26d2c","notion","","","","","","","","","","","","","","","","","","","","arXiv:2011.04041","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6GW65IWY","preprint","2020","Sundararajan, Mukund; Najmi, Amir","The many Shapley values for model explanation","","","","10.48550/arXiv.1908.08474","http://arxiv.org/abs/1908.08474","The Shapley value has become a popular method to attribute the prediction of a machine-learning model on an input to its base features. The use of the Shapley value is justified by citing [16] showing that it is the \emph{unique} method that satisfies certain good properties (\emph{axioms}). There are, however, a multiplicity of ways in which the Shapley value is operationalized in the attribution problem. These differ in how they reference the model, the training data, and the explanation context. These give very different results, rendering the uniqueness result meaningless. Furthermore, we find that previously proposed approaches can produce counterintuitive attributions in theory and in practice---for instance, they can assign non-zero attributions to features that are not even referenced by the model. In this paper, we use the axiomatic approach to study the differences between some of the many operationalizations of the Shapley value for attribution, and propose a technique called Baseline Shapley (BShap) that is backed by a proper uniqueness result. We also contrast BShap with Integrated Gradients, another extension of Shapley value to the continuous setting.","2020-02-07","2022-08-18 02:29:30","2022-08-18 02:29:30","2022-08-18 02:29:30","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1908.08474 [cs, econ]","","C:\Users\ambreen.hanif\Zotero\storage\FDUL4XTX\1908.html; C:\Users\ambreen.hanif\Zotero\storage\DSKGSQWF\Sundararajan_Najmi_2020_The many Shapley values for model explanation.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1908.08474","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KFSVUH8Q","journalArticle","2013","Lavelle, Jane Suilin; Botterill, George; Lock, Suzanne","Contrastive explanation and the many absences problem","Synthese","","1573-0964","10.1007/s11229-012-0205-9","https://doi.org/10.1007/s11229-012-0205-9","We often explain by citing an absence or an omission. Apart from the problem of assigning a causal role to such apparently negative factors as absences and omissions, there is a puzzle as to why only some absences and omissions, out of indefinitely many, should figure in explanations. In this paper we solve this ’many absences problem’ by using the contrastive model of explanation. The contrastive model of explanation is developed by adapting Peter Lipton’s account. What initially appears to be only a trivial amendment to Lipton’s Difference Condition enables us both to offer a much more satisfactory solution to the ’many absences problem’ than David Lewis did, and also to explain why explanation in terms of absences and omissions should be so common.","2013-11-01","2022-08-18 02:08:21","2022-08-18 02:08:21","2022-08-18 02:08:21","3495-3510","","16","190","","Synthese","","","","","","","","en","","","","","Springer Link","","","","C:\Users\ambreen.hanif\Zotero\storage\QLC8BI4P\Lavelle et al_2013_Contrastive explanation and the many absences problem.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T8JYGXCZ","journalArticle","1990","Lipton, Peter","Contrastive Explanation*","Royal Institute of Philosophy Supplements","","1755-3555, 1358-2461","10.1017/S1358246100005130","https://www.cambridge.org/core/journals/royal-institute-of-philosophy-supplements/article/abs/contrastive-explanation/EB3C55BBB37E6D0B2A88705EBD1F3BA5","According to a causal model of explanation, we explain phenomena by giving their causes or, where the phenomena are themselves causal regularities, we explain them by giving a mechanism linking cause and effect. If we explain why smoking causes cancer, we do not give the cause of this causal connection, but we do give the causal mechanism that makes it. The claim that to explain is to give a cause is not only natural and plausible, but it also avoids many of the objections to other accounts of explanation, such as the views that to explain is to give a reason to believe the phenomenon occurred, to somehow make the phenomenon familiar, or to give a Deductive-Nomological argument. Unlike the reason for belief account, a causal model makes a clear distinction between understanding why a phenomenon occurs and merely knowing that it does, and the model does so in a way that makes understanding unmysterious and objective. Understanding is not some sort of super-knowledge, but simply more knowledge: knowledge of the phenomenon and knowledge of its causal history. A causal model makes it clear how something can explain without itself being explained, and so avoids the regress of whys, since we can know a phenomenon's cause without knowing the cause of the cause. It also accounts for legitimate self-evidencing explanations, explanations where the phenomenon is an essential part of the evidence that the explanation is correct, so the explanation can not supply a non-circular reason for believing the phenomenon occurred. There is no barrier to knowing a cause through its effects and also knowing that it is their cause. The speed of recession of a star explains its observed red-shift, even though the shift is an essential part of the evidence for its speed of recession. The model also avoids the most serious objection to the familiarity view, which is that some phenomena are familiar yet not understood, since a phenomenon can be perfectly familiar, such as the blueness of the sky or the fact that the same side of the moon always faces the earth, even if we do not know its cause. Finally, a causal model avoids many of the objections to the Deductive-Nomological model. Ordinary explanations do not have to meet the requirements of the Deductive-Nomological model, because one does not need to give a law to give a cause, and one does not need to know a law to have good reason to believe that a cause is a cause. As for the notorious over-permissiveness of the Deductive-Nomological model, the reason recession explains red-shift but not conversely, is simply that causes explain effects but not conversely, and the reason a conjunction of laws does not explain its conjuncts is that conjunctions do not cause their conjuncts.","1990-03","2022-08-18 02:02:42","2022-08-18 02:02:42","2022-08-18 02:02:42","247-266","","","27","","","","","","","","","","en","","","","","Cambridge University Press","","Publisher: Cambridge University Press","","C:\Users\ambreen.hanif\Zotero\storage\YZ9WTSSQ\EB3C55BBB37E6D0B2A88705EBD1F3BA5.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J7WZMCWV","conferencePaper","2019","Choi, Minsuk; Park, Cheonbok; Yang, Soyoung; Kim, Yonggyu; Choo, Jaegul; Hong, Sungsoo Ray","AILA: Attentive Interactive Labeling Assistant for Document Classification through Attention-Based Deep Neural Networks","Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","978-1-4503-5970-2","","10.1145/3290605.3300460","https://doi.org/10.1145/3290605.3300460","Document labeling is a critical step in building various machine learning applications. However, the step can be time-consuming and arduous, requiring a significant amount of human efforts. To support an efficient document labeling environment, we present a system called Attentive Interactive Labeling Assistant (AILA). In its core, AILA uses Interactive Attention Module (IAM), a novel module that visually highlights words in a document that labelers may pay attention to when labeling a document. IAM utilizes attention-based Deep Neural Networks which not only support a prediction of which words to highlight but also enable labelers to indicate words that should be assigned a high attention weight while labeling to improve the future quality of word prediction.We evaluated the labeling efficiency and the accuracy by comparing the conditions with and without IAM in our study. The results showed that participants' labeling efficiency increased significantly under the condition with IAM than the condition without IAM, while the two conditions maintained roughly the same labeling accuracy.","2019-05-02","2022-08-18 01:57:18","2022-08-18 01:57:18","2022-08-17","1–12","","","","","","AILA","CHI '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MVD9WUSS","conferencePaper","2019","Hohman, Fred; Head, Andrew; Caruana, Rich; DeLine, Robert; Drucker, Steven M.","Gamut: A Design Probe to Understand How Data Scientists Understand Machine Learning Models","Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","978-1-4503-5970-2","","10.1145/3290605.3300809","https://doi.org/10.1145/3290605.3300809","Without good models and the right tools to interpret them, data scientists risk making decisions based on hidden biases, spurious correlations, and false generalizations. This has led to a rallying cry for model interpretability. Yet the concept of interpretability remains nebulous, such that researchers and tool designers lack actionable guidelines for how to incorporate interpretability into models and accompanying tools. Through an iterative design process with expert machine learning researchers and practitioners, we designed a visual analytics system, Gamut, to explore how interactive interfaces could better support model interpretation. Using Gamut as a probe, we investigated why and how professional data scientists interpret models, and how interface affordances can support data scientists in answering questions about model interpretability. Our investigation showed that interpretability is not a monolithic concept: data scientists have different reasons to interpret models and tailor explanations for specific audiences, often balancing competing concerns of simplicity and completeness. Participants also asked to use Gamut in their work, highlighting its potential to help data scientists understand their own data.","2019-05-02","2022-08-18 01:57:11","2022-08-18 01:57:11","2022-08-17","1–13","","","","","","Gamut","CHI '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"85S778GZ","conferencePaper","2019","Wang, Danding; Yang, Qian; Abdul, Ashraf; Lim, Brian Y.","Designing Theory-Driven User-Centric Explainable AI","Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems","978-1-4503-5970-2","","10.1145/3290605.3300831","https://doi.org/10.1145/3290605.3300831","From healthcare to criminal justice, artificial intelligence (AI) is increasingly supporting high-consequence human decisions. This has spurred the field of explainable AI (XAI). This paper seeks to strengthen empirical application-specific investigations of XAI by exploring theoretical underpinnings of human decision making, drawing from the fields of philosophy and psychology. In this paper, we propose a conceptual framework for building human-centered, decision-theory-driven XAI based on an extensive review across these fields. Drawing on this framework, we identify pathways along which human cognitive patterns drives needs for building XAI and how XAI can mitigate common cognitive biases. We then put this framework into practice by designing and implementing an explainable clinical diagnostic tool for intensive care phenotyping and conducting a co-design exercise with clinicians. Thereafter, we draw insights into how this framework bridges algorithm-generated explanations and human decision-making theories. Finally, we discuss implications for XAI design and development.","2019-05-02","2022-08-18 01:57:01","2022-08-18 01:57:01","2022-08-17","1–15","","","","","","","CHI '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C59QJZH5","journalArticle","2018","Klein, Gary","Explaining Explanation, Part 3: The Causal Landscape","IEEE Intelligent Systems","","1941-1294","10.1109/MIS.2018.022441353","","This is the third in a series of essays about explanation. After laying out the core theoretical concepts in the first article, including aspects of causation and abduction, the second article presented some empirical research to reveal the great variety of purposes and types of causal reasoning, as well as a number of different causal explanation patterns. Taking the notion of reasoning patterns a step further, the author describes a method whereby a decision maker can go from a causal explanation to a viable course of action for making positive change in the future, not to mention aiding decision making in general.","2018-03","2022-08-18 01:55:16","2022-08-18 01:55:16","","83-88","","2","33","","","Explaining Explanation, Part 3","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Intelligent Systems","","C:\Users\ambreen.hanif\Zotero\storage\CAISG48T\8378482.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"52NHVJEK","journalArticle","2018","Hoffman, Robert; Miller, Tim; Mueller, Shane T.; Klein, Gary; Clancey, William J.","Explaining Explanation, Part 4: A Deep Dive on Deep Nets","IEEE Intelligent Systems","","1941-1294","10.1109/MIS.2018.033001421","","This is the fourth in a series of essays about explainable AI. Previous essays laid out the theoretical and empirical foundations. This essay focuses on Deep Nets, and con-siders methods for allowing system users to generate self-explanations. This is accomplished by exploring how the Deep Net systems perform when they are operating at their boundary conditions. Inspired by recent research into adversarial examples that demonstrate the weakness-es of Deep Nets, we invert the purpose of these adversar-ial examples and argue that spoofing can be used as a tool to answer contrastive explanation questions via user-driven exploration.","2018-05","2022-08-18 01:55:16","2022-08-18 01:55:16","","87-95","","3","33","","","Explaining Explanation, Part 4","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Intelligent Systems","","C:\Users\ambreen.hanif\Zotero\storage\T8NGFHMP\8423529.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P2YVMMC2","journalArticle","2017","Hoffman, Robert R.; Mueller, Shane T.; Klein, Gary","Explaining Explanation, Part 2: Empirical Foundations","IEEE Intelligent Systems","","1941-1294","10.1109/MIS.2017.3121544","","This article surveys empirical research that reveals the variety and diversity of the forms and purposes of causal reasoning, and reveals the myths that have driven philosophical analysis, psychological research, and computational approaches. The intent of the essay is to broaden the horizons for the development of intelligent systems that serve explanatory functions.","2017","2022-08-18 01:55:05","2022-08-18 01:55:05","","78-86","","4","32","","","Explaining Explanation, Part 2","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Intelligent Systems","","C:\Users\ambreen.hanif\Zotero\storage\JZ3ZQIPM\8012316.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MI732JS2","journalArticle","2017","Hoffman, Robert R.; Klein, Gary","Explaining Explanation, Part 1: Theoretical Foundations","IEEE Intelligent Systems","","1941-1294","10.1109/MIS.2017.54","","This is the first in a series of essays that addresses the manifest programmatic interest in developing intelligent systems that help people make good decisions in messy, complex, and uncertain circumstances by exploring several questions: What is an explanation? How do people explain things? How might intelligent systems explain their workings? How might intelligent systems help humans be better understanders as well as better explainers? This article addresses the theoretical foundations.","2017-05","2022-08-18 01:55:00","2022-08-18 01:55:00","","68-73","","3","32","","","Explaining Explanation, Part 1","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Intelligent Systems","","C:\Users\ambreen.hanif\Zotero\storage\LTNRNPEN\7933919.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7LWEEMSC","preprint","2017","You, Seungil; Ding, David; Canini, Kevin; Pfeifer, Jan; Gupta, Maya","Deep Lattice Networks and Partial Monotonic Functions","","","","10.48550/arXiv.1709.06680","http://arxiv.org/abs/1709.06680","We propose learning deep models that are monotonic with respect to a user-specified set of inputs by alternating layers of linear embeddings, ensembles of lattices, and calibrators (piecewise linear functions), with appropriate constraints for monotonicity, and jointly training the resulting network. We implement the layers and projections with new computational graph nodes in TensorFlow and use the ADAM optimizer and batched stochastic gradients. Experiments on benchmark and real-world datasets show that six-layer monotonic deep lattice networks achieve state-of-the art performance for classification and regression with monotonicity guarantees.","2017-09-19","2022-08-18 01:42:57","2022-08-18 01:42:57","2022-08-18 01:42:57","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1709.06680 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\IJPKKXYV\1709.html; C:\Users\ambreen.hanif\Zotero\storage\BWJV2YJR\You et al_2017_Deep Lattice Networks and Partial Monotonic Functions.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1709.06680","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZLATRTQX","preprint","2017","Frosst, Nicholas; Hinton, Geoffrey","Distilling a Neural Network Into a Soft Decision Tree","","","","10.48550/arXiv.1711.09784","http://arxiv.org/abs/1711.09784","Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.","2017-11-27","2022-08-18 01:42:36","2022-08-18 01:42:36","2022-08-18 01:42:36","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1711.09784 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\4348WSIV\1711.html; C:\Users\ambreen.hanif\Zotero\storage\RTRW9ZZP\Frosst_Hinton_2017_Distilling a Neural Network Into a Soft Decision Tree.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1711.09784","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RPTIHKMW","preprint","2015","Hinton, Geoffrey; Vinyals, Oriol; Dean, Jeff","Distilling the Knowledge in a Neural Network","","","","10.48550/arXiv.1503.02531","http://arxiv.org/abs/1503.02531","A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.","2015-03-09","2022-08-18 01:42:03","2022-08-18 01:42:03","2022-08-18 01:42:03","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1503.02531 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\XNMC335D\1503.html; C:\Users\ambreen.hanif\Zotero\storage\77VU4G99\Hinton et al_2015_Distilling the Knowledge in a Neural Network.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1503.02531","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JQHC9AU8","preprint","2020","Koh, Pang Wei; Liang, Percy","Understanding Black-box Predictions via Influence Functions","","","","10.48550/arXiv.1703.04730","http://arxiv.org/abs/1703.04730","How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.","2020-12-29","2022-08-18 01:41:42","2022-08-18 01:41:42","2022-08-18 01:41:41","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1703.04730 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\3EJ243JE\1703.html; C:\Users\ambreen.hanif\Zotero\storage\QDIINA8T\Koh_Liang_2020_Understanding Black-box Predictions via Influence Functions.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1703.04730","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WGI2YVYN","preprint","2018","Yeh, Chih-Kuan; Kim, Joon Sik; Yen, Ian E. H.; Ravikumar, Pradeep","Representer Point Selection for Explaining Deep Neural Networks","","","","10.48550/arXiv.1811.09720","http://arxiv.org/abs/1811.09720","We propose to explain the predictions of a deep neural network, by pointing to the set of what we call representer points in the training set, for a given test point prediction. Specifically, we show that we can decompose the pre-activation prediction of a neural network into a linear combination of activations of training points, with the weights corresponding to what we call representer values, which thus capture the importance of that training point on the learned parameters of the network. But it provides a deeper understanding of the network than simply training point influence: with positive representer values corresponding to excitatory training points, and negative values corresponding to inhibitory points, which as we show provides considerably more insight. Our method is also much more scalable, allowing for real-time feedback in a manner not feasible with influence functions.","2018-11-23","2022-08-18 01:41:09","2022-08-18 01:41:09","2022-08-18 01:41:09","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1811.09720 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\4QUU6W4P\1811.html; C:\Users\ambreen.hanif\Zotero\storage\NXQNKYWQ\Yeh et al_2018_Representer Point Selection for Explaining Deep Neural Networks.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1811.09720","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DRIUZFIP","preprint","2016","Nguyen, Phuoc; Tran, Truyen; Wickramasinghe, Nilmini; Venkatesh, Svetha","Deepr: A Convolutional Net for Medical Records","","","","10.48550/arXiv.1607.07519","http://arxiv.org/abs/1607.07519","Feature engineering remains a major bottleneck when creating predictive systems from electronic medical records. At present, an important missing element is detecting predictive regular clinical motifs from irregular episodic records. We present Deepr (short for Deep record), a new end-to-end deep learning system that learns to extract features from medical records and predicts future risk automatically. Deepr transforms a record into a sequence of discrete elements separated by coded time gaps and hospital transfers. On top of the sequence is a convolutional neural net that detects and combines predictive local clinical motifs to stratify the risk. Deepr permits transparent inspection and visualization of its inner working. We validate Deepr on hospital data to predict unplanned readmission after discharge. Deepr achieves superior accuracy compared to traditional techniques, detects meaningful clinical motifs, and uncovers the underlying structure of the disease and intervention space.","2016-07-25","2022-08-18 01:40:32","2022-08-18 01:40:32","2022-08-18 01:40:32","","","","","","","Deepr","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1607.07519 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\AZJN9AYZ\1607.html; C:\Users\ambreen.hanif\Zotero\storage\CJSUJ58X\Nguyen et al_2016_Deepr.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1607.07519","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IH7DB7QC","preprint","2019","Kapishnikov, Andrei; Bolukbasi, Tolga; Viégas, Fernanda; Terry, Michael","XRAI: Better Attributions Through Regions","","","","10.48550/arXiv.1906.02825","http://arxiv.org/abs/1906.02825","Saliency methods can aid understanding of deep neural networks. Recent years have witnessed many improvements to saliency methods, as well as new ways for evaluating them. In this paper, we 1) present a novel region-based attribution method, XRAI, that builds upon integrated gradients (Sundararajan et al. 2017), 2) introduce evaluation methods for empirically assessing the quality of image-based saliency maps (Performance Information Curves (PICs)), and 3) contribute an axiom-based sanity check for attribution methods. Through empirical experiments and example results, we show that XRAI produces better results than other saliency methods for common models and the ImageNet dataset.","2019-08-20","2022-08-18 01:39:52","2022-08-18 01:39:52","2022-08-18 01:39:52","","","","","","","XRAI","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1906.02825 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\VM3T2DNN\1906.html; C:\Users\ambreen.hanif\Zotero\storage\5SAE5DFC\Kapishnikov et al_2019_XRAI.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1906.02825","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K5DVPN8V","journalArticle","2018","Ribeiro, Marco Tulio; Singh, Sameer; Guestrin, Carlos","Anchors: High-Precision Model-Agnostic Explanations","Proceedings of the AAAI Conference on Artificial Intelligence","","2374-3468, 2159-5399","10.1609/aaai.v32i1.11491","https://ojs.aaai.org/index.php/AAAI/article/view/11491","We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, “sufﬁcient” conditions for predictions. We propose an algorithm to efﬁciently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the ﬂexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.","2018-04-25","2022-08-18 01:38:56","2022-08-18 01:38:58","2022-08-18 01:38:55","","","1","32","","AAAI","Anchors","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\ambreen.hanif\Zotero\storage\AFRXFIVZ\Ribeiro et al. - 2018 - Anchors High-Precision Model-Agnostic Explanation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZUGIZC4D","preprint","2017","Lei, Jing; G'Sell, Max; Rinaldo, Alessandro; Tibshirani, Ryan J.; Wasserman, Larry","Distribution-Free Predictive Inference For Regression","","","","10.48550/arXiv.1604.04173","http://arxiv.org/abs/1604.04173","We develop a general framework for distribution-free predictive inference in regression, using conformal inference. The proposed methodology allows for the construction of a prediction band for the response variable using any estimator of the regression function. The resulting prediction band preserves the consistency properties of the original estimator under standard assumptions, while guaranteeing finite-sample marginal coverage even when these assumptions do not hold. We analyze and compare, both empirically and theoretically, the two major variants of our conformal framework: full conformal inference and split conformal inference, along with a related jackknife method. These methods offer different tradeoffs between statistical accuracy (length of resulting prediction intervals) and computational efficiency. As extensions, we develop a method for constructing valid in-sample prediction intervals called {\it rank-one-out} conformal inference, which has essentially the same computational efficiency as split conformal inference. We also describe an extension of our procedures for producing prediction bands with locally varying length, in order to adapt to heteroskedascity in the data. Finally, we propose a model-free notion of variable importance, called {\it leave-one-covariate-out} or LOCO inference. Accompanying this paper is an R package {\tt conformalInference} that implements all of the proposals we have introduced. In the spirit of reproducibility, all of our empirical results can also be easily (re)generated using this package.","2017-03-08","2022-08-18 01:38:34","2022-08-18 01:38:34","2022-08-18 01:38:34","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1604.04173 [math, stat]","","C:\Users\ambreen.hanif\Zotero\storage\LWVVTNBR\1604.html; C:\Users\ambreen.hanif\Zotero\storage\SK2487K2\Lei et al_2017_Distribution-Free Predictive Inference For Regression.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1604.04173","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5TBC7YFR","preprint","2017","Dillon, Joshua V.; Langmore, Ian; Tran, Dustin; Brevdo, Eugene; Vasudevan, Srinivas; Moore, Dave; Patton, Brian; Alemi, Alex; Hoffman, Matt; Saurous, Rif A.","TensorFlow Distributions","","","","10.48550/arXiv.1711.10604","http://arxiv.org/abs/1711.10604","The TensorFlow Distributions library implements a vision of probability theory adapted to the modern deep-learning paradigm of end-to-end differentiable computation. Building on two basic abstractions, it offers flexible building blocks for probabilistic computation. Distributions provide fast, numerically stable methods for generating samples and computing statistics, e.g., log density. Bijectors provide composable volume-tracking transformations with automatic caching. Together these enable modular construction of high dimensional distributions and transformations not possible with previous libraries (e.g., pixelCNNs, autoregressive flows, and reversible residual networks). They are the workhorse behind deep probabilistic programming systems like Edward and empower fast black-box inference in probabilistic models built on deep-network components. TensorFlow Distributions has proven an important part of the TensorFlow toolkit within Google and in the broader deep learning community.","2017-11-28","2022-08-18 01:38:20","2022-08-18 01:38:20","2022-08-18 01:38:20","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1711.10604 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\XB3RXNT2\1711.html; C:\Users\ambreen.hanif\Zotero\storage\PILJUQ33\Dillon et al_2017_TensorFlow Distributions.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1711.10604","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KCLU2G8W","book","2021","Saeed, Waddah; Omlin, Christian","Explainable AI (XAI): A Systematic Meta-Survey of Current Challenges and Future Opportunities","","","","","","The past decade has seen significant progress in artificial intelligence (AI), which has resulted in algorithms being adopted for resolving a variety of problems. However, this success has been met by increasing model complexity and employing black-box AI models that lack transparency. In response to this need, Explainable AI (XAI) has been proposed to make AI more transparent and thus advance the adoption of AI in critical domains. Although there are several reviews of XAI topics in the literature that identified challenges and potential research directions in XAI, these challenges and research directions are scattered. This study, hence, presents a systematic meta-survey for challenges and future research directions in XAI organized in two themes: (1) general challenges and research directions in XAI and (2) challenges and research directions in XAI based on machine learning life cycle's phases: design, development, and deployment. We believe that our meta-survey contributes to XAI literature by providing a guide for future exploration in the XAI area.","2021-11-11","2022-08-05 00:09:39","2022-08-05 00:09:39","","","","","","","","Explainable AI (XAI)","","","","","","","","","","","","ResearchGate","","","","; C:\Users\ambreen.hanif\Zotero\storage\NP6WQ2E9\Saeed_Omlin_2021_Explainable AI (XAI).pdf","https://www.researchgate.net/publication/356198426_Explainable_AI_XAI_A_Systematic_Meta-Survey_of_Current_Challenges_and_Future_Opportunities","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J6GFEGV7","book","2020","Khadivizand, Sam; Beheshti, Amin; Sobhanmanesh, Fariborz; Sheng, Quan Z.; Istanbouli, Elias; Wood, Steven; Pezaro, Damon","Towards intelligent feature engineering for risk-based customer segmentation in banking","","978-1-4503-8924-2","","","","","2020","2021-01-29 01:59:41","2022-08-03 04:56:12","","","74-83","","1","","","","","","","","Association for Computing Machinery","","","","","","","","","Publication Title: The 18th International Conference on Advances in Mobile Computing and Multimedia (MoMM '20), November 30-December 2, 2020, Chiang Mai, Thailand DOI: 10.1145/3428690.3429172 Citation Key: Khadivizand2020 Issue: 1","","C:\Users\ambreen.hanif\Zotero\storage\YXRIDDH3\Khadivizand et al_2020_Towards intelligent feature engineering for risk-based customer segmentation in.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CXTDZLX3","conferencePaper","2010","Namvar, M.; Gholamian, M. R.; KhakAbi, S.","A Two Phase Clustering Method for Intelligent Customer Segmentation","Modelling and Simulation 2010 International Conference on Intelligent Systems","","","10.1109/ISMS.2010.48","","Customer Segmentation is an increasingly significant issue in today's competitive commercial area. Many literatures have reviewed the application of data mining technology in customer segmentation, and achieved sound effectives. But in the most cases, it is performed using customer data from a special point of view, rather than from systematical method considering all stages of CRM. This paper, with the aid of data mining tools, constructs a new customer segmentation method based on RFM, demographic and LTV data. The new customer segmentation method consists of two phases. Firstly, with K-means clustering, customers are clustered into different segments regarding their RFM. Secondly, using demographic data, each cluster again is partitioned into new clusters. Finally, using LTV, a profile for customer is created. The method has been applied to a dataset from Iranian bank, which resulted in some useful management measures and suggestions.","2010-01","2021-03-15 17:58:02","2022-08-03 04:55:30","","215-219","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2166-0670","","C:\Users\ambreen.hanif\Zotero\storage\BF5VK9F8\Namvar et al. - 2010 - A Two Phase Clustering Method for Intelligent Cust.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Modelling and Simulation 2010 International Conference on Intelligent Systems","","","","","","","","","","","","","","",""
"ZQQRCCM6","book","2019","Rabiul Islam, Sheikh; Eberle, William; Bundy, Sid; Khaled Ghafoor, Sheikh","Infusing domain knowledge in AI-based ""black box"" models for better explainability with application in bankruptcy prediction","","978-1-4503-9999-9","","","https://doi.org/preprint","Although ""black box"" models such as Artificial Neural Networks, Support Vector Machines, and Ensemble Approaches continue to show superior performance in many disciplines, their adoption in the sensitive disciplines (e.g., finance, healthcare) is questionable due to the lack of interpretability and explainability of the model. In fact, future adoption of ""black box"" models is difficult because of the recent rule of ""right of explanation"" by the European Union where a user can ask for an explanation behind an algorithmic decision, and the newly proposed bill by the US government, the ""Algorithmic Accountability Act"", which would require companies to assess their machine learning systems for bias and discrimination and take corrective measures. Top Bankruptcy Prediction Models are A.I.-Based and are in need of better explainability-the extent to which the internal working mechanisms of an AI system can be explained in human terms. Although explainable artificial intelligence is an emerging field of research, infusing domain knowledge for better explainability might be a possible solution. In this work, we demonstrate a way to collect and infuse domain knowledge into a ""black box"" model for bankruptcy prediction. Our understanding from the experiments reveals that infused domain knowledge makes the output from the black box model more interpretable and explainable. CCS CONCEPTS • Computing methodologies → Inductive logic learning.","2019","2021-06-15 02:40:36","2022-08-03 04:55:02","2021-06-15","","","","","","","","","","","","","","","","","","","","","arXiv: 1905.11474v2","","C:\Users\ambreen.hanif\Zotero\storage\PLQ3DHBZ\Rabiul Islam et al_2019_Infusing domain knowledge in AI-based black box models for better.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T6U6FSC6","webpage","","","Techniques for interpretable machine learning | Communications of the ACM","","","","","https://dl.acm.org/doi/fullHtml/10.1145/3359786","","","2022-07-07 06:21:19","2022-07-07 06:21:19","2022-07-07 06:21:19","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\L6IP2EKB\3359786.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F42CH8ND","webpage","2020","Brownie, Jason","4 Types of Classification Tasks in Machine Learning","","","","","https://machinelearningmastery.com/types-of-classification-in-machine-learning/","","2020-04-08","2021-10-28 02:51:07","2022-07-07 05:54:47","2021-10-28","","","","","","","","","","","","","","","","","","","","","","","","","classification; ML","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2SAT8CFP","report","","Riddell, Graeme A.; Delden, Hedwig Van; Vanhout, Roel; Maier, Holger R.; Newman, Jeffrey P.; Zecchin, Aaron C.; Dandy, Graeme C.","MULTI-HAZARD MITIGATION PLANNING, COMBINING MODELLING, SCENARIOS AND OPTIMISATION: RESULTS FROM SOUTH AUSTRALIA Non-peer reviewed research proceedings from the Bushfire and Natural Hazards CRC & AFAC conference Sydney, 4-6 September 2017","","","","","","","","2022-04-06 01:01:15","2022-07-07 05:18:57","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H9A5Z2VB","report","2020","Maier, Holger; Riddell, Graeme; Delden, Hedwig Van; Araya, Sofanit; Zecchin, Aaron; Vanhout, Roel; Dandy, Graeme; Hamers, Eike","IMPROVED DECISION SUPPORT FOR NATURAL HAZARD RISK REDUCTION","","","","","","","2020","2022-04-06 01:01:14","2022-07-07 05:18:52","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PP7ZB2W7","preprint","2022","Ali, Ameen; Schnake, Thomas; Eberle, Oliver; Montavon, Grégoire; Müller, Klaus-Robert; Wolf, Lior","XAI for Transformers: Better Explanations through Conservative Propagation","","","","","http://arxiv.org/abs/2202.07304","Transformers have become an important workhorse of machine learning, with numerous applications. This necessitates the development of reliable methods for increasing their transparency. Multiple interpretability methods, often based on gradient information, have been proposed. We show that the gradient in a Transformer reflects the function only locally, and thus fails to reliably identify the contribution of input features to the prediction. We identify Attention Heads and LayerNorm as main reasons for such unreliable explanations and propose a more stable way for propagation through these layers. Our proposal, which can be seen as a proper extension of the well-established LRP method to Transformers, is shown both theoretically and empirically to overcome the deficiency of a simple gradient-based approach, and achieves state-of-the-art explanation performance on a broad range of Transformer models and datasets.","2022-06-23","2022-07-05 00:11:41","2022-07-05 00:11:41","2022-07-05 00:11:41","","","","","","","XAI for Transformers","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2202.07304 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\IPLLEWUS\Ali et al_2022_XAI for Transformers.pdf; C:\Users\ambreen.hanif\Zotero\storage\ZWJU3VPU\2202.html","","","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2202.07304","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I57D8ZY3","preprint","2022","Zhou, Yilun; Ribeiro, Marco Tulio; Shah, Julie","ExSum: From Local Explanations to Model Understanding","","","","","http://arxiv.org/abs/2205.00130","Interpretability methods are developed to understand the working mechanisms of black-box models, which is crucial to their responsible deployment. Fulfilling this goal requires both that the explanations generated by these methods are correct and that people can easily and reliably understand them. While the former has been addressed in prior work, the latter is often overlooked, resulting in informal model understanding derived from a handful of local explanations. In this paper, we introduce explanation summary (ExSum), a mathematical framework for quantifying model understanding, and propose metrics for its quality assessment. On two domains, ExSum highlights various limitations in the current practice, helps develop accurate model understanding, and reveals easily overlooked properties of the model. We also connect understandability to other properties of explanations such as human alignment, robustness, and counterfactual minimality and plausibility.","2022-04-29","2022-07-05 00:11:21","2022-07-05 00:11:21","2022-07-05 00:11:21","","","","","","","ExSum","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2205.00130 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\2LKY64J2\2205.html; C:\Users\ambreen.hanif\Zotero\storage\D45FRFUD\Zhou et al_2022_ExSum.pdf","","","Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2205.00130","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P3TGI9WF","preprint","2021","Bensaid, Eden; Martino, Mauro; Hoover, Benjamin; Strobelt, Hendrik","FairyTailor: A Multimodal Generative Framework for Storytelling","","","","","http://arxiv.org/abs/2108.04324","Storytelling is an open-ended task that entails creative thinking and requires a constant flow of ideas. Natural language generation (NLG) for storytelling is especially challenging because it requires the generated text to follow an overall theme while remaining creative and diverse to engage the reader. In this work, we introduce a system and a web-based demo, FairyTailor, for human-in-the-loop visual story co-creation. Users can create a cohesive children's fairytale by weaving generated texts and retrieved images with their input. FairyTailor adds another modality and modifies the text generation process to produce a coherent and creative sequence of text and images. To our knowledge, this is the first dynamic tool for multimodal story generation that allows interactive co-formation of both texts and images. It allows users to give feedback on co-created stories and share their results.","2021-07-12","2022-07-04 23:41:50","2022-07-04 23:41:50","2022-07-04 23:41:10","","","","","","","FairyTailor","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2108.04324 [cs]","","false; C:\Users\ambreen.hanif\Zotero\storage\SGBJIVFD\Bensaid et al. - 2021 - FairyTailor A Multimodal Generative Framework for.html","https://arxiv.org/pdf/2108.04324.pdf","","Computer Science - Artificial Intelligence; Computer Science - Computer Vision and Pattern Recognition; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2108.04324","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V6RH5X5M","webpage","","","Adoption of AI advances, but foundational barriers remain | McKinsey","","","","","https://www.mckinsey.com/featured-insights/artificial-intelligence/ai-adoption-advances-but-foundational-barriers-remain","","","2022-06-28 00:03:07","2022-06-28 00:03:07","2022-06-28","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4TDJ5QI8","journalArticle","2021","Cyras, Kristijonas; Rago, Antonio; Albini, Emanuele; Baroni, Pietro; Toni, Francesca","Argumentative XAI: A Survey","IJCAI International Joint Conference on Artificial Intelligence","","10450823","10.48550/arxiv.2105.11266","https://arxiv.org/abs/2105.11266v1","Explainable AI (XAI) has been investigated for decades and, together with AI itself, has witnessed unprecedented growth in recent years. Among various approaches to XAI, argumentative models have been advocated in both the AI and social science literature, as their dialectical nature appears to match some basic desirable features of the explanation activity. In this survey we overview XAI approaches built using methods from the field of computational argumentation, leveraging its wide array of reasoning abstractions and explanation delivery methods. We overview the literature focusing on different types of explanation (intrinsic and post-hoc), different models with which argumentation-based explanations are deployed, different forms of delivery, and different argumentation frameworks they use. We also lay out a roadmap for future work.","2021-05-24","2022-06-22 03:34:38","2022-06-22 03:34:43","2022-06-22","4392-4399","","","","","","","","","","","","","","","","","","","","arXiv: 2105.11266 Publisher: International Joint Conferences on Artificial Intelligence ISBN: 9780999241196","","C:\Users\ambreen.hanif\Zotero\storage\X6I8GX23\Cyras et al_2021_Argumentative XAI.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KPLPDZE5","journalArticle","2018","Beheshti, Amin; Vaghani, Kushal; Benatallah, Boualem; Tabebordbar, Alireza","CrowdCorrect: A curation pipeline for social data cleansing and curation","Lecture Notes in Business Information Processing","","18651348","10.1007/978-3-319-92901-9_3/COVER/","https://link.springer.com/chapter/10.1007/978-3-319-92901-9_3","Process and data are equally important for business process management. Data-driven approaches in process analytics aims to value decisions that can be backed up with verifiable private and open data. Over the last few years, data-driven analysis of how knowledge workers and customers interact in social contexts, often with data obtained from social networking services such as Twitter and Facebook, have become a vital asset for organizations. For example, governments started to extract knowledge and derive insights from vastly growing open data to improve their services. A key challenge in analyzing social data is to understand the raw data generated by social actors and prepare it for analytic tasks. In this context, it is important to transform the raw data into a contextualized data and knowledge. This task, known as data curation, involves identifying relevant data sources, extracting data and knowledge, cleansing, maintaining, merging, enriching and linking data and knowledge. In this paper we present CrowdCorrect, a data curation pipeline to enable analysts cleansing and curating social data and preparing it for reliable business data analytics. The first step offers automatic feature extraction, correction and enrichment. Next, we design micro-tasks and use the knowledge of the crowd to identify and correct information items that could not be corrected in the first step. Finally, we offer a domain-model mediated method to use the knowledge of domain experts to identify and correct items that could not be corrected in previous steps. We adopt a typical scenario for analyzing Urban Social Issues from Twitter as it relates to the Government Budget, to highlight how CrowdCorrect significantly improves the quality of extracted knowledge compared to the classical curation pipeline and in the absence of knowledge of the crowd and domain experts.","2018","2022-06-21 01:23:03","2022-06-21 01:23:03","2022-06-21","24-38","","","317","","","","","","","","","","","","","","","","","Publisher: Springer Verlag ISBN: 9783319929002","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3RWQPF6L","journalArticle","2018","Chen, Chaofan; Lin, Kangcheng; Rudin, Cynthia; Shaposhnik, Yaron; Wang, Sijia; Wang, Tong","An Interpretable Model with Globally Consistent Explanations for Credit Risk","","","","10.48550/arxiv.1811.12615","http://arxiv.org/abs/1811.12615","We propose a possible solution to a public challenge posed by the Fair Isaac Corporation (FICO), which is to provide an explainable model for credit risk assessment. Rather than present a black box model and explain it afterwards, we provide a globally interpretable model that is as accurate as other neural networks. Our ""two-layer additive risk model"" is decomposable into subscales, where each node in the second layer represents a meaningful subscale, and all of the nonlinearities are transparent. We provide three types of explanations that are simpler than, but consistent with, the global model. One of these explanation methods involves solving a minimum set cover problem to find high-support globally-consistent explanations. We present a new online visualization tool to allow users to explore the global model and its explanations.","2018-11-29","2022-06-20 23:55:26","2022-06-20 23:55:36","2022-06-20","","","","","","","","","","","","","","","","","","","","","arXiv: 1811.12615","","C:\Users\ambreen.hanif\Zotero\storage\KETIBAYJ\Chen et al_2018_An Interpretable Model with Globally Consistent Explanations for Credit Risk.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C4P4QEV6","report","2019","Bracke, Philippe; Datta, Anupam; Jung, Carsten; Sen, Shayak","Machine learning explainability in finance: an application to default risk analysis","","","","","","","2019","2021-09-30 00:47:05","2022-06-10 01:25:33","2021-09-30","","","","","","","","","","","","Bank of England","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\2HDP8BZQ\Bracke et al_2019_Machine learning explainability in finance.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MUZYHKF9","webpage","","","IEEE Xplore Full-Text PDF:","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8466590","","","2022-05-17 04:15:54","2022-05-17 04:15:54","2022-05-17","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CCRUH9N3","journalArticle","2020","Khattab, Omar; Zaharia, Matei","ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT","SIGIR 2020 - Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval","","","10.48550/arxiv.2004.12832","https://arxiv.org/abs/2004.12832v2","Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.","2020-04-27","2022-05-17 04:02:41","2022-05-17 04:02:49","2022-05-17","39-48","","","","","","","","","","","","","","","","","","","","arXiv: 2004.12832 Publisher: Association for Computing Machinery, Inc ISBN: 9781450380164","","C:\Users\ambreen.hanif\Zotero\storage\6GPGB952\Khattab_Zaharia_2020_ColBERT.pdf","","","bert; deep language models; efficiency; neural ir","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CQ75Q9S2","journalArticle","2020","Khattab, Omar; Zaharia, Matei","ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT","SIGIR 2020 - Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval","","","10.48550/arxiv.2004.12832","https://arxiv.org/abs/2004.12832v2","Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.","2020-04-27","2022-05-13 12:22:49","2022-05-13 12:22:58","2022-05-13","39-48","","","","","","","","","","","","","","","","","","","","arXiv: 2004.12832 Publisher: Association for Computing Machinery, Inc ISBN: 9781450380164","","C:\Users\ambreen.hanif\Zotero\storage\U9UIFNV2\Khattab_Zaharia_2020_ColBERT.pdf","","","bert; deep language models; efficiency; neural ir","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FQA3ZNPQ","newspaperArticle","","","","","","","","","","","2022-04-27 02:55:10","2022-04-27 02:55:10","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MUDKZXZX","journalArticle","","Ferreira, Juliana Jansen; Monteiro, Mateus","The human-AI relationship in decision-making:","","","","","","","","2022-04-08 04:38:09","2022-04-08 04:38:09","","9","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\3EQEG4IA\Ferreira and Monteiro - The human-AI relationship in decision-making.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W3J65T3W","book","2011","New South Wales; Department of Planning","Hazardous industry planning advisory paper.","","978-0-7347-5917-7 978-0-7347-5877-4 978-0-7347-5878-1 978-0-7347-5923-8 978-0-7347-5864-4 978-0-7347-5862-0 978-0-7347-5860-6 978-0-7347-5872-9 978-0-7347-5871-2 978-1-74263-028-1 978-0-7347-5873-6 978-0-7347-5932-0","","","","Contains twelve Hazardous industry planning advisory papers (HIPAPs) issued by the Dept. of Planning to assist stakeholders in implementing an integrated assessment process. These guidelines were updated in 2011 to incorporate recent developments in risk assessment and management techniques, land use safety planning and current best practice.","2011","2022-04-07 06:11:47","2022-04-07 06:11:47","","","","","","","","","","","","","NSW Dept. of Planning","Sydney, N.S.W.","en","","","","","Open WorldCat","","OCLC: 769806707","","C:\Users\ambreen.hanif\Zotero\storage\IGBNW7QA\New South Wales and Department of Planning - 2011 - Hazardous industry planning advisory paper..pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7QKITB3F","journalArticle","2021","Luan, Chaoxu; Liu, Renzhi; Peng, Sicheng","Land-use suitability assessment for urban development using a GIS-based soft computing approach: A case study of Ili Valley, China","Ecological Indicators","","1470-160X","https://doi.org/10.1016/j.ecolind.2020.107333","https://www.sciencedirect.com/science/article/pii/S1470160X20312759","Land-use suitability assessment is an important step in land use planning for urban development. We propose a GIS-based soft computing approach (GSC), which is a combination of two multi-criteria analysis methods, i.e., the ordered weighted averaging (OWA) method and the logic scoring of preference (LSP) method, to evaluate and map land-use suitability for urban development in Ili Valley, China. The evaluation uses 13 factors as suitability criteria for urban development. These factors are related to the topography and geology, socio-economic feasibility, ecological restrictions, and prohibitive constraints. Based on the final suitability results, Ili Valley was classified using five suitability levels: highly suitable, suitable, moderately suitable, marginally suitable, and not suitable. By comparing seven preference decision coefficient (α) scenarios, we determined that the area of the land that is highly suitable for urban development decreases, and the area of the marginally suitable land increases with increasing α. All of the scenarios show that approximately 32.6% of the land area is not suitable. Among the seven scenarios, the policy orientation of three of the scenarios was an urban expansion policy orientation (α = 0.5), a balanced policy orientation (α = 1), and an ecological protection policy orientation (α = 2). The result of the policy orientation analysis of urban development can be used as an urban development boundary for different development preferences and stages. The local land-use planning was evaluated by overlaying its planned urban development zones with the areas of the resultant suitability map. Specific recommendations are presented for the scale and timing of urban development in Ili Valley.","2021","2022-04-07 04:09:56","2022-04-07 04:09:56","","107333","","","123","","","","","","","","","","","","","","","","","","","","","","Mandatory suitability factors; Multi-criteria assessment; Ordered weighted averaging; Policy orientation analysis; Urban areas","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"52W6E59Q","report","2021","Bommasani, Rishi; Hudson Ehsan; Adeli Russ Altman Simran Arora Sydney von Arx Michael S Bernstein Jeannette Bohg Antoine Bosselut Emma Brunskill Erik Brynjolfsson Shyamal Buch Dallas Card Rodrigo Castellon Niladri Chatterji Annie Chen Kathleen Creel Jared Quincy Davis Dorottya Demsz…, Drew A","On the Opportunities and Risks of Foundation Models; On the Opportunities and Risks of Foundation Models","","","","","","AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles (e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities, and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.","2021","2022-04-04 01:58:48","2022-04-04 01:58:48","2022-04-04","","","","","","","","","","","","","","","","","","","","","arXiv: 2108.07258v2","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IL8MAARM","webpage","","","Analytics","","","","","https://analytics.google.com/analytics/web/#/report-home/a173028051w304291830p261287315","","","2022-02-24 23:13:33","2022-02-24 23:13:33","2022-02-24 23:13:32","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3THQJKD6","journalArticle","2004","Affairs, Indigenous","80 - Personal particulars for character assessment","","","","","","","2004","2022-02-13 20:17:40","2022-02-13 20:20:56","","3-4","","","80","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\K999NFHH\Affairs_2004_80 - Personal particulars for character assessment.pdf","","","80; character assessment; National Security; personal particulars; PIC4002","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I7HEDBUP","journalArticle","2021","Hanif, Ambreen","MRES Y2 Weekly Reports Summary :","","","","","","","2021","2022-02-13 20:17:40","2022-02-13 20:20:49","","4-6","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\C3W5K6SN\Hanif_2021_MRES Y2 Weekly Reports Summary.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J8EIEH8I","computerProgram","2017","Alex Goldstein, Adam Kapelner, Justin Bleich","CRAN - Package ICEbox","","","","","https://cran.r-project.org/web/packages/ICEbox/index.html","","2017","2022-01-14 03:13:04","2022-01-14 03:14:27","2022-01-14","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A6MKURNI","webpage","","","Incorporating Explainable Artificial Intelligence (XAI) to aid the Understanding of Machine Learning in the Healthcare Domain","","","","","https://www.researchgate.net/publication/346717871_Incorporating_Explainable_Artificial_Intelligence_XAI_to_aid_the_Understanding_of_Machine_Learning_in_the_Healthcare_Domain","","","2021-11-25 02:17:58","2022-01-06 05:25:16","2021-11-25","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BAMG79UI","webpage","","","Explainable AI in banking: Ingenta Connect","","","","","https://www.ingentaconnect.com/content/hsp/jdb001/2020/00000004/00000004/art00007","","","2021-11-25 02:35:38","2021-11-25 02:35:38","2021-11-25","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SA3UQWGK","webpage","","","How do you measure trust in deep learning? – TechTalks","","","","","https://bdtechtalks.com/2020/11/23/deep-learning-trust-metrics/","","","2021-11-25 02:27:18","2021-11-25 02:27:18","2021-11-25","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A74JZDLB","webpage","","","How to explain neural networks using SHAP | Your Data Teacher","","","","","https://www.yourdatateacher.com/2021/05/17/how-to-explain-neural-networks-using-shap/","","","2021-11-25 02:16:03","2021-11-25 02:16:03","2021-11-25","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HZZXCREP","webpage","","","SHAP Values Explained Exactly How You Wished Someone Explained to You | by Samuele Mazzanti | Towards Data Science","","","","","https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30","","","2021-11-25 02:14:37","2021-11-25 02:14:37","2021-11-25","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N7KDQZ65","webpage","","","Machine Learning in Finance – Present and Future Applications | Emerj","","","","","https://emerj.com/ai-sector-overviews/machine-learning-in-finance/","","","2021-11-25 01:58:11","2021-11-25 01:58:11","2021-11-25","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S2FJMHMZ","webpage","","","Explainable AI for Understanding Decisions & Data-Driven Optimization in Deep Neural Networks | by Nisarg Dave | Medium","","","","","https://medium.com/@nisargdave/explainable-ai-for-understanding-decisions-data-driven-optimization-in-deep-neural-networks-70b0e947dc18","","","2021-11-24 23:42:14","2021-11-24 23:42:14","2021-11-25","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9JZC5H3M","webpage","2021","Nobel, Roger","Why AI needs to be explainable: Part two - Zegami Machine Learning","","","","","https://zegami.com/blog/ai-explainable-part-two/","","2021","2021-11-24 23:28:07","2021-11-24 23:31:48","2021-11-25","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PFKP437N","webpage","","","Code.org","","","","","https://studio.code.org/s/coursea-2020","Anyone can learn computer science. Make games, apps and art with code.","","2021-11-20 00:23:06","2021-11-20 00:23:06","2021-11-20 00:21:24","","","","","","","","","","","","","","","","","","","","","","","false","https://studio.code.org/s/coursea-2020","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B92XFPCG","webpage","","Kijko, Pawel","The Best Tools for Machine Learning Model Visualization - neptune.ai","","","","","https://neptune.ai/blog/the-best-tools-for-machine-learning-model-visualization","","","2021-10-28 01:06:11","2021-11-19 01:59:02","2021-10-28","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HXNVTLC7","webpage","2021","","Lending indicators, August 2021 | Australian Bureau of Statistics","","","","","https://www.abs.gov.au/statistics/economy/finance/lending-indicators/latest-release","","2021","2021-10-20 02:52:13","2021-11-19 00:28:34","2021-10-20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8MGF73JN","magazineArticle","2017","Dignum, Virginia","Responsible Artificial Intelligence: Designing Ai for Human Values","Daffodil International University","","","","dspace.daffodilvarsity.edu.bd:8080/handle/123456789/2181","","2017-09-25","2021-09-28 02:32:36","2021-10-27 04:48:31","2021-09-28","","","","","","","","","","","","","","","","","","","","","Publisher: Daffodil International University","","","","","Artificial intelligence; Article; design for values; ethics; societal impact","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EBP7AV3U","book","2021","Thirthe Gowda, M. T.; Chandrika, J.","Analysis and classification of ripped tobacco leaves using machine learning techniques","","9789811552571","","","","Tobacco crop is one of the major crops in the world and plays a vital role in the international market. After cultivation of tobacco plants, classification of ripped tobacco leaves is one of the challenging tasks. Generally in India, the classification of ripped leaves is done by a manual process only. Machine learning-based classification model is introduced to nullify the human intervention in the classification of tobacco ripped leaves. The proposed model is designed according to classification of three important features like color, texture, and shape. For the experimental purpose the images of the leaves are captured using mobile sensor and the dataset was created. The dataset consists of 2040 tobacco leaf images; from the image dataset 1378 images are used for training and 622 images are used for testing. The proposed model is validated by many machine learning algorithms, where the classification model achieved an efficiency of 94.2% on validation accuracy and 86% in testing accuracy. The proposed model has significant high performance on the classification of ripped tobacco leaves.","2021","2021-02-10 03:05:13","2021-10-27 04:47:06","","","171-180","","53","","","","","","","","","","","","","","","","","Publication Title: Lecture Notes on Data Engineering and Communications Technologies DOI: 10.1007/978-981-15-5258-8_18 Citation Key: Thirthe2021AnalysisTechniques ISSN: 23674520","","C:\Users\ambreen.hanif\Zotero\storage\XDVHGXCZ\2021_Book_EvolutionaryComputingAndMobile.pdf","","","Classification; FCV; Features; Grading; Tobacco leaf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MZ8WT42V","webpage","","","General Data Protection Regulation (GDPR) – Official Legal Text","","","","","https://gdpr-info.eu/","","","2021-10-27 04:22:37","2021-10-27 04:22:37","2021-10-27","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QSRSR35V","journalArticle","2019","Gee, Alan H.; Garcia-Olano, Diego; Ghosh, Joydeep; Paydarfar, David","Explaining Deep Classification of Time-Series Data with Learned Prototypes","CEUR workshop proceedings","","","","/pmc/articles/PMC8050893/","The emergence of deep learning networks raises a need for explainable AI so that users and domain experts can be confident applying them to high-risk decisions. In this paper, we leverage data from the latent space induced by deep learning models to learn stereotypical representations or ""prototypes"" during training to elucidate the algorithmic decision-making process. We study how leveraging prototypes effect classification decisions of two dimensional time-series data in a few different settings: (1) electrocardiogram (ECG) waveforms to detect clinical bradycardia, a slowing of heart rate, in preterm infants, (2) respiration waveforms to detect apnea of prematurity, and (3) audio waveforms to classify spoken digits. We improve upon existing models by optimizing for increased prototype diversity and robustness, visualize how these prototypes in the latent space are used by the model to distinguish classes, and show that prototypes are capable of learning features on two dimensional time-series data to produce explainable insights during classification tasks. We show that the prototypes are capable of learning real-world features - bradycardia in ECG, apnea in respiration, and articulation in speech - as well as features within sub-classes. Our novel work leverages learned prototypical framework on two dimensional time-series data to produce explainable insights during classification tasks.","2019-08","2021-10-26 22:36:46","2021-10-26 22:36:50","2021-10-27","15","","","2429","","","","","","","","","","","","","","","","","PMID: 33867901 Publisher: NIH Public Access","","; ; C:\Users\ambreen.hanif\Zotero\storage\SI6MZ66B\Gee et al_2019_Explaining Deep Classification of Time-Series Data with Learned Prototypes.pdf","/pmc/articles/PMC8050893/?report=abstract; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8050893/","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IJDPBXWJ","webpage","2021","Weinberger, David","Playing with AI Fairness","","","","","https://pair-code.github.io/what-if-tool/ai-fairness.html","","2021","2021-10-26 04:39:18","2021-10-26 04:39:18","2021-10-26","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TYVN5DNI","webpage","","","AI has disappointed on Covid | Financial Times","","","","","https://www.ft.com/content/0aafc2de-f46d-4646-acfd-4ed7a7f6feaa","","","2021-10-26 03:11:42","2021-10-26 03:11:42","2021-10-26","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WX83L2SQ","webpage","","","IBM pitched Watson as a revolution in cancer care. It's nowhere close","","","","","https://www.statnews.com/2017/09/05/watson-ibm-cancer/","","","2021-10-26 03:05:06","2021-10-26 03:05:06","2021-10-26","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JENQTGBQ","webpage","","","Stories of AI Failure and How to Avoid Similar AI Fails - Lexalytics","","","","","https://www.lexalytics.com/lexablog/stories-ai-failure-avoid-ai-fails-2020","","","2021-10-26 03:00:51","2021-10-26 03:00:51","2021-10-26","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NIZ3HQMS","webpage","","","In Depth Guide to the Top 100+ AI Use Cases & Applications","","","","","https://research.aimultiple.com/ai-usecases/","","","2021-10-26 02:57:22","2021-10-26 02:57:22","2021-10-26","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IYC5BNUW","journalArticle","2021","To, Thesis Submitted","INTELLIGENT VECTOR BASED CUSTOMER","","","","","","","2021","2021-03-03 12:17:51","2021-10-26 00:24:36","","","","February","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\AKXYL7FH\To_2021_INTELLIGENT VECTOR BASED CUSTOMER.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z5D8FFXU","webpage","2017","Vives, Xavier","The Impact of Fintech on Banking - European Economy","","","","","https://european-economy.eu/2017-2/the-impact-of-fintech-on-banking/","","2017","2021-10-20 02:40:21","2021-10-25 23:59:12","2021-10-20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3KK2HUTI","webpage","","","How Big Data Has Changed Finance","","","","","https://www.investopedia.com/articles/active-trading/040915/how-big-data-has-changed-finance.asp","","","2021-10-20 02:39:21","2021-10-20 02:39:21","2021-10-20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TLVTQUT4","webpage","2017","Jennifer Q. Trelewicz","Big Data and Big Money: The Role of Data in the Financial Sector","","","","","https://www.infoq.com/articles/big-data-in-finance/","","2017","2021-10-20 02:38:07","2021-10-20 02:38:07","2021-10-20","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V2ATFVRL","book","1953","Kuhn, Harold William; Tucker, Albert William","Contributions to the Theory of Games (AM-28),","","","","","https://books.google.com.au/books?hl=en&lr=&id=Pd3TCwAAQBAJ&oi=fnd&pg=PA307&ots=gtpVCa6lsW&sig=n9tq29WKNDtpDX779VfchFxF4Fg&redir_esc=y#v=onepage&q&f=false","","1953","2021-06-05 23:20:34","2021-09-27 23:31:13","2021-06-06","","301-317","","II","","","","","","","","Princeton University Press","","","","","","","","","Publication Title: Book","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6B9NULG8","webpage","","","XAI: Explainable AI – Data Protection Hobbling the Prediction Machine? | White & Case LLP","","","","","https://www.whitecase.com/publications/alert/xai-explainable-ai-data-protection-hobbling-prediction-machine","","","2021-08-29 00:46:48","2021-08-29 00:46:48","2021-08-29","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R77VRP25","webpage","","","OpenAI Five","","","","","https://openai.com/five/","","","2021-08-26 01:51:19","2021-08-26 01:51:19","2021-08-26","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S6JAV6DR","webpage","","","Global Artificial Intelligence Market Size 2021 Rise at","","","","","https://www.globenewswire.com/news-release/2021/06/28/2253975/0/en/Global-Artificial-Intelligence-Market-Size-2021-Rise-at-35-6-CAGR-Will-Grow-to-USD-299-64-Billion-by-2026-Facts-Factors.html","","","2021-08-25 08:35:55","2021-08-25 08:35:55","2021-08-25","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"53JE95DI","book","2019","Hall, Patrick; Gill, Navdeep","Explain Your AI H2O Driverless AI","","","","","","H2O Driverless AI is an industry-leading and award-winning automatic machine learning platform that empowers data scientists to be more productive by accelerating workflows with automatic feature engineering, customizable user-defined modeling recipes, and automatic model deployment, among many other leading-edge capabilities. Moreover, its unique offering of interpretable models, post hoc explanations, and basic disparate impact testing enable data scientists to establish trust in their work and provide model explanations to business partners and potentially to regulators. Your explainable AI journey awaits.","2019","2021-08-05 23:56:14","2021-08-06 00:23:50","2021-08-06","","","","II","","","","","","","","O'Reilly Media","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\PH654CUC\Hall_Gill_2019_Explain Your AI H2O Driverless AI.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"99Z5NFUM","webpage","","","[1808.00033] Techniques for Interpretable Machine Learning","","","","","https://arxiv.org/abs/1808.00033","","","2021-08-06 00:22:24","2021-08-06 00:22:24","2021-08-06","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V7ZV9NED","report","","Ferreira, Leonardo Augusto; Guimarães, Frederico Gadelha; Silva, Rodrigo","Applying Genetic Programming to Improve Interpretability in Machine Learning Models","","","","","https://gplearn.readthedocs.io/","Explainable Artificial Intelligence (or xAI) has become an important research topic in the fields of Machine Learning and Deep Learning. In this paper, we propose a Genetic Programming (GP) based approach, named Genetic Programming Explainer (GPX), to the problem of explaining decisions computed by AI systems. The method generates a noise set located in the neighborhood of the point of interest, whose prediction should be explained, and fits a local explanation model for the analyzed sample. The tree structure generated by GPX provides a comprehensible analytical, possibly non-linear, symbolic expression which reflects the local behavior of the complex model. We considered three machine learning techniques that can be recognized as complex black-box models: Random Forest, Deep Neural Network and Support Vector Machine in twenty data sets for regression and classifications problems. Our results indicate that the GPX is able to produce more accurate understanding of complex models than the state of the art. The results validate the proposed approach as a novel way to deploy GP to improve interpretability.","","2021-06-02 23:57:37","2021-06-02 23:57:40","2021-06-03","","","","","","","","","","","","","","","","","","","","","arXiv: 2005.09512v1","","C:\Users\ambreen.hanif\Zotero\storage\JKCQ3J4I\Ferreira et al_Applying Genetic Programming to Improve Interpretability in Machine Learning.pdf","","","Explainability; Genetic Programming; Index Terms-Interpretability; Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YL9FXEUX","report","","Danilevsky, Marina; Qian, Kun; Aharonov, Ranit; Katsis, Yannis; Sen, Prithviraj","A Survey of the State of Explainable AI for Natural Language Processing","","","","","https://xainlp2020.github.io/xainlp/","Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.","","2021-05-19 23:36:54","2021-05-20 00:21:10","2021-05-20","","","","","","","","","","","","","","","","","","","","","arXiv: 2010.00711v1","","C:\Users\ambreen.hanif\Zotero\storage\2FRA623L\Danilevsky et al_A Survey of the State of Explainable AI for Natural Language Processing.pdf; C:\Users\ambreen.hanif\Zotero\storage\7AC4AM5Y\full-text.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R9UN75Y9","webpage","","","How to Choose a Feature Selection Method For Machine Learning","","","","","https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/","","","2021-04-29 22:10:57","2021-04-29 22:10:57","2021-04-30","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IGYNBHL3","webpage","","","Automated Feature Engineering Tools | by Rajneesh Jha | Analytics Vidhya | Medium","","","","","https://medium.com/analytics-vidhya/automated-feature-engineering-tools-44d00be56e3a","","","2021-04-29 22:08:13","2021-04-29 22:08:13","2021-04-30","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DE7FIL5V","webpage","","","Featuretools | An open source framework for automated feature engineering Quick Start","","","","","https://www.featuretools.com/","","","2021-04-29 22:01:57","2021-04-29 22:01:57","2021-04-29 22:01:56","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5A5NVQJD","webpage","","","Featuretools | An open source framework for automated feature engineering Quick Start","","","","","https://www.featuretools.com/","","","2021-04-29 22:00:24","2021-04-29 22:00:24","2021-04-30","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2EYXKZQ8","journalArticle","2016","Sarvari, Peiman Alipour; Ustundag, Alp; Takci, Hidayet","Performance evaluation of different customer segmentation approaches based on RFM and demographics analysis","Kybernetes","","0368-492X","10.1108/K-07-2015-0180","https://doi.org/10.1108/K-07-2015-0180","Purpose The purpose of this paper is to determine the best approach to customer segmentation and to extrapolate associated rules for this based on recency, frequency and monetary (RFM) considerations as well as demographic factors. In this study, the impacts of RFM and demographic attributes have been challenged in order to enrich factors that lend comprehension to customer segmentation. Different types of scenario were designed, performed and evaluated meticulously under uniform test conditions. The data for this study were extracted from the database of a global pizza restaurant chain in Turkey. This paper summarizes the findings of the study and also provides evidence of its empirical implications to improve the performance of customer segmentation as well as achieving extracted rule perfection via effective model factors and variations. Accordingly, marketing and service processes will work more effectively and efficiently for customers and society. The implication of this study is that it explains a clear concept for interaction between producers and consumers. Design/methodology/approach Customer relationship management, which aims to manage record and evaluate customer interactions, is generally regarded as a vital tool for companies that wish to be successful in the rapidly changing global market. The prediction of customer behaviors is a strategically important and difficult issue because of the high variance and wide range of customer orders and preferences. So to have an effective tool for extracting rules based on customer purchasing behavior, considering tangible and intangible criteria is highly important. To overcome the challenges imposed by the multifaceted nature of this problem, the authors utilized artificial intelligence methods, including k-means clustering, Apriori association rule mining (ARM) and neural networks. The main idea was that customer clusters are better enhanced when segmentation processes are based on RFM analysis accompanied by demographic data. Weighted RFM (WRFM) and unweighted RFM values/scores were applied with and without demographic factors and utilized to compose different types and numbers of clusters. The Apriori algorithm was used to extract rules of association. The performance analyses of scenarios have been conducted based on these extracted rules. The number of rules, elapsed time and prediction accuracy were used to evaluate the different scenarios. The results of evaluations were compared with the outputs of another available technique. Findings The results showed that having an appropriate segmentation approach is vital if there are to be strong association rules. Also, it has been determined from the results that the weights of RFM attributes affect rule association performance positively. Moreover, to capture more accurate customer segments, a combination of RFM and demographic attributes is recommended for clustering. The results’ analyses indicate the undeniable importance of demographic data merged with WRFM. Above all, this challenge introduced the best possible sequence of factors for an analysis of clustering and ARM based on RFM and demographic data. Originality/value The work compared k-means and Kohonen clustering methods in its segmentation phase to prove the superiority of adopted segmentation techniques. In addition, this study indicated that customer segments containing WRFM scores and demographic data in the same clusters brought about stronger and more accurate association rules for the understanding of customer behavior. These so-called achievements were compared with the results of classical approaches in order to support the credibility of the proposed methodology. Based on previous works, classical methods for customer segmentation have overlooked any combination of demographic data with WRFM during clustering before proceeding to their rule extraction stages.","2016-01-01","2021-03-15 18:08:30","2021-03-15 18:08:30","2021-03-15 18:08:29","1129-1157","","7","45","","","","","","","","","","","","","","","Emerald Insight","","","","C:\Users\ambreen.hanif\Zotero\storage\9DR59KWI\Sarvari et al. - 2016 - Performance evaluation of different customer segme.html","","","Association rule algorithm; Customer segmentation; Demographic variables; Performance evaluation; RFM analysis; Self-organizing map (SOM)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SSY4RNNX","journalArticle","2018","Khalili-Damghani, Kaveh; Abdi, Farshid; Abolmakarem, Shaghayegh","Hybrid soft computing approach based on clustering, rule mining, and decision tree analysis for customer segmentation problem: Real case of customer-centric industries","Applied Soft Computing","","1568-4946","10.1016/j.asoc.2018.09.001","https://www.sciencedirect.com/science/article/pii/S1568494618305052","This paper proposes a hybrid soft computing approach on the basis of clustering, rule extraction, and decision tree methodology to predict the segment of the new customers in customer-centric companies. In the first module, K-means algorithm is applied to cluster the past customers of company on the basis of their purchase behavior. In the second module, a hybrid feature selection method based on filtering and a multi-attribute decision making method is proposed. Finally, On the basis of customers’ characteristics and using decision tree analysis, IF–THEN rules are mined. The proposed approach is applied in two case studies in the field of insurance and telecommunication in order to predict potentially profitable leads and outline the most influential features available to customers in order to perform this prediction. The results validate the efficacy and applicability of proposed approach to handle real-life cases.","2018-12-01","2021-03-15 17:59:03","2021-03-15 17:59:03","2021-03-15 17:59:00","816-828","","","73","","Applied Soft Computing","Hybrid soft computing approach based on clustering, rule mining, and decision tree analysis for customer segmentation problem","","","","","","","en","","","","","ScienceDirect","","","","C:\Users\ambreen.hanif\Zotero\storage\F62AUKGE\Khalili-Damghani et al. - 2018 - Hybrid soft computing approach based on clustering.html","","","Clustering algorithm; Data mining; Decision tree; Feature selection; Rule mining","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"73XPAH9Q","journalArticle","2020","Mousaeirad, Salman","Intelligent Vector-based Customer Segmentation in the Banking Industry","arXiv:2012.11876 [cs]","","","","http://arxiv.org/abs/2012.11876","Customer Segmentation is the process of dividing customers into groups based on common characteristics. An intelligent Customer Segmentation will not only enable an organization to effectively allocate marketing resources (e.g., Recommender Systems in the Banking sector) but also it will enable identifying the customer cohorts that are most likely to benefit from a specific policy (e.g., to discover diverse patient groups in the Health sector). While there has been a significant improvement in approaches to Customer Segmentation, the main challenge remains to be the understanding of the reasons behind the segmentation need. This task is challenging as it is subjective and depends on the goal of segmentation as well as the analyst's perspective. To address this challenge, in this paper, we present an intelligent vector-based customer segmentation approach. The proposed approach will leverage feature engineering to enable analysts to identify important features (from a pool of features such as demographics, geography, psychographics, behavioral, and more) and feed them into a neural embedding framework named Customer2Vec. The Customer2Vec combines the neural network classification and clustering methods as supervised and unsupervised learning techniques to embed the customer vector. We adopt a typical scenario in the Banking Sector to highlight how Customer2Vec significantly improves the quality of the segmentation and detecting customer similarities.","2020-12-22","2021-03-15 17:58:12","2021-03-15 17:58:12","2021-03-15 17:58:11","","","","","","","","","","","","","","","","","","","arXiv.org","","arXiv: 2012.11876","","C:\Users\ambreen.hanif\Zotero\storage\GM2J6E39\Mousaeirad - 2020 - Intelligent Vector-based Customer Segmentation in .pdf; C:\Users\ambreen.hanif\Zotero\storage\ZG55ZTKR\Mousaeirad - 2020 - Intelligent Vector-based Customer Segmentation in .html","","","Computer Science - Information Retrieval","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JNU9IVNV","webpage","","","Lessons Learned at the 2017 CIKM AnalytiCup Machine Learning Competition","Alibaba Cloud Community","","","","https://www.alibabacloud.com/blog/lessons-learned-at-the-2017-cikm-analyticup-machine-learning-competition_593828","The CIKM AnalytiCup Machine Learning Competition is a platform to explore how different AI algorithms compete against each other in solving real-world problems.","","2021-03-03 14:35:10","2021-03-03 14:35:10","2021-03-03 14:35:10","","","","","","","","","","","","","","en","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\6IMXA8GU\Lessons Learned at the 2017 CIKM AnalytiCup Machin.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UABHUT5J","blogPost","2014","Brownlee, Jason","Discover Feature Engineering, How to Engineer Features and How to Get Good at It","Machine Learning Mastery","","","","https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/","Feature engineering is an informal topic, but one that is absolutely known and agreed to be key to success in applied machine learning. In creating this guide I went wide and deep and synthesized all of the material I could. You will discover what feature engineering is, what problem it solves, why it matters, how […]","2014-09-25","2021-03-03 13:43:46","2021-03-03 13:43:46","2021-03-03 13:43:46","","","","","","","","","","","","","","en-US","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\UQGBTCZ8\Brownlee - 2014 - Discover Feature Engineering, How to Engineer Feat.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6BQENYAK","book","2017","Nicholson, T.A.J.","Optimization in Industry","","978-3-030-01640-1","","","","This book describes different approaches for solving industrial problems like product design, process optimization, quality enhancement, productivity improvement and cost minimization. Several optimization techniques are described. The book covers case studies on the applications of classical as well as evolutionary and swarm optimization tools for solving industrial issues. The content is very helpful for industry personnel, particularly engineers from the Operation, R&D and Quality Assurance sectors, and also the academic researchers of different engineering and/or business administration background. Optimization is a standard activity in everyday life. We use it consciously or unconsciously for almost all our daily jobs. When we try to find a way to finish an assignment in our professional life in less effort or time, it is an optimization procedure. Deciding the best way to reach the workplace from the time and distance aspects, depending on the time of the day, is also an optimization. Many people in this world try to reduce the daily cost of living for their survival. All these actions have some amount of mental calculations behind it, and most of the time the calculation is not that mathematical. The optimization process basically finds the values of the variables those control the objective we need to optimize (i.e., min- imize or maximize) while satisfying some constraints. This process becomes mathematical when we employ it in the professional sectors like finance, con- struction, manufacturing, etc. In those cases, the number of variables is quite high, and they are correlated in a complex way. Mathematical optimization has various components. The first is the objective function, which defines the attribute to be optimized in terms of the dependent variables or design variables. For example, in manufacturing process, it describes the profit or the cost or the product quality. The design variables are the variables which control the value of the attribute, which is being optimized. The amounts of different resources used and the time spent in a manufacturing process may be considered as the design variables. The third important component in an opti- mization process is the constraint. It may be single or a set of constraints, which allow the process to take on certain values of the design variables but exclude others. Among the different approaches of optimization, the classical derivative-based approaches were the most established and common optimization methods. But in recent years, advent of metaheuristic methods has changed the domain of opti- mization and made those methods more acceptable due to their ability to handle complex problems and lesser possibility of getting stuck in the local optima. A metaheuristic method has the strategy to guide and modify the pure heuristics to produce better solutions which are beyond the solutions generated in heuristic processes. The metaheuristic optimization algorithms consist of the evolutionary algorithms, the swarm intelligence, other bio- and nature-inspired algorithms, and some Physics and process-based algorithms. In this book, most of the chapters deal with several case studies related to application of these metaheuristic algorithms in different domains of industry. Optimizations methods are extensively being applied to different sectors of the industries and professional life, which includes the information system, the financial operations, the manufacturing systems, engineering design and design optimization, operations and supply chain management, internet of things, and multicriteria decision-making. Various metaheuristic optimization tools are being used in these spheres of life. In this book, after a brief description of the metaheuristic tools in the first chapter, eleven more chapters have been dedicated for dealing with such applications of the optimization techniques in industrially relevant fields. The authors from different countries have shared their experience in the optimization of systems like banking, steel making, manufacturing processes, electrical vehicles design, and civil constructions. The editors express their gratitude to all the authors for their effort to make excellent contributions for the book. The editors are also grateful to all the reviewers who have taken the pain to improve the quality of the chapters further and make the book even better. The colleagues, friends, and family members of both the editors are gratefully acknowledged. The editors also acknowledge the brilliance of the Springer team shaping the compilation beautifully and express thanks to them.","2017","2021-02-10 03:05:13","2021-02-10 03:05:26","","","","","","","","","","","","","","","","","","","","","","Publication Title: Optimization in Industry DOI: 10.4324/9781315125824","","C:\Users\ambreen.hanif\Zotero\storage\KT2IV7ZU\2019_Book_OptimizationInIndustry.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"28JKBDFF","journalArticle","2011","Chen, Mu-Yen","Bankruptcy prediction in firms with statistical and intelligent techniques and a comparison of evolutionary computation approaches","Computers & Mathematics with Applications","","0898-1221","10.1016/j.camwa.2011.10.030","https://www.sciencedirect.com/science/article/pii/S0898122111008947","In this paper, we compare some traditional statistical methods for predicting financial distress to some more “unconventional” methods, such as decision tree classification, neural networks, and evolutionary computation techniques, using data collected from 200 Taiwan Stock Exchange Corporation (TSEC) listed companies. Empirical experiments were conducted using a total of 42 ratios including 33 financial, 8 non-financial and 1 combined macroeconomic index, using principle component analysis (PCA) to extract suitable variables. This paper makes four critical contributions: (1) with nearly 80% fewer financial ratios by the PCA method, the prediction performance is still able to provide highly-accurate forecasts of financial bankruptcy; (2) we show that traditional statistical methods are better able to handle large datasets without sacrificing prediction performance, while intelligent techniques achieve better performance with smaller datasets and would be adversely affected by huge datasets; (3) empirical results show that C5.0 and CART provide the best prediction performance for imminent bankruptcies; and (4) Support Vector Machines (SVMs) with evolutionary computation provide a good balance of high-accuracy short- and long-term performance predictions for healthy and distressed firms. Therefore, the experimental results show that the Particle Swarm Optimization (PSO) integrated with SVM (PSO–SVM) approach could be considered for predicting potential financial distress.","2011-12-01","2021-02-10 02:50:54","2021-02-10 02:50:54","2021-02-10 02:50:54","4514-4524","","12","62","","Computers & Mathematics with Applications","","","","","","","","en","","","","","ScienceDirect","","","","C:\Users\ambreen.hanif\Zotero\storage\RBMGWZAQ\Chen - 2011 - Bankruptcy prediction in firms with statistical an.pdf","","","Decision tree classification; Financial bankruptcy prediction; Support vector machine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SSTKYT2C","conferencePaper","2001","Santini, Massimo; Tettamanzi, Andrea","Genetic Programming for Financial Time Series Prediction","Proceedings of the 4th European Conference on Genetic Programming","978-3-540-41899-3","","","","This paper describes an application of genetic programming to forecasting financial markets that allowed the authors to rank first in a competition organized within the CEC2000 on ""Dow Jones Prediction"". The approach is substantially driven by the rules of that competition, and is characterized by individuals being made up of multiple GP expressions and specific genetic operators.","2001-04-18","2021-02-10 02:39:27","2021-02-10 02:39:27","2021-02-09","361–370","","","","","","","EuroGP '01","","","","Springer-Verlag","Berlin, Heidelberg","","","","","","ACM Digital Library","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PA5VDWB4","journalArticle","2020","Brabazon, Anthony; Kampouridis, Michael; O’Neill, Michael","Applications of genetic programming to finance and economics: past, present, future","Genetic Programming and Evolvable Machines","","1573-7632","10.1007/s10710-019-09359-z","https://doi.org/10.1007/s10710-019-09359-z","While the origins of genetic programming (GP) stretch back over 50 years, the field of GP was invigorated by John Koza’s popularisation of the methodology in the 1990s. A particular feature of the GP literature since then has been a strong interest in the application of GP to real-world problem domains. One application domain which has attracted significant attention is that of finance and economics, with several hundred papers from this subfield being listed in the GP bibliography. In this article we outline why finance and economics has been a popular application area for GP and briefly indicate the wide span of this work. However, despite this research effort there is relatively scant evidence of the usage of GP by the mainstream finance community in academia or industry. We speculate why this may be the case, describe what is needed to make this research more relevant from a finance perspective, and suggest some future directions for the application of GP in finance and economics.","2020-06-01","2021-02-10 02:27:49","2021-02-10 02:27:49","2021-02-10 02:27:48","33-53","","1","21","","Genet Program Evolvable Mach","Applications of genetic programming to finance and economics","","","","","","","en","","","","","Springer Link","","","","C:\Users\ambreen.hanif\Zotero\storage\MSTZJQ5U\Brabazon et al. - 2020 - Applications of genetic programming to finance and.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EB8Z9URI","webpage","","","Anon (2018) - IMDb","","","","","https://www.imdb.com/title/tt5397194/","","","2021-02-10 02:09:12","2021-02-10 02:09:12","2021-02-10 02:09:10","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CDI6AZRE","webpage","2012","Bonde, Ganesh","Stock price prediction using genetic algorithms and evolution strategies","","","","","/paper/Stock-price-prediction-using-genetic-algorithms-and-Bonde/29fa7a9131fd975cbb81801311fbb75b4c41bf0e","To many, the stock market is a very challenging and interesting field. In this paper we try to predict whether the prices of the stocks are going to increase or decrease on the next day. We are predicting the highest stock price for eight different companies individually. For each company six attributes are used which help us to find whether the prices are going to increase or decrease. The evolutionary techniques used for this experiment are genetic algorithms and evolution strategies. Using these algorithms we are trying to find the connection weight for each attribute, which helps in predicting the highest price of the stock. The input for each attribute is given to a sigmoid function after it is amplified based on its connection weight. The experimental results show that this new way of predicting the stock price is promising. In each case the algorithms were able to predict with an accuracy of at least 70.00%. Since this approach is new any further study in this field can definitely give better results.","2012","2021-02-05 02:27:33","2021-02-05 02:27:33","2021-02-05 02:27:33","","","","","","","","","","","","","","en","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\9JS6AKZW\Bonde - 2012 - Stock price prediction using genetic algorithms an.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZV3L8Y6A","document","","","StockForecast_RRN_ICDS2019 proceding.pdf","","","","","","","","2021-01-29 00:57:22","2021-01-29 01:59:43","","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\DBSL2RFZ\StockForecast_RRN_ICDS2019 proceding.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"82HZ8LSD","document","","","ICISS 2020.pdf","","","","","","","","2021-01-29 00:57:21","2021-01-29 00:57:31","","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\QFX5V7MA\ICISS 2020.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FXL7KV9C","journalArticle","1989","Hart, Sergiu","Shapley Value","Game Theory","","","10.1007/978-1-349-20181-5_25","https://link.springer.com/chapter/10.1007/978-1-349-20181-5_25","The value of an uncertain outcome (a `gamble', `lottery', etc.) to a participant is an evaluation, in the participant's utility scale, of the prospective outcomes: It is an a priori measure of what he expects to obtain (this is the subject of `utility theory'). In a similar way, one is interested in evaluating a game; that is, measuring the value of each player in the game.","1989","2021-09-27 23:32:25","2022-12-20 05:06:50","2021-09-28","210-216","","","","","","","","","","","","","","","","","","","","31 citations (Crossref) [2022-12-20] Publisher: Palgrave Macmillan, London QID: Q56157244","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TP76CDEY","journalArticle","2017","Ratner, Alexander; Bach, Stephen H.; Ehrenberg, Henry; Fries, Jason; Wu, Sen; Ré, Christopher","Snorkel: Rapid training data creation with weak supervision","Proceedings of the VLDB Endowment","","21508097","10.14778/3157794.3157797","","Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a first-of-its-kind system that enables users to train stateof- the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the first end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a flexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8 × faster and increase predictive performance an average 45.5% versus seven hours of hand labeling. We study the modeling tradeoffs in this new setting and propose an optimizer for automating tradeoff decisions that gives up to 1.8× speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Affairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60% of the predictive performance of large hand-curated training sets.","2017","2021-02-10 03:05:12","2022-12-20 05:06:50","","269-282","","3","11","","","","","","","","","","","","","","","","","216 citations (Crossref) [2022-12-20] arXiv: 1711.10160 QID: Q53818911","","C:\Users\ambreen.hanif\Zotero\storage\4CJDJUM7\Ratner et al_2017_Snorkel.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PCTWIAXM","journalArticle","2019","Rudin, Cynthia","Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead","Nature Machine Intelligence","","","10.1038/s42256-019-0048-x","","Black box machine learning models are currently being used for high stakes decision-making throughout society, causing problems throughout healthcare, criminal justice, and in other domains. People have hoped that creating methods for explaining these black box models will alleviate some of these problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practices and can potentially cause catastrophic harm to society. There is a way forward-it is to design models that are inherently interpretable. This manuscript clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare, and computer vision.","2019","2021-09-28 06:03:31","2022-12-20 05:06:47","2021-09-28","206-215","","5","1","","","","","","","","","","","","","","","","","1738 citations (Crossref) [2022-12-20] arXiv: 1811.10154v3 QID: Q102362865","","C:\Users\ambreen.hanif\Zotero\storage\Y4UG6S4R\Rudin_Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TXK3VMHK","journalArticle","2018","Tong, Chao; Roberts, Richard; Borgo, Rita; Walton, Sean; Laramee, Robert S.; Wegba, Kodzo; Lu, Aidong; Wang, Yun; Qu, Huamin; Luo, Qiong; Ma, Xiaojuan","Storytelling and Visualization: An Extended Survey","Information 2018, Vol. 9, Page 65","","20782489","10.3390/INFO9030065","https://www.mdpi.com/2078-2489/9/3/65/htm","Throughout history, storytelling has been an effective way of conveying information and knowledge. In the field of visualization, storytelling is rapidly gaining momentum and evolving cutting-edge techniques that enhance understanding. Many communities have commented on the importance of storytelling in data visualization. Storytellers tend to be integrating complex visualizations into their narratives in growing numbers. In this paper, we present a survey of storytelling literature in visualization and present an overview of the common and important elements in storytelling visualization. We also describe the challenges in this field as well as a novel classification of the literature on storytelling in visualization. Our classification scheme highlights the open and unsolved problems in this field as well as the more mature storytelling sub-fields. The benefits offer a concise overview and a starting point into this rapidly evolving research trend and provide a deeper understanding of this topic.","2018-03-14","2022-01-24 00:06:19","2022-12-20 05:06:45","2022-01-24","65","","3","9","","","","","","","","","","","","","","","","","42 citations (Crossref) [2022-12-20] Publisher: Multidisciplinary Digital Publishing Institute","","; C:\Users\ambreen.hanif\Zotero\storage\I8RVHMRQ\Tong et al_2018_Storytelling and Visualization.pdf","https://www.mdpi.com/2078-2489/9/3/65","","information visualization; narrative; scientific visualization; storytelling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QHAT8DKH","journalArticle","2019","Brown, Noam; Sandholm, Tuomas","Superhuman AI for multiplayer poker","Science","","0036-8075","10.1126/SCIENCE.AAY2400","https://science.sciencemag.org/content/365/6456/885","Computer programs have shown superiority over humans in two-player games such as chess, Go, and heads-up, no-limit Texas hold'em poker. However, poker games usually include six players—a much trickier challenge for artificial intelligence than the two-player variant. Brown and Sandholm developed a program, dubbed Pluribus, that learned how to play six-player no-limit Texas hold'em by playing against five copies of itself (see the Perspective by Blair and Saffidine). When pitted against five elite professional poker players, or with five copies of Pluribus playing against one professional, the computer performed significantly better than humans over the course of 10,000 hands of poker. Science , this issue p. [885][1]; see also p. [864][2] In recent years there have been great strides in artificial intelligence (AI), with games often serving as challenge problems, benchmarks, and milestones for progress. Poker has served for decades as such a challenge problem. Past successes in such benchmarks, including poker, have been limited to two-player games. However, poker in particular is traditionally played with more than two players. Multiplayer games present fundamental additional issues beyond those in two-player games, and multiplayer poker is a recognized AI milestone. In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold’em poker, the most popular form of poker played by humans.  [1]: /lookup/doi/10.1126/science.aay2400  [2]: /lookup/doi/10.1126/science.aay7774","2019-08-30","2021-08-26 01:44:32","2022-12-20 05:06:44","2021-08-26","885-890","","6456","365","","","","","","","","","","","","","","","","","171 citations (Crossref) [2022-12-20] PMID: 31296650 Publisher: American Association for the Advancement of Science Citation Key: Brown2019 QID: Q91807888","","; C:\Users\ambreen.hanif\Zotero\storage\MXMKJIU7\Brown_Sandholm_2019_Superhuman AI for multiplayer poker.pdf","https://science.sciencemag.org/content/365/6456/885.abstract","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L7F5S9JJ","journalArticle","2019","Jobin, Anna; Ienca, Marcello; Vayena, Effy","The global landscape of AI ethics guidelines","Nature Machine Intelligence 2019 1:9","","2522-5839","10.1038/s42256-019-0088-2","https://www.nature.com/articles/s42256-019-0088-2","In the past five years, private companies, research institutions and public sector organizations have issued principles and guidelines for ethical artificial intelligence (AI). However, despite an apparent agreement that AI should be ‘ethical’, there is debate about both what constitutes ‘ethical AI’ and which ethical requirements, technical standards and best practices are needed for its realization. To investigate whether a global agreement on these questions is emerging, we mapped and analysed the current corpus of principles and guidelines on ethical AI. Our results reveal a global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy), with substantive divergence in relation to how these principles are interpreted, why they are deemed important, what issue, domain or actors they pertain to, and how they should be implemented. Our findings highlight the importance of integrating guideline-development efforts with substantive ethical analysis and adequate implementation strategies. As AI technology develops rapidly, it is widely recognized that ethical guidelines are required for safe and fair implementation in society. But is it possible to agree on what is ‘ethical AI’? A detailed analysis of 84 AI ethics reports around the world, from national and international organizations, companies and institutes, explores this question, finding a convergence around core principles but substantial divergence on practical implementation.","2019-09-02","2021-09-29 01:25:40","2022-12-20 05:06:42","2021-09-29","389-399","","9","1","","","","","","","","","","","","","","","","","836 citations (Crossref) [2022-12-20] Publisher: Nature Publishing Group QID: Q110620976","","","","","Ethics; Information systems and information technology; Information technology; Science; technology and society","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z3HRC8NW","journalArticle","2003","Dzindolet, Mary T.; Peterson, Scott A.; Pomranky, Regina A.; Pierce, Linda G.; Beck, Hall P.","The role of trust in automation reliance","International Journal of Human-Computer Studies","","1071-5819","10.1016/S1071-5819(03)00038-7","","A recent and dramatic increase in the use of automation has not yielded comparable improvements in performance. Researchers have found human operators often underutilize (disuse) and overly rely on (misuse) automated aids (Parasuraman and Riley, 1997). Three studies were performed with Cameron University students to explore the relationship among automation reliability, trust, and reliance. With the assistance of an automated decision aid, participants viewed slides of Fort Sill terrain and indicated the presence or absence of a camouflaged soldier. Results from the three studies indicate that trust is an important factor in understanding automation reliance decisions. Participants initially considered the automated decision aid trustworthy and reliable. After observing the automated aid make errors, participants distrusted even reliable aids, unless an explanation was provided regarding why the aid might err. Knowing why the aid might err increased trust in the decision aid and increased automation reliance, even when the trust was unwarranted. Our studies suggest a need for future research focused on understanding automation use, examining individual differences in automation reliance, and developing valid and reliable self-report measures of trust in automation.","2003-06-01","2022-06-06 02:08:11","2022-12-20 05:06:41","2022-06-06","697-718","","6","58","","","","","","","","","","","","","","","","","559 citations (Crossref) [2022-12-20] Publisher: Academic Press QID: Q102362859","","","","","Automation reliance; Automation trust; Disuse; Misuse","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R7BW3H2L","journalArticle","2006","Leban, Gregor; Zupan, Blaž; Vidmar, Gaj; Bratko, Ivan","VizRank: Data Visualization Guided by Machine Learning","Data Mining and Knowledge Discovery 2006 13:2","","1573-756X","10.1007/S10618-005-0031-5","https://link.springer.com/article/10.1007/s10618-005-0031-5","Data visualization plays a crucial role in identifying interesting patterns in exploratory data analysis. Its use is, however, made difficult by the large number of possible data projections showing different attribute subsets that must be evaluated by the data analyst. In this paper, we introduce a method called VizRank, which is applied on classified data to automatically select the most useful data projections. VizRank can be used with any visualization method that maps attribute values to points in a two-dimensional visualization space. It assesses possible data projections and ranks them by their ability to visually discriminate between classes. The quality of class separation is estimated by computing the predictive accuracy of k-nearest neighbor classifier on the data set consisting of x and y positions of the projected data points and their class information. The paper introduces the method and presents experimental results which show that VizRank's ranking of projections highly agrees with subjective rankings by data analysts. The practical use of VizRank is also demonstrated by an application in the field of functional genomics.","2006-05-16","2021-11-25 02:31:37","2022-12-20 05:06:40","2021-11-25","119-136","","2","13","","","","","","","","","","","","","","","","","56 citations (Crossref) [2022-12-20] Publisher: Springer QID: Q57681823","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TMM28A85","journalArticle","2019","Samek, Wojciech; Müller, Klaus-Robert","Towards Explainable Artificial Intelligence","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","","","10.1007/978-3-030-28954-6_1","https://doi.org/10.1007/978-3-030-28954-6_1.","In recent years, machine learning (ML) has become a key enabling technology for the sciences and industry. Especially through improvements in methodology, the availability of large databases and increased computational power, today's ML algorithms are able to achieve excellent performance (at times even exceeding the human level) on an increasing number of complex tasks. Deep learning models are at the forefront of this development. However, due to their nested non-linear structure, these powerful models have been generally considered ""black boxes"", not providing any information about what exactly makes them arrive at their predictions. Since in many applications, e.g., in the medical domain, such lack of transparency may be not acceptable, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This introductory paper presents recent developments and applications in this field and makes a plea for a wider use of explainable learning algorithms in practice.","2019","2021-09-28 05:32:11","2022-12-20 05:06:40","2021-09-28","5-22","","","11700 LNCS","","","","","","","","","","","","","","","","","144 citations (Crossref) [2022-12-20] arXiv: 1909.12072v1 Citation Key: Samek2019 QID: Q102362940","","C:\Users\ambreen.hanif\Zotero\storage\D28654GA\Samek_Müller_2019_Towards Explainable Artificial Intelligence.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PNFNZI24","journalArticle","","Mahoney e-Discovery Cleary Gottlieb Steen, Christian J; LLP Washington, Hamilton DC; Zhang, Jianping; Huber-Fliflet, Nathaniel; Gronvall, Peter; Zhao, Haozhen","A Framework for Explainable Text Classification in Legal Document Review","","","","","","Companies regularly spend millions of dollars producing electronically-stored documents in legal matters. Over the past two decades, attorneys have been using a variety of technologies to conduct this exercise, and most recently, parties on both sides of the 'legal aisle' are accepting the use of machine learning techniques like text classification to cull massive volumes of data and to identify responsive documents for use in these matters. While text classification is regularly used to reduce the discovery costs in legal matters, text classification also faces a peculiar perception challenge: amongst lawyers, this technology is sometimes looked upon as a ""black box."" Put simply, very little information is provided for attorneys to understand why documents are classified as responsive. In recent years, a group of AI and Machine Learning researchers have been actively researching Explainable AI. In an explainable AI system, actions or decisions are human understandable. In legal 'document review' scenarios, a document can be identified as responsive, as long as one or more of the text snippets (small passages of text) in a document are deemed responsive. In these scenarios, if text classification can be used to locate these responsive snippets, then attorneys could easily evaluate the model's document classification decision. When deployed with defined and explainable results, text classification can drastically enhance the overall quality and speed of the document review process by reducing the time it takes to review documents. Moreover, explainable predictive coding provides lawyers with greater confidence in the results of that supervised learning task. This paper describes a framework for explainable text classification as a valuable tool in legal services: for enhancing the quality and efficiency of legal document review and for assisting in locating responsive snippets within responsive documents. This framework has been implemented in our legal analytics product, which has been used in hundreds of legal matters. We also report our experimental results using the data from an actual legal matter that used this type of document review.","","2021-09-28 05:32:50","2022-12-20 05:06:39","2021-09-28","","","","","","","","","","","","","","","","","","","","","Citation Key: Mahoneye-DiscoveryClearyGottliebSteen","","C:\Users\ambreen.hanif\Zotero\storage\BM4KW6YK\Mahoney e-Discovery Cleary Gottlieb Steen et al_A Framework for Explainable Text Classification in Legal Document Review.pdf","","","⛔ No DOI found; explainable AI; false negatives; legal document review; machine learning; predictive coding; text categorization; text classification; XAI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LR3R3CA7","journalArticle","2020","Danilevsky, Marina; Qian, Kun; Aharonov, Ranit; Katsis, Yannis; Kawas, Ban; Sen, Prithviraj","A Survey of the State of Explainable AI for Natural Language Processing","arXiv","","","","http://arxiv.org/abs/2010.00711","Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.","2020-10-01","2021-05-10 03:48:02","2022-12-20 05:06:39","2021-05-10","","","","","","","","","","","","","","","","","","","","","arXiv: 2010.00711 Publisher: arXiv","","C:\Users\ambreen.hanif\Zotero\storage\5XWD5JPB\full-text.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I9MZQ378","journalArticle","2017","Sundararajan, Mukund; Taly, Ankur; Yan, Qiqi","Axiomatic Attribution for Deep Networks","34th International Conference on Machine Learning, ICML 2017","","","","https://arxiv.org/abs/1703.01365v2","We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.","2017-03-04","2021-10-28 00:11:14","2022-12-20 05:06:38","2021-10-28","5109-5118","","","7","","","","","","","","","","","","","","","","","arXiv: 1703.01365 Publisher: International Machine Learning Society (IMLS)","","C:\Users\ambreen.hanif\Zotero\storage\7SG4W8LK\full-text.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"47UJHAXL","journalArticle","","Cyras, Kristijonaš; Rago, Antonio; Albini, Emanuele; Baroni, Pietro; Toni, Francesca","Argumentative XAI: A Survey *","","","","","","Explainable AI (XAI) has been investigated for decades and, together with AI itself, has witnessed unprecedented growth in recent years. Among various approaches to XAI, argumentative models have been advocated in both the AI and social science literature , as their dialectical nature appears to match some basic desirable features of the explanation activity. In this survey we overview XAI approaches built using methods from the field of computational argumentation, leveraging its wide array of reasoning abstractions and explanation delivery methods. We overview the literature focusing on different types of explanation (intrinsic and post-hoc), different models with which argumentation-based explanations are deployed, different forms of delivery , and different argumentation frameworks they use. We also lay out a roadmap for future work.","","2022-07-05 00:00:37","2022-12-20 05:06:37","2022-07-05","","","","","","","","","","","","","","","","","","","","","arXiv: 2105.11266v1","","C:\Users\ambreen.hanif\Zotero\storage\ISXJJZRL\Cyras et al_Argumentative XAI.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VMASA4GE","journalArticle","2020","Wexler, James; Pushkarna, Mahima; Bolukbasi, Tolga; Wattenberg, Martin; Viegas, Fernanda; Wilson, Jimbo","The what-if tool: Interactive probing of machine learning models","IEEE Transactions on Visualization and Computer Graphics","","","10.1109/TVCG.2019.2934619","","A key challenge in developing and deploying Machine Learning (ML) systems is understanding their performance across a wide range of inputs. To address this challenge, we created the What-If Tool, an open-source application that allows practitioners to probe, visualize, and analyze ML systems, with minimal coding. The What-If Tool lets practitioners test performance in hypothetical situations, analyze the importance of different data features, and visualize model behavior across multiple models and subsets of input data. It also lets practitioners measure systems according to multiple ML fairness metrics. We describe the design of the tool, and report on real-life usage at different organizations.","2020-01-01","2021-10-26 04:21:41","2022-12-20 05:06:35","2021-10-26","56-65","","1","26","","","","","","","","","","","","","","","","","67 citations (Crossref) [2022-12-20] Publisher: IEEE Computer Society QID: Q92821750","","C:\Users\ambreen.hanif\Zotero\storage\MZJQG32T\full-text.pdf","","","Interactive Machine Learning; Model Comparison; Model Debugging","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PY88QFLC","journalArticle","2020","Riddell, Graeme A.; van Delden, Hedwig; Maier, Holger R.; Zecchin, Aaron C.","Tomorrow's disasters – Embedding foresight principles into disaster risk assessment and treatment","International Journal of Disaster Risk Reduction","","2212-4209","10.1016/J.IJDRR.2019.101437","","Disaster risk is a complex, uncertain and evolving threat to society which changes based on broad drivers of hazard, exposure and vulnerability such as population, economic and climatic change, along with new technologies and social preferences. It also evolves as a function of decisions of public policy and public/private investment which alters future risk profiles. These factors however are often not captured within disaster risk assessments and explicitly excluded from the UN General Assembly definition of a disaster risk assessment which focuses on the current state of risk. This means that 1) we cannot adequately capture changes in risk and risk assessments are out of date as soon as published but also 2) we cannot show the benefit of proactive risk treatments in our risk assessments. This paper therefore outlines a generic, scale-neutral, framework for integrating foresight – thinking about the future – into risk assessment methodologies. This is demonstrated by its application to a disaster risk assessment of heatwave risk in Tasmania, Australia, and shows how risk changes across three future scenarios and what proactive treatments could be possible mitigating the identified drivers of future risk.","2020-05-01","2022-04-04 02:01:07","2022-12-20 05:06:34","2022-04-04","101437","","","45","","","","","","","","","","","","","","","","","5 citations (Crossref) [2022-12-20] Publisher: Elsevier","","","","","Disaster risk management; Foresight; Risk assessment; Risk treatment; Scenarios","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VN58KZIK","journalArticle","2020","Choi, Jaekeol; Choi, Jungin; Rhee, Wonjong","Interpreting Neural Ranking Models using Grad-CAM","arXiv preprint arXiv:2005.05768","","","","https://arxiv.org/abs/2005.05768v1","Recently, applying deep neural networks in IR has become an important and timely topic. For instance, Neural Ranking Models(NRMs) have shown promising performance compared to the traditional ranking models. However, explaining the ranking results has become even more difficult with NRM due to the complex structure of neural networks. On the other hand, a great deal of research is under progress on Interpretable Machine Learning(IML), including Grad-CAM. Grad-CAM is an attribution method and it can visualize the input regions that contribute to the network's output. In this paper, we adopt Grad-CAM for interpreting the ranking results of NRM. By adopting Grad-CAM, we analyze how each query-document term pair contributes to the matching score for a given pair of query and document. The visualization results provide insights on why a certain document is relevant to the given query. Also, the results show that neural ranking model captures the subtle notion of relevance. Our interpretation method and visualization results can be used for snippet generation and user-query intent analysis.","2020-05-12","2021-10-28 00:56:20","2022-12-20 05:06:34","2021-10-28","","","","","","","","","","","","","","","","","","","","","arXiv: 2005.05768","","C:\Users\ambreen.hanif\Zotero\storage\RVETNEUK\full-text.pdf","","","⛔ No DOI found; attribution method; Grad-CAM; Interpretablity; neural ranking model","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y5FUWP9R","journalArticle","2021","Averkin, A.; Yarushev, S.","Fuzzy Rules Extraction from Deep Neural Networks","","","","","","","2021","2021-09-30 01:47:33","2022-12-20 05:06:34","2021-09-30","","","","","","","","","","","","","","","","","","","","","","","","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3PRQKZK8","journalArticle","2018","Hagras, Hani","Toward Human-Understandable, Explainable AI","Computer","","15580814","10.1109/MC.2018.3620965","","Recent increases in computing power, coupled with rapid growth in the availability and quantity of data have rekindled our interest in the theory and applications of artificial intelligence (AI). However, for AI to be confidently rolled out by industries and governments, users want greater transparency through explainable AI (XAI) systems. The author introduces XAI concepts, and gives an overview of areas in need of further exploration-such as type-2 fuzzy logic systems-to ensure such systems can be fully understood and analyzed by the lay user.","2018-09-01","2021-11-25 00:30:41","2022-12-20 05:06:33","2021-11-25","28-36","","9","51","","","","","","","","","","","","","","","","","134 citations (Crossref) [2022-12-20] Publisher: IEEE Computer Society QID: Q102633368","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GARX92KQ","journalArticle","2018","Hoffman, Robert R.; Mueller, Shane T.; Klein, Gary; Litman, Jordan","Metrics for Explainable AI: Challenges and Prospects","arXiv preprint arXiv:1812.04608","","","","https://arxiv.org/abs/1812.04608v2","The question addressed in this paper is: If we present to a user an AI system that explains how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? In other words, how do we know that an explanainable AI system (XAI) is any good? Our focus is on the key concepts of measurement. We discuss specific methods for evaluating: (1) the goodness of explanations, (2) whether users are satisfied by explanations, (3) how well users understand the AI systems, (4) how curiosity motivates the search for explanations, (5) whether the user's trust and reliance on the AI are appropriate, and finally, (6) how the human-XAI work system performs. The recommendations we present derive from our integration of extensive research literatures and our own psychometric evaluations.","2018-12-11","2021-09-28 04:05:28","2022-12-20 05:06:33","2021-09-28","","","","","","","","","","","","","","","","","","","","","arXiv: 1812.04608","","C:\Users\ambreen.hanif\Zotero\storage\WGTD37BF\Hoffman et al_2018_Metrics for Explainable AI.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9WVKV8MF","journalArticle","2019","Schlegel, Udo; Arnout, Hiba; El-Assady, Mennatallah; Oelke, Daniela; Keim, Daniel A.","Towards a rigorous evaluation of XAI methods on time series","Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019","","","10.1109/ICCVW.2019.00516","","Explainable Artificial Intelligence (XAI) methods are typically deployed to explain and debug black-box machine learning models. However, most proposed XAI methods are black-boxes themselves and designed for images. Thus, they rely on visual interpretability to evaluate and prove explanations. In this work, we apply XAI methods previously used in the image and text-domain on time series. We present a methodology to test and evaluate various XAI methods on time series by introducing new verification techniques to incorporate the temporal dimension. We further conduct preliminary experiments to assess the quality of selected XAI method explanations with various verification methods on a range of datasets and inspecting quality metrics on it. We demonstrate that in our initial experiments, SHAP works robust for all models, but others like DeepLIFT, LRP, and Saliency Maps work better with specific architectures.","2019-10-01","2021-09-30 01:57:02","2022-12-20 05:06:32","2021-09-30","4197-4201","","","","","","","","","","","","","","","","","","","","32 citations (Crossref) [2022-12-20] Publisher: Institute of Electrical and Electronics Engineers Inc.","","","","","Explainable-ai; Explainable-ai-evaluation; Time-Series","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TIZ66AIP","journalArticle","2019","Kursuncu, Ugur; Gaur, Manas; Sheth, Amit","Knowledge Infused Learning (K-IL): Towards Deep Incorporation of Knowledge in Deep Learning","CEUR Workshop Proceedings","","","","http://arxiv.org/abs/1912.00512","Learning the underlying patterns in data goes beyond instance-based generalization to external knowledge represented in structured graphs or networks. Deep learning that primarily constitutes neural computing stream in AI has shown significant advances in probabilistically learning latent patterns using a multi-layered network of computational nodes (i.e., neurons/hidden units). Structured knowledge that underlies symbolic computing approaches and often supports reasoning, has also seen significant growth in recent years, in the form of broad-based (e.g., DBPedia, Yago) and domain, industry or application specific knowledge graphs. A common substrate with careful integration of the two will raise opportunities to develop neuro-symbolic learning approaches for AI, where conceptual and probabilistic representations are combined. As the incorporation of external knowledge will aid in supervising the learning of features for the model, deep infusion of representational knowledge from knowledge graphs within hidden layers will further enhance the learning process. Although much work remains, we believe that knowledge graphs will play an increasing role in developing hybrid neuro-symbolic intelligent systems (bottom-up deep learning with top-down symbolic computing) as well as in building explainable AI systems for which knowledge graphs will provide scaffolding for punctuating neural computing. In this position paper, we describe our motivation for such a neuro-symbolic approach and framework that combines knowledge graph and neural networks.","2019-12-01","2021-06-15 02:43:50","2022-12-20 05:06:32","2021-06-15","","","","2600","","","","","","","","","","","","","","","","","arXiv: 1912.00512 Publisher: CEUR-WS","","C:\Users\ambreen.hanif\Zotero\storage\RZ56M4GC\Kursuncu et al_2019_Knowledge Infused Learning (K-IL).pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WSNSUGYE","journalArticle","","Cavanillas, José María; Curry, · Edward; Wahlster, Wolfgang","New Horizons for a Data-Driven Economy A Roadmap for Usage and Exploitation of Big Data in Europe","","","","","","","","2022-04-14 03:20:51","2022-12-20 05:06:31","2022-04-14","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\5KKXSMIU\full-text.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PMH8CSV6","journalArticle","","Schwalbe, Gesina","XAI Method Properties: A (Meta-)study","","","","","","In the meantime, a wide variety of terminologies, motivations , approaches and evaluation criteria have been developed within the scope of research on explainable artificial intelligence (XAI). Many taxonomies can be found in the literature, each with a different focus, but also showing many points of overlap. In this paper, we summarize the most cited and current taxonomies in a meta-analysis in order to highlight the essential aspects of the state-of-the-art in XAI. We also present and add terminologies as well as concepts from a large number of survey articles on the topic. Last but not least, we illustrate concepts from the higher-level taxonomy with more than 50 example methods, which we categorize accordingly, thus providing a wide-ranging overview of aspects of XAI and paving the way for use case-appropriate as well as context-specific subsequent research.","","2021-11-25 02:28:59","2022-12-20 05:06:30","2021-11-25","","","","","","","","","","","","","","","","","","","","","arXiv: 2105.07190v1","","C:\Users\ambreen.hanif\Zotero\storage\UNV936K7\full-text.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RYMAJHY3","journalArticle","","Ribeiro, Marco Tulio; Singh, Sameer; Guestrin, Carlos","Model-Agnostic Interpretability of Machine Learning","","","","","","Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to inter-pretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison , and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.","","2022-07-05 00:09:25","2022-12-20 05:06:28","2022-07-05","","","","","","","","","","","","","","","","","","","","","arXiv: 1606.05386v1","","C:\Users\ambreen.hanif\Zotero\storage\CTFR27VV\Ribeiro et al_Model-Agnostic Interpretability of Machine Learning.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XZ7MYZ6Z","journalArticle","2015","Ronneberger, Olaf; Fischer, Philipp; Brox, Thomas","U-Net: Convolutional Networks for Biomedical Image Segmentation","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","","16113349","10.1007/978-3-319-24574-4_28","https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28","There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples...","2015","2022-06-06 02:03:35","2022-12-20 05:06:28","2022-06-06","234-241","","","9351","","","","","","","","","","","","","","","","","14331 citations (Crossref) [2022-12-20] Publisher: Springer, Cham ISBN: 9783319245737 QID: Q104451999","","C:\Users\ambreen.hanif\Zotero\storage\K2MNI5BT\Ronneberger et al_2015_U-Net.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ARX2JTHG","journalArticle","2020","Das, Arun; Rad, Paul","Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey","","","","","https://arxiv.org/abs/2006.11371v2","Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.","2020-06-16","2021-10-14 04:36:18","2022-12-20 05:06:28","2021-10-14","","","","","","","","","","","","","","","","","","","","","arXiv: 2006.11371","","C:\Users\ambreen.hanif\Zotero\storage\TR8BZ674\Das_Rad_2020_Opportunities and Challenges in Explainable Artificial Intelligence (XAI).pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SUAPR8IB","journalArticle","2016","Nguyen, Anh; Dosovitskiy, Alexey; Yosinski, Jason; Brox, Thomas; Clune, Jeff","Synthesizing the preferred inputs for neurons in neural networks via deep generator networks","Advances in neural information processing systems","","","","","Deep neural networks (DNNs) have demonstrated state-of-the-art results on many pattern recognition tasks, especially vision classification problems. Understanding the inner workings of such computational brains is both fascinating basic science that is interesting in its own right-similar to why we study the human brain-and will enable researchers to further improve DNNs. One path to understanding how a neural network functions internally is to study what each of its neurons has learned to detect. One such method is called activation maximization (AM), which synthesizes an input (e.g. an image) that highly activates a neuron. Here we dramatically improve the qualitative state of the art of activation maximization by harnessing a powerful, learned prior: a deep generator network (DGN). The algorithm (1) generates qualitatively state-of-the-art synthetic images that look almost real, (2) reveals the features learned by each neuron in an interpretable way, (3) generalizes well to new datasets and somewhat well to different network architectures without requiring the prior to be relearned, and (4) can be considered as a high-quality generative method (in this case, by generating novel, creative, interesting, recognizable images).","2016","2021-09-27 07:19:10","2022-12-20 05:06:27","2021-09-27","3387-3395","","","29","","","","","","","","","","","","","","","","","QID: Q46994068","","C:\Users\ambreen.hanif\Zotero\storage\NQLVSLWS\Nguyen et al_2016_Synthesizing the preferred inputs for neurons in neural networks via deep.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HQQYAENI","journalArticle","2008","Liu, Hsiang Hsi; Ong, Chorng Shyong","Variable selection in clustering for marketing segmentation using genetic algorithms","Expert Systems with Applications","","09574174","10.1016/j.eswa.2006.09.039","","Marketing segmentation is widely used for targeting a smaller market and is useful for decision makers to reach all customers effectively with one basic marketing mix. Although clustering algorithms is popularly employed in dealing with this problem, it cannot be useful unless irrelevant variables are removed because irrelevant variables will distort the clustering structure and make the results useless. In this paper, genetic algorithms (GA) is used for variable selection and for determining the numbers of clusters. A real case of bank data set is used for illustrating the application of marketing segmentation. The results show that variable selection through GA can effectively find the global optimum solution, and the accuracy of the classified model is dramatically increased after clustering. © 2006 Elsevier Ltd. All rights reserved.","2008-01-01","2021-06-02 23:52:11","2022-12-20 05:06:27","2021-06-03","502-510","","1","34","","","","","","","","","","","","","","","","","38 citations (Crossref) [2022-12-20] Publisher: Pergamon","","C:\Users\ambreen.hanif\Zotero\storage\UW3F2U4L\Liu_Ong_2008_Variable selection in clustering for marketing segmentation using genetic.pdf","","","Clustering algorithms; Genetic algorithms; Marketing segmentation; Variables selection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6JU8X8BH","journalArticle","2020","Apley, Daniel W.; Zhu, Jingyu","Visualizing the effects of predictor variables in black box supervised learning models","Journal of the Royal Statistical Society. Series B: Statistical Methodology","","14679868","10.1111/RSSB.12377","https://arxiv.org/abs/1612.08468v2","In many supervised learning applications, understanding and visualizing the effects of the predictor variables on the predicted response is of paramount importance. A shortcoming of black box supervised learning models (e.g. complex trees, neural networks, boosted trees, random forests, nearest neighbours, local kernel-weighted methods and support vector regression) in this regard is their lack of interpretability or transparency. Partial dependence plots, which are the most popular approach for visualizing the effects of the predictors with black box supervised learning models, can produce erroneous results if the predictors are strongly correlated, because they require extrapolation of the response at predictor values that are far outside the multivariate envelope of the training data. As an alternative to partial dependence plots, we present a new visualization approach that we term accumulated local effects plots, which do not require this unreliable extrapolation with correlated predictors. Moreover, accumulated local effects plots are far less computationally expensive than partial dependence plots. We also provide an R package ALEPlot as supplementary material to implement our proposed method.","2020-09-01","2022-01-06 05:44:18","2022-12-20 05:06:26","2022-01-06","1059-1086","","4","82","","","","","","","","","","","","","","","","","221 citations (Crossref) [2022-12-20] arXiv: 1612.08468 Publisher: Blackwell Publishing Ltd","","C:\Users\ambreen.hanif\Zotero\storage\VZUANMXC\Apley_Zhu_2020_Visualizing the effects of predictor variables in black box supervised learning.pdf","","","Visualization; Functional analysis of variance; Marginal plots; Partial dependence plots; Supervised learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S68Q32WU","journalArticle","2020","Gorski, Lukasz; Ramakrishna, Shashishekar; Nowosielski, Jedrzej M.","Towards Grad-CAM Based Explainability in a Legal Text Processing Pipeline","","","","","https://arxiv.org/abs/2012.09603v1","Explainable AI(XAI)is a domain focused on providing interpretability and explainability of a decision-making process. In the domain of law, in addition to system and data transparency, it also requires the (legal-) decision-model transparency and the ability to understand the models inner working when arriving at the decision. This paper provides the first approaches to using a popular image processing technique, Grad-CAM, to showcase the explainability concept for legal texts. With the help of adapted Grad-CAM metrics, we show the interplay between the choice of embeddings, its consideration of contextual information, and their effect on downstream processing.","2020-12-15","2021-10-26 00:01:38","2022-12-20 05:06:25","2021-10-26","","","","","","","","","","","","","","","","","","","","","arXiv: 2012.09603","","C:\Users\ambreen.hanif\Zotero\storage\N3BLQXCT\Gorski et al_2020_Towards Grad-CAM Based Explainability in a Legal Text Processing Pipeline.pdf","","","⛔ No DOI found; Grad-CAM ·; Heat; Knowledge; Language; Legal; Maps· CNN; Models ·; Representation ·","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PE4L4DUI","journalArticle","2020","Zand, Shahabodin Khadivi","Towards intelligent risk-based customer segmentation in banking","arXiv","","23318422","","","Business Processes, i.e., a set of coordinated tasks and activities to achieve a business goal, and their continuous improvements are key to the operation of any organization. In banking, business processes are increasingly dynamic as various technologies have made dynamic processes more prevalent. For example, customer segmentation, i.e., the process of grouping related customers based on common activities and behaviors, could be a data-driven and knowledge-intensive process. In this paper, we present an intelligent data-driven pipeline composed of a set of processing elements to move customers’ data from one system to another, transforming the data into the contextualized data and knowledge along the way. The goal is to present a novel intelligent customer segmentation process which automates the feature engineering, i.e., the process of using (banking) domain knowledge to extract features from raw data via data mining techniques, in the banking domain. We adopt a typical scenario for analyzing customer transaction records, to highlight how the presented approach can significantly improve the quality of risk-based customer segmentation in the absence of feature engineering.As result, our proposed method is able to achieve accuracy of %91 compared to classical approaches in terms of detecting, identifying and classifying transaction to the right classification.","2020","2021-01-29 01:59:41","2022-12-20 05:06:25","","","","","","","","","","","","","","","","","","","","","","arXiv: 2009.13929","","C:\Users\ambreen.hanif\Zotero\storage\GJ8IALRW\Zand_2020_Towards intelligent risk-based customer segmentation in banking.pdf","","","⛔ No DOI found; Business Process Analytics; Customer Segmentation; Data Curation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G2CYEYVW","journalArticle","2020","Ferreira, Juliana J.; Monteiro, Mateus S.","What Are People Doing About XAI User Experience? A Survey on AI Explainability Research and Practice","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","","16113349","10.1007/978-3-030-49760-6_4","https://link.springer.com/chapter/10.1007/978-3-030-49760-6_4","Explainability is a hot topic nowadays for artificial intelligent (AI) systems. The role of machine learning (ML) models on influencing human decisions shed light on the back-box of computing systems. AI based system are more than just ML models. ML models are one element for the AI explainability’ design and needs to be combined with other elements so it can have significant meaning for people using AI systems. There are different goals and motivations for AI explainability. Regardless the goal for AI explainability, there are more to AI explanation than just ML models or algorithms. The explainability of an AI systems behavior needs to consider different dimensions: 1) who is the receiver of that explanation, 2) why that explanation is needed, and 3) in which context and other situated information the explanation is presented. Considering those three dimensions, the explanation can be effective by fitting the user needs and expectation in the right moment and format. The design of an AI explanation user experience is central for the pressing need from people and the society to understand how an AI system may impact on human decisions. In this paper, we present a literature review on AI explainability research and practices. We first looked at the computer science (CS) community research to identify the main research themes about AI explainability, or “explainable AI”. Then, we focus on Human-Computer Interaction (HCI) research trying to answer three questions about the selected publications: to whom the AI explainability is for (who), which is the purpose of the AI explanation (why), and in which context the AI explanation is presented (what + when).","2020-07-19","2021-11-24 23:44:10","2022-12-20 05:06:24","2021-11-25","56-73","","","12201 LNCS","","","","","","","","","","","","","","","","","12 citations (Crossref) [2022-12-20] Publisher: Springer, Cham ISBN: 9783030497590","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"57BS2CPE","conferencePaper","2015","Agrawal, Aishwarya; Lu, Jiasen; Antol, Stanislaw; Mitchell, Margaret; Zitnick, C Lawrence; Batra, Dhruv; Parikh, Devi","VQA: Visual Question Answering www.visualqa.org","Proceedings of the IEEE international conference on computer vision","","","","www.visualqa.org","We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ∼0.25M images, ∼0.76M questions, and ∼10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa).","2015","2021-08-26 01:36:24","2022-12-20 05:06:24","","2425-2433","","","","","","","","","","","","","","","","","","","","arXiv: 1505.00468v7 Citation Key: agarwal2015VQA","","C:\Users\ambreen.hanif\Zotero\storage\VIHUB5VU\Agrawal et al_2015_VQA.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HP82V46E","journalArticle","2019","Rudin, Cynthia; Radin, Joanna","Why Are We Using Black Box Models in AI When We Don’t Need To? A Lesson From An Explainable AI Competition","Harvard Data Science Review","","","10.1162/99608F92.5A8A3A3D","https://hdsr.mitpress.mit.edu/pub/f9kuryi8/release/4","In 2018, a landmark challenge in artificial intelligence (AI) took place, namely, the Explainable Machine Learning Challenge. The goal of the competition was to create a complicated black box model for the dataset and explain how it worked. One team did not follow the rules. Instead of sending in a black box, they created a model that was fully interpretable. This leads to the question of whether the real world of machine learning is similar to the Explainable Machine Learning Challenge, where black box models are used even when they are not needed. We discuss this team’s thought processes during the competition and their implications, which reach far beyond the competition itself.Keywords: interpretability, explainability, machine learning, finance","2019-11-22","2021-11-25 00:27:52","2022-12-20 05:06:23","2021-11-25","2019","","2","1","","","","","","","","","","","","","","","","","89 citations (Crossref) [2022-12-20] Publisher: PubPub QID: Q102633600","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XAJDHSB6","journalArticle","","Alufaisan, Yasmeen; Marusich, Laura R; Bakdash, Jonathan Z; Zhou, Yan; Kantarcioglu, Murat","Does Explainable Artificial Intelligence Improve Human Decision-Making?","","","","","https://www.","Explainable AI provides insights to users into the why for model predictions, offering potential for users to better understand and trust a model, and to recognize and correct AI predictions that are incorrect. Prior research on human and explainable AI interactions has focused on measures such as interpretability, trust, and usability of the explanation. There are mixed findings whether explainable AI can improve actual human decision-making and the ability to identify the problems with the underlying model. Using real datasets, we compare objective human decision accuracy without AI (con-trol), with an AI prediction (no explanation), and AI prediction with explanation. We find providing any kind of AI prediction tends to improve user decision accuracy, but no conclusive evidence that explainable AI has a meaningful impact. Moreover, we observed the strongest predictor for human decision accuracy was AI accuracy and that users were somewhat able to detect when the AI was correct vs. incorrect, but this was not significantly affected by including an explanation. Our results indicate that, at least in some situations, the why information provided in explainable AI may not enhance user decision-making, and further research may be needed to understand how to integrate explainable AI into real systems.","","2021-09-28 05:51:38","2022-12-20 05:06:23","2021-09-28","","","","","","","","","","","","","","","","","","","","","QID: Q102633894","","C:\Users\ambreen.hanif\Zotero\storage\2HLVSMGV\full-text.pdf; ","https://www.crossref.org/openurl?pid=zoteroDOI@wiernik.org&url_ver=Z39.88-2004&ctx_ver=Z39.88-2004&rfr_id=info%3Asid%2Fzotero.org%3A2&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rft.genre=article&rft.atitle=Does%20Explainable%20Artificial%20Intelligence%20Improve%20Human%20Decision-Making%3F&rft.aufirst=Yasmeen&rft.aulast=Alufaisan&rft.au=Yasmeen%20Alufaisan&rft.au=Laura%20R%20Marusich&rft.au=Jonathan%20Z%20Bakdash&rft.au=Yan%20Zhou&rft.au=Murat%20Kantarcioglu","","❓ Multiple DOI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VG527U75","journalArticle","2017","Zhang, Quanshi; Wu, Ying Nian; Zhu, Song-Chun","Interpretable Convolutional Neural Networks","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","https://arxiv.org/abs/1710.00935v4","This paper proposes a method to modify traditional convolutional neural networks (CNNs) into interpretable CNNs, in order to clarify knowledge representations in high conv-layers of CNNs. In an interpretable CNN, each filter in a high conv-layer represents a certain object part. We do not need any annotations of object parts or textures to supervise the learning process. Instead, the interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. Our method can be applied to different types of CNNs with different structures. The clear knowledge representation in an interpretable CNN can help people understand the logics inside a CNN, i.e., based on which patterns the CNN makes the decision. Experiments showed that filters in an interpretable CNN were more semantically meaningful than those in traditional CNNs.","2017-10-02","2021-09-27 07:23:22","2022-12-20 05:06:23","2021-09-27","8827-8836","","","","","","","","","","","","","","","","","","","","arXiv: 1710.00935 Publisher: IEEE Computer Society QID: Q102362919","","C:\Users\ambreen.hanif\Zotero\storage\UIBCHMVQ\Zhang et al_2017_Interpretable Convolutional Neural Networks.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X5HYWIKH","journalArticle","2019","Gunning, D; Stefik, M; Choi, J; Miller, T; Stumpf, S; Orcid ; Yang, G-Z","XAI-Explainable artificial intelligence","Science Robotics","","","10.1126/scirobotics.aay7120","http://openaccess.city.ac.uk/","This is the accepted version of the paper. This version of the publication may differ from the final published version. Permanent repository link: https://openaccess.city.ac.uk/id/eprint/23405/ Link to published version: http://dx. Explanations are essential for users to effectively understand, trust, and manage powerful artificial intelligence applications.","2019","2021-09-20 01:11:08","2022-12-20 05:06:22","2021-09-20","7120","","37","4","","","","","","","","","","","","","","","","","345 citations (Crossref) [2022-12-20] QID: Q101156928","","C:\Users\ambreen.hanif\Zotero\storage\Z3HP5UHA\Gunning et al_2019_XAI-Explainable artificial intelligence.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IRSTJJ8Z","journalArticle","","Bodria, Francesco; Panisson, André; Perotti, Alan; Piaggesi, Simone","Explainability Methods for Natural Language Processing: Applications to Sentiment Analysis (Discussion Paper)","","","","","","Sentiment analysis is the process of classifying natural language sentences as expressing positive or negative sentiments, and it is a crucial task where the explanation of a prediction might arguably be as necessary as the prediction itself. We analysed different explanation techniques, and we applied them to the classification task of Sentiment Analysis. We explored how attention-based techniques can be exploited to extract meaningful sentiment scores with a lower computational cost than existing XAI methods.","","2021-09-28 05:51:16","2022-12-20 05:06:21","2021-09-28","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\PTA46CPH\Bodria et al_Explainability Methods for Natural Language Processing.pdf","","","⛔ No DOI found; eXplainable Artificial Intelligence; Natural Language Pro-cessing; Sentiment Analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M2GY5Q5Q","journalArticle","2021","Almutairi, Faisal M.; Sidiropoulos, Nicholas D.; Yang, Bo","XPL-CF: Explainable Embeddings for Feature-based Collaborative Filtering","International Conference on Information and Knowledge Management, Proceedings","","","10.1145/3459637.3482221","","Collaborative filtering (CF) methods are making an impact on our daily lives in a wide range of applications, including recommender systems and personalization. Latent factor methods, e.g., matrix factorization (MF), have been the state-of-the-art in CF, however they lack interpretability and do not provide a straightforward explanation for their predictions. Explainability is gaining momentum in recommender systems for accountability, and because a good explanation can swing an undecided user. Most recent explainable recommendation methods require auxiliary data such as review text or item content on top of item ratings. In this paper, we address the case where no additional data are available and propose augmenting the classical MF framework for CF with a prior that encodes each user's embedding as a sparse linear combination of item embeddings, and vice versa for each item embedding. Our XPL-CF approach automatically reveals these user-item relationships, which underpin the latent factors and explain how the resulting recommendations are formed. We showcase the effectiveness of XPL-CF on real data from various application domains. We also evaluate the explainability of the user-item relationship obtained from XPL-CF through numeric evaluation and case study examples.","2021-10-26","2022-05-17 04:26:42","2022-12-20 05:06:20","2022-05-17","2847-2851","","","","","","","","","","","","","","","","","","","","0 citations (Crossref) [2022-12-20] Publisher: Association for Computing Machinery ISBN: 9781450384469","","","","","collaborative filtering; explainable recommendation; matrix factorization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EXMWHUMJ","journalArticle","2021","Rojat, Thomas; Puget, Raphaël; Filliat, David; Ser, Javier Del; Gelin, Rodolphe; Díaz-Rodríguez, Natalia","Explainable Artificial Intelligence (XAI) on Time Series Data: A Survey","arXiv preprint arXiv:2104.00950","","","","","Most of state of the art methods applied on time series consist of deep learning methods that are too complex to be interpreted. This lack of interpretability is a major drawback, as several applications in the real world are critical tasks, such as the medical field or the autonomous driving field. The explainability of models applied on time series has not gather much attention compared to the computer vision or the natural language processing fields. In this paper, we present an overview of existing explainable AI (XAI) methods applied on time series and illustrate the type of explanations they produce. We also provide a reflection on the impact of these explanation methods to provide confidence and trust in the AI systems.","2021","2021-09-28 05:32:29","2022-12-20 05:06:20","2021-09-28","","","","","","","","","","","","","","","","","","","","","arXiv: 2104.00950v1","","C:\Users\ambreen.hanif\Zotero\storage\E5YZN23F\Rojat et al_2021_Explainable Artificial Intelligence (XAI) on Time Series Data.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ATRQKABE","journalArticle","1995","Craven, Mark W; Shavlik, Jude W","Extracting Thee-Structured Representations of Thained Networks","Neural Information Processing Systems","","","","","A significant limitation of neural networks is that the representations they learn are usually incomprehensible to humans. We present a novel algorithm , TREPAN, for extracting comprehensible, symbolic representations from trained neural networks. Our algorithm uses queries to induce a decision tree that approximates the concept represented by a given network. Our experiments demonstrate that TREPAN is able to produce decision trees that maintain a high level of fidelity to their respective networks while being com-prehensible and accurate. Unlike previous work in this area, our algorithm is general in its applicability and scales well to large networks and problems with high-dimensional input spaces.","1995","2021-07-28 23:47:37","2022-12-20 05:06:20","2021-07-29","24-30","","","8","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\RHN64TAK\Craven_Shavlik_1995_Extracting Thee-Structured Representations of Thained Networks.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8BPLCL8R","journalArticle","2016","Kim, Been; Khanna, Rajiv; Koyejo, Oluwasanmi","Examples are not Enough, Learn to Criticize! Criticism for Interpretability","Advances in neural information processing systems","","","","","Example-based explanations are widely used in the effort to improve the inter-pretability of highly complex distributions. However, prototypes alone are rarely sufficient to represent the gist of the complexity. In order for users to construct better mental models and understand complex data distributions, we also need criticism to explain what are not captured by prototypes. Motivated by the Bayesian model criticism framework, we develop MMD-critic which efficiently learns prototypes and criticism, designed to aid human interpretability. A human subject pilot study shows that the MMD-critic selects prototypes and criticism that are useful to facilitate human understanding and reasoning. We also evaluate the prototypes selected by MMD-critic via a nearest prototype classifier, showing competitive performance compared to baselines.","2016","2021-09-28 00:33:14","2022-12-20 05:06:19","2021-09-28","","","","29","","","","","","","","","","","","","","","","","QID: Q46993888","","C:\Users\ambreen.hanif\Zotero\storage\3ULPHGIZ\Kim et al_2016_Examples are not Enough, Learn to Criticize.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T7AT22I7","journalArticle","2015","Goodfellow, Ian J.; Shlens, Jonathon; Szegedy, Christian","Explaining and harnessing adversarial examples","3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings","","","","","Several machine learning models, including neural networks, consistently misclassify adversarial examples—inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks’ vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.","2015","2022-06-21 01:12:26","2022-12-20 05:06:18","2022-06-21","","","","","","","","","","","","","","","","","","","","","arXiv: 1412.6572 Publisher: International Conference on Learning Representations, ICLR QID: Q45318729","","","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RLTBBJF7","journalArticle","","Vaswani, Ashish; Brain, Google; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia","Attention Is All You Need","","","","","","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","","2022-05-17 04:49:54","2022-12-20 05:06:17","2022-05-17","","","","","","","","","","","","","","","","","","","","","arXiv: 1706.03762v5 QID: Q30249683","","C:\Users\ambreen.hanif\Zotero\storage\LDH5JDHE\Vaswani et al_Attention Is All You Need.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"86KNJMK4","journalArticle","2019","Fisher, Aaron; Rudin, Cynthia; Dominici, Francesca","All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously","Journal of Machine Learning Research","","","","http://jmlr.org/papers/v20/18-760.html.","Variable importance (VI) tools describe how much covariates contribute to a prediction model's accuracy. However, important variables for one well-performing model (for example , a linear model f (x) = x T β with a fixed coefficient vector β) may be unimportant for another model. In this paper, we propose model class reliance (MCR) as the range of VI values across all well-performing model in a prespecified class. Thus, MCR gives a more comprehensive description of importance by accounting for the fact that many prediction models, possibly of different parametric forms, may fit the data well. In the process of deriving MCR, we show several informative results for permutation-based VI estimates, based on the VI measures used in Random Forests. Specifically, we derive connections between permutation importance estimates for a single prediction model, U-statistics, conditional variable importance, conditional causal effects, and linear model coefficients. We then give probabilistic bounds for MCR, using a novel, generalizable technique. We apply MCR to a public data set of Broward County criminal records to study the reliance of recidivism prediction models on sex and race. In this application, MCR can be used to help inform VI for unknown, proprietary models.","2019","2021-09-27 07:12:07","2022-12-20 05:06:17","2021-09-27","1-81","","","20","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\Y5T58G75\full-text.pdf","","","⛔ No DOI found; conditional variable importance; interpretable models; permutation importance; Rashomon; transparency; U-statistics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PMGR9QYD","journalArticle","2021","Selten, F.J.","Can Explainable AI Mitigate Decision-Making Errors Induced by Algorithms in Street-Level Police Work? An Experiment.","","","","","http://dspace.library.uu.nl/handle/1874/404351","Universiteit Utrecht","2021","2021-09-28 05:54:14","2022-12-20 05:06:16","2021-09-28","","","","","","","","","","","","","","","","","","","","","","","","http://localhost/handle/1874/404351","","⛔ No DOI found; algorithms; automation bias; confirmation bias; Explainable Artificial Intelligence; Public Administration and Organisational Science; street-level bureaucracy","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PGZ2C2PJ","journalArticle","2017","Frosst, Nicholas; Hinton, Geoffrey","Distilling a Neural Network Into a Soft Decision Tree","arXiv preprint arXiv:1711.09784","","","","","Deep neural networks have proved to be a very effective way to perform classification tasks. They excel when the input data is high dimensional, the relationship between the input and the output is complicated, and the number of labeled training examples is large [Szegedy et al., 2015, Wu et al., 2016, Jozefowicz et al., 2016, Graves et al., 2013]. But it is hard to explain why a learned network makes a particular classification decision on a particular test case. This is due to their reliance on distributed hierarchical representations. If we could take the knowledge acquired by the neural net and express the same knowledge in a model that relies on hierarchical decisions instead, explaining a particular decision would be much easier. We describe a way of using a trained neural net to create a type of soft decision tree that generalizes better than one learned directly from the training data.","2017","2021-09-27 03:08:43","2022-12-20 05:06:16","2021-09-27","","","","","","","","","","","","","","","","","","","","","arXiv: 1711.09784v1","","C:\Users\ambreen.hanif\Zotero\storage\5LW3B3QV\Frosst_Hinton_2017_Distilling a Neural Network Into a Soft Decision Tree.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9D5E668X","journalArticle","2019","Weerts, Hilde J. P.; van Ipenburg, Werner; Pechenizkiy, Mykola","A Human-Grounded Evaluation of SHAP for Alert Processing","","","","","https://arxiv.org/abs/1907.03324v1","In the past years, many new explanation methods have been proposed to achieve interpretability of machine learning predictions. However, the utility of these methods in practical applications has not been researched extensively. In this paper we present the results of a human-grounded evaluation of SHAP, an explanation method that has been well-received in the XAI and related communities. In particular, we study whether this local model-agnostic explanation method can be useful for real human domain experts to assess the correctness of positive predictions, i.e. alerts generated by a classifier. We performed experimentation with three different groups of participants (159 in total), who had basic knowledge of explainable machine learning. We performed a qualitative analysis of recorded reflections of experiment participants performing alert processing with and without SHAP information. The results suggest that the SHAP explanations do impact the decision-making process, although the model's confidence score remains to be a leading source of evidence. We statistically test whether there is a significant difference in task utility metrics between tasks for which an explanation was available and tasks in which it was not provided. As opposed to common intuitions, we did not find a significant difference in alert processing performance when a SHAP explanation is available compared to when it is not.","2019-07-07","2021-11-25 02:34:06","2022-12-20 05:06:15","2021-11-25","","","","","","","","","","","","","","","","","","","","","arXiv: 1907.03324","","C:\Users\ambreen.hanif\Zotero\storage\MSBN5DIQ\Weerts et al_2019_A Human-Grounded Evaluation of SHAP for Alert Processing.pdf","","","⛔ No DOI found; explainable predictive analytics; human-computer interaction; human-grounded evaluation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MIQ7WMB5","journalArticle","2019","Chen, Jindong; Hu, Yizhou; Liu, Jingping; Xiao, Yanghua; Jiang, Haiyun","Deep Short Text Classification with Knowledge Powered Attention","33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019","","","","http://arxiv.org/abs/1902.08050","Short text classification is one of important tasks in Natural Language Processing (NLP). Unlike paragraphs or documents, short texts are more ambiguous since they have not enough contextual information, which poses a great challenge for classification. In this paper, we retrieve knowledge from external knowledge source to enhance the semantic representation of short texts. We take conceptual information as a kind of knowledge and incorporate it into deep neural networks. For the purpose of measuring the importance of knowledge, we introduce attention mechanisms and propose deep Short Text Classification with Knowledge powered Attention (STCKA). We utilize Concept towards Short Text (C- ST) attention and Concept towards Concept Set (C-CS) attention to acquire the weight of concepts from two aspects. And we classify a short text with the help of conceptual information. Unlike traditional approaches, our model acts like a human being who has intrinsic ability to make decisions based on observation (i.e., training data for machines) and pays more attention to important knowledge. We also conduct extensive experiments on four public datasets for different tasks. The experimental results and case studies show that our model outperforms the state-of-the-art methods, justifying the effectiveness of knowledge powered attention.","2019-02-21","2021-06-08 20:47:39","2022-12-20 05:06:15","2021-06-09","6252-6259","","","","","","","","","","","","","","","","","","","","arXiv: 1902.08050 Publisher: AAAI Press","","C:\Users\ambreen.hanif\Zotero\storage\CXLXLNMT\Chen et al_2019_Deep Short Text Classification with Knowledge Powered Attention.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JMYTJ2K4","journalArticle","2021","Hardt, Michaela; Chen, Xiaoguang; Cheng, Xiaoyi; Donini, Michele; Gelman, Jason; Gollaprolu, Satish; He, John; Larroy, Pedro; Liu, Xinyu; Mccarthy, Nick; Rathi, Ashish; Rees, Scott; Siva, Ankit; Tsai, Erhyuan; Vasist, Keerthan; Yilmaz, Pinar; Bilal Zafar, Muhammad; Das, Sanjiv; Haas, Kevin; Hill, Tyler; Amazon, Krishnaram Kenthapadi; Services, Web","Amazon SageMaker Clarify: Machine Learning Bias Detection and Explainability in the Cloud; Amazon SageMaker Clarify: Machine Learning Bias Detection and Explainability in the Cloud","","","","","https://github.com/algofairness/fairness-comparison","Understanding the predictions made by machine learning (ML) models and their potential biases remains a challenging and labor-intensive task that depends on the application, the dataset, and the specific model. We present Amazon SageMaker Clarify, an explain-ability feature for Amazon SageMaker that launched in December 2020, providing insights into data and ML models by identifying biases and explaining predictions. It is deeply integrated into Amazon SageMaker, a fully managed service that enables data scientists and developers to build, train, and deploy ML models at any scale. Clarify supports bias detection and feature importance computation across the ML lifecycle, during data preparation, model evaluation, and post-deployment monitoring. We outline the desiderata derived from customer input, the modular architecture, and the methodology for bias and explanation computations. Further, we describe the technical challenges encountered and the tradeoffs we had to make. For illustration, we discuss two customer use cases. We present our deployment results including qualitative customer feedback and a quantitative evaluation. Finally, we summarize lessons learned, and discuss best practices for the successful adoption of fairness and explanation tools in practice.","2021","2022-06-21 01:30:35","2022-12-20 05:06:14","2022-06-21","","","","","","","","","","","","","","","","","","","","","arXiv: 2109.03285v1","","C:\Users\ambreen.hanif\Zotero\storage\PKYAF65Q\Hardt et al_2021_Amazon SageMaker Clarify.pdf","","","⛔ No DOI found; Machine learning; Explainability; Fairness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZD955TQU","journalArticle","2018","Lundberg, Scott M.; Erion, Gabriel G.; Lee, Su-In","Consistent Individualized Feature Attribution for Tree Ensembles","","","","","https://arxiv.org/abs/1802.03888v3","Interpreting predictions from tree ensemble methods such as gradient boosting machines and random forests is important, yet feature attribution for trees is often heuristic and not individualized for each prediction. Here we show that popular feature attribution methods are inconsistent, meaning they can lower a feature's assigned importance when the true impact of that feature actually increases. This is a fundamental problem that casts doubt on any comparison between features. To address it we turn to recent applications of game theory and develop fast exact tree solutions for SHAP (SHapley Additive exPlanation) values, which are the unique consistent and locally accurate attribution values. We then extend SHAP values to interaction effects and define SHAP interaction values. We propose a rich visualization of individualized feature attributions that improves over classic attribution summaries and partial dependence plots, and a unique ""supervised"" clustering (clustering based on feature attributions). We demonstrate better agreement with human intuition through a user study, exponential improvements in run time, improved clustering performance, and better identification of influential features. An implementation of our algorithm has also been merged into XGBoost and LightGBM, see http://github.com/slundberg/shap for details.","2018-02-12","2022-01-16 14:11:28","2022-12-20 05:06:12","2022-01-17","","","","","","","","","","","","","","","","","","","","","arXiv: 1802.03888","","C:\Users\ambreen.hanif\Zotero\storage\5HK3P2KC\Lundberg et al_2018_Consistent Individualized Feature Attribution for Tree Ensembles.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B6CDQ6CA","journalArticle","2020","Schlegel, Udo; Oelke, Daniela; Keim, Daniel A; El-Assady, Mennatallah","An Empirical Study of Explainable AI Techniques on Deep Learning Models For Time Series Tasks","arXiv preprint arXiv:2012.04344","","","","","Decision explanations of machine learning black-box models are often generated by applying Explainable AI (XAI) techniques. However, many proposed XAI methods produce unverified outputs. Evaluation and verification are usually achieved with a visual interpretation by humans on individual images or text. In this preregistration, we propose an empirical study and benchmark framework to apply attribution methods for neural networks developed for images and text data on time series. We present a methodology to automatically evaluate and rank at-tribution techniques on time series using perturbation methods to identify reliable approaches.","2020","2021-10-26 23:13:43","2022-12-20 05:06:12","2021-10-27","","","","","","","","","","","","","","","","","","","","","arXiv: 2012.04344v1","","C:\Users\ambreen.hanif\Zotero\storage\3PCJZAKM\full-text.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GJTGGI33","journalArticle","2017","Wachter, Sandra; Mittelstadt, Brent; Russell, Chris","Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR","Harvard Journal of Law & Technology (Harvard JOLT)","","","","https://heinonline.org/HOL/Page?handle=hein.journals/hjlt31&id=859&div=&collection=","","2017","2021-09-28 00:42:00","2022-12-20 05:06:12","2021-09-28","841","","","31","","","","","","","","","","","","","","","","","","","","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ASA3MVXE","journalArticle","2019","Mueller, Shane T.; Hoffman, Robert R.; Clancey, William; Emrey, Abigail; Klein, Gary","Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of Key Ideas and Publications, and Bibliography for Explainable AI","arXiv preprint arXiv:1902.01876","","","","https://arxiv.org/abs/1902.01876v1","This is an integrative review that address the question, ""What makes for a good explanation?"" with reference to AI systems. Pertinent literatures are vast. Thus, this review is necessarily selective. That said, most of the key concepts and issues are expressed in this Report. The Report encapsulates the history of computer science efforts to create systems that explain and instruct (intelligent tutoring systems and expert systems). The Report expresses the explainability issues and challenges in modern AI, and presents capsule views of the leading psychological theories of explanation. Certain articles stand out by virtue of their particular relevance to XAI, and their methods, results, and key points are highlighted. It is recommended that AI/XAI researchers be encouraged to include in their research reports fuller details on their empirical or experimental methods, in the fashion of experimental psychology research reports: details on Participants, Instructions, Procedures, Tasks, Dependent Variables (operational definitions of the measures and metrics), Independent Variables (conditions), and Control Conditions.","2019-02-05","2021-09-28 03:53:30","2022-12-20 05:06:11","2021-09-28","","","","","","","","","","","","","","","","","","","","","arXiv: 1902.01876 Citation Key: Mueller2019","","C:\Users\ambreen.hanif\Zotero\storage\AXTIZYBU\Mueller et al_2019_Explanation in Human-AI Systems.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZCXRC88X","journalArticle","2021","Islam, Sheikh Rabiul; Eberle, William; Ghafoor, Sheikh Khaled; Ahmed, Mohiuddin","Explainable Artificial Intelligence Approaches: A Survey","arXiv preprint arXiv:2101.09429","","","","http://arxiv.org/abs/2101.09429","The lack of explainability of a decision from an Artificial Intelligence (AI) based ""black box"" system/model, despite its superiority in many real-world applications, is a key stumbling block for adopting AI in many high stakes applications of different domain or industry. While many popular Explainable Artificial Intelligence (XAI) methods or approaches are available to facilitate a human-friendly explanation of the decision, each has its own merits and demerits, with a plethora of open challenges. We demonstrate popular XAI methods with a mutual case study/task (i.e., credit default prediction), analyze for competitive advantages from multiple perspectives (e.g., local, global), provide meaningful insight on quantifying explainability, and recommend paths towards responsible or human-centered AI using XAI as a medium. Practitioners can use this work as a catalog to understand, compare, and correlate competitive advantages of popular XAI methods. In addition, this survey elicits future research directions towards responsible or human-centric AI systems, which is crucial to adopt AI in high stakes applications.","2021-01-23","2021-06-27 23:18:40","2022-12-20 05:06:11","2021-06-28","","","","","","","","","","","","","","","","","","","","","arXiv: 2101.09429","","C:\Users\ambreen.hanif\Zotero\storage\2JPWNCLQ\Islam et al_2021_Explainable Artificial Intelligence Approaches.pdf","","","⛔ No DOI found; Explainability Quantification; Human-centered Artificial Intelligence; Index Terms-Explainable Artificial Intelligence; Interpretability !","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SSMQBI4W","journalArticle","2018","Dube, Simant","High Dimensional Spaces, Deep Learning and Adversarial Examples","","","","","http://arxiv.org/abs/1801.00634","In this paper, we analyze deep learning from a mathematical point of view and derive several novel results. The results are based on intriguing mathematical properties of high dimensional spaces. We first look at perturbation based adversarial examples and show how they can be understood using topological and geometrical arguments in high dimensions. We point out mistake in an argument presented in prior published literature, and we present a more rigorous, general and correct mathematical result to explain adversarial examples in terms of topology of image manifolds. Second, we look at optimization landscapes of deep neural networks and examine the number of saddle points relative to that of local minima. Third, we show how multiresolution nature of images explains perturbation based adversarial examples in form of a stronger result. Our results state that expectation of $L_2$-norm of adversarial perturbations is $O\left(\frac{1}{\sqrt{n}}\right)$ and therefore shrinks to 0 as image resolution $n$ becomes arbitrarily large. Finally, by incorporating the parts-whole manifold learning hypothesis for natural images, we investigate the working of deep neural networks and root causes of adversarial examples and discuss how future improvements can be made and how adversarial examples can be eliminated.","2018-01-02","2022-06-21 01:12:26","2022-12-20 05:06:10","2022-06-21","","","","","","","","","","","","","","","","","","","","","arXiv: 1801.00634","","","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R8MTTEHK","journalArticle","2010","Baehrens, David; Harmeling, Stefan; Kawanabe, Motoaki; Hansen KHANSEN, Katja; Edward Rasmussen, Carl","How to Explain Individual Classification Decisions","Journal of Machine Learning Research","","","","","After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method.","2010","2021-09-28 00:05:23","2022-12-20 05:06:10","2021-09-28","1803-1831","","","11","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\ES7AVW5U\Baehrens et al_2010_How to Explain Individual Classification Decisions.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2Y24UVML","journalArticle","2002","Brabazon, Anthony; O'Neill, Michael; Matthews, Robin; Ryan, Conor","Grammatical Evolution And Corporate Failure Prediction","GECCO 2002: Proceedings of the Genetic and Evolutionary Computation Conference","","","","","This study examines the potential of Grammatical Evolution to uncover a series of useful rules which can assist in predicting corporate failure using information drawn from financial statements. A sample of 178 publically quoted, failed and non-failed US firms, drawn from …","2002","2021-02-11 00:00:00","2022-12-20 05:06:10","","1011-1018","","","","","","","","","","","","","","","","","","","","ISBN: 1-55860-878-8","","C:\Users\ambreen.hanif\Zotero\storage\ZEWUZG7J\Brabazon et al_2002_Grammatical Evolution And Corporate Failure Prediction.pdf","","","⛔ No DOI found; corporate failure prediction; genetic programming; genotype to phenotype mapping; grammars; grammatical evolution; real world applications","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BNEY32UA","journalArticle","2021","Bensaid, Eden; Martino, Mauro; Hoover, Ben; Strobelt, Hendrik","FAIRYTAILOR: A MULTIMODAL GENERATIVE FRAMEWORK FOR STORYTELLING","","","","","https://fairytailor.org","Storytelling is an open-ended task that entails creative thinking and requires a constant flow of ideas. Natural language generation (NLG) for storytelling is especially challenging because it requires the generated text to follow an overall theme while remaining creative and diverse to engage the reader. In this work, we introduce a system and a web-based demo, FairyTailor 1 , for human-in-the-loop visual story co-creation. Users can create a cohesive children's fairytale by weaving generated texts and retrieved images with their input. FairyTailor adds another modality and modifies the text generation process to produce a coherent and creative sequence of text and images. To our knowledge, this is the first dynamic tool for multimodal story generation that allows interactive co-formation of both texts and images. It allows users to give feedback on co-created stories and share their results. We release the demo source code 2 for other researchers' use.","2021","2022-07-04 23:42:45","2022-12-20 05:06:09","2022-07-05","","","","","","","","","","","","","","","","","","","","","arXiv: 2108.04324v1 ISBN: 2108.04324v1","","C:\Users\ambreen.hanif\Zotero\storage\MPFW7FPP\Bensaid et al_2021_FAIRYTAILOR.pdf","","","⛔ No DOI found; Language; Automated; Generation ·; Human-in-the-loop; Multimodality ·; Natural; Story","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V7RG7PPW","journalArticle","2018","Narayanan, Menaka; Chen, Emily; He, Jeffrey; Kim, Been; Gershman, Sam; Doshi-Velez, Finale","How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation","","","","","http://arxiv.org/abs/1802.00682","Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable in the specific context of verification. Suppose we have a machine learning system that predicts X, and we provide rationale for this prediction X. Given an input, an explanation, and an output, is the output consistent with the input and the supposed rationale? Via a series of user-studies, we identify what kinds of increases in complexity have the greatest effect on the time it takes for humans to verify the rationale, and which seem relatively insensitive.","2018-02-02","2021-09-30 00:20:19","2022-12-20 05:06:09","2021-09-30","","","","","","","","","","","","","","","","","","","","","arXiv: 1802.00682","","","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WKVGTP9I","conferencePaper","2017","Fong, Ruth C.; Vedaldi, Andrea","Interpretable Explanations of Black Boxes by Meaningful Perturbation","Proceedings of the IEEE international conference on computer vision","","","","","As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks ""look"" in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.","2017","2021-09-28 03:16:06","2022-12-20 05:06:09","2021-09-28","3429-3437","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\TXGVZEVZ\Fong_Vedaldi_2017_Interpretable Explanations of Black Boxes by Meaningful Perturbation.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C34QFWHI","conferencePaper","2018","Zhou, Bolei; Sun, Yiyou; Bau, David; Torralba, Antonio","Interpretable Basis Decomposition for Visual Explanation","Proceedings of the European Conference on Computer Vision (ECCV)","","","","https://github.com/CSAILVision/IBD","Explanations of the decisions made by a deep neural network are important for human end-users to be able to understand and diagnose the trustworthiness of the system. Current neural networks used for visual recognition are generally used as black boxes that do not provide any human interpretable justification for a prediction. In this work we propose a new framework called Interpretable Basis Decomposition for providing visual explanations for classification networks. By decomposing the neural activations of the input image into semantically interpretable components pre-trained from a large concept corpus, the proposed framework is able to disentangle the evidence encoded in the activation feature vector, and quantify the contribution of each piece of evidence to the final prediction. We apply our framework for providing explanations to several popular networks for visual recognition, and show it is able to explain the predictions given by the networks in a human-interpretable way. The human interpretability of the visual explanations provided by our framework and other recent explanation methods is evaluated through Amazon Mechanical Turk, showing that our framework generates more faithful and interpretable explanations 1 .","2018","2021-09-28 02:19:15","2022-12-20 05:06:09","2021-09-28","119-134","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\IT9M5PPT\Zhou et al_2018_Interpretable Basis Decomposition for Visual Explanation.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MJHW7KTB","journalArticle","2018","Guidotti, Riccardo; Monreale, Anna; Ruggieri, Salvatore; Pedreschi, Dino; Turini, Franco; Giannotti, Fosca","Local Rule-Based Explanations of Black Box Decision Systems","arXiv preprint arXiv:1805.10820","","","","https://arxiv.org/abs/1805.10820v1","The recent years have witnessed the rise of accurate but obscure decision systems which hide the logic of their internal decision processes to the users. The lack of explanations for the decisions of black box systems is a key ethical issue, and a limitation to the adoption of machine learning components in socially sensitive and safety-critical contexts. %Therefore, we need explanations that reveals the reasons why a predictor takes a certain decision. In this paper we focus on the problem of black box outcome explanation, i.e., explaining the reasons of the decision taken on a specific instance. We propose LORE, an agnostic method able to provide interpretable and faithful explanations. LORE first leans a local interpretable predictor on a synthetic neighborhood generated by a genetic algorithm. Then it derives from the logic of the local interpretable predictor a meaningful explanation consisting of: a decision rule, which explains the reasons of the decision; and a set of counterfactual rules, suggesting the changes in the instance's features that lead to a different outcome. Wide experiments show that LORE outperforms existing methods and baselines both in the quality of explanations and in the accuracy in mimicking the black box.","2018-05-28","2021-09-27 23:39:58","2022-12-20 05:06:09","2021-09-28","","","","","","","","","","","","","","","","","","","","","arXiv: 1805.10820","","C:\Users\ambreen.hanif\Zotero\storage\XBCG2TNV\full-text.pdf","","","⛔ No DOI found; Data mining; CCS CONCEPTS • Information systems → Decision support systems; Data analytics; KEYWORDS Explanation, Decision Systems, Rules","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C5CZDNF2","journalArticle","","Javed, Noman; Ali, Taher","Genetically evolved Neural Networks for forecasting the Stock Prices","","","","","","","","2021-01-29 00:57:22","2022-12-20 05:06:09","","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\BCBNXH35\genetically-evolved-neural-11.pdf","","","⛔ No DOI found; evolutionary computing; neural network; stock market","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N2Q97ACW","conferencePaper","2015","Noh, Hyeonwoo Hong; Seunghoon Han, Bohyung","Learning Deconvolution Network for Semantic Segmentation","Proceedings of the IEEE International Conference on Computer Vision","","","","","","2015","2021-10-28 00:51:14","2022-12-20 05:06:08","","1520-1528","","","","","","","","","","","","","","","","","","","","","","","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QT9P5ZMT","journalArticle","2017","Shrikumar, Avanti; Greenside, Peyton; Kundaje, Anshul","Learning Important Features Through Propagating Activation Differences","34th International Conference on Machine Learning, ICML 2017","","","","https://arxiv.org/abs/1704.02685v2","The purported ""black box"" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides: bit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code: http://goo.gl/RM8jvH.","2017-04-10","2021-09-28 06:07:27","2022-12-20 05:06:08","2021-09-28","4844-4866","","","7","","","","","","","","","","","","","","","","","arXiv: 1704.02685 Publisher: International Machine Learning Society (IMLS)","","","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8CN9Y8HR","conferencePaper","2017","Labaf, Maryam; Hitzler, Pascal; Evans, Anthony B","Propositional Rule Extraction from Neural Networks under Background Knowledge","NeSy","","","","","It is well-known that the input-output behaviour of a neural network can be recast in terms of a set of propositional rules, and under certain weak preconditions this is also always possible with positive (or definite) rules. Furthermore, in this case there is in fact a unique minimal (technically, reduced) set of such rules which perfectly captures the input-output mapping. In this paper, we investigate to what extent these results and corresponding rule extraction algorithms can be lifted to take additional background knowledge into account. It turns out that uniqueness of the solution can then no longer be guaranteed. However, the background knowledge often makes it possible to extract simpler, and thus more easily understandable , rulesets which still perfectly capture the input-output mapping.","2017","2021-09-28 02:23:15","2022-12-20 05:06:08","2021-09-28","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\UVU7K3VU\Labaf et al_2017_Propositional Rule Extraction from Neural Networks under Background Knowledge.pdf; C:\Users\ambreen.hanif\Zotero\storage\QUISHAMN\full-text.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RUYVVGZA","journalArticle","2014","Lin, Tsung-Yi; Maire, Michael; Belongie, Serge; Bourdev, Lubomir; Girshick, Ross; Hays, James; Perona, Pietro; Ramanan, Deva; Zitnick, C. Lawrence; Dollár, Piotr","Microsoft COCO: Common Objects in Context","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","","","","https://arxiv.org/abs/1405.0312v3","We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.","2014-05-01","2021-08-26 01:14:20","2022-12-20 05:06:08","2021-08-26","740-755","","PART 5","8693 LNCS","","","","","","","","","","","","","","","","","arXiv: 1405.0312 Publisher: Springer Verlag QID: Q30337048","","C:\Users\ambreen.hanif\Zotero\storage\NIQPI3YL\Lin et al_2014_Microsoft COCO.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LPUWWSRI","journalArticle","2018","Barker, Jocelyn; Bansal, Gagan; Gajewar, Amita; Golyaev, Konstantin; Conners, Matt","Secure and automated enterprise revenue forecasting","32nd AAAI Conference on Artificial Intelligence, AAAI 2018","","","","","Revenue forecasting is required by most enterprises for strategic business planning and for providing expected future results to investors. However, revenue forecasting processes in most companies are time-consuming and error-prone as they are performed manually by hundreds of financial analysts. In this paper, we present a novel machine learning based revenue forecasting solution that we developed to forecast 100% of Microsoft's revenue (around $85 Billion in 2016), and is now deployed into production as an end-to-end automated and secure pipeline in Azure. Our solution combines historical trend and seasonal patterns with additional information, e.g., sales pipeline data, within a unified modeling framework. In this paper, we describe our framework including the features, method for hyperparameters tuning of ML models using time series cross-validation, and generation of prediction intervals. We also describe how we architected an end-to-end secure and automated revenue forecasting solution on Azure using Cortana Intelligence Suite. over consecutive quarters, our machine learning models have continuously produced forecasts with an average accuracy of 98-99 percent for various divisions within Microsoft's Finance organization. As a result, our models have been widely adopted by them and are now an integral part of Microsoft's most important forecasting processes, from providing Wall Street guidance to managing global sales performance.","2018","2021-02-10 03:05:12","2022-12-20 05:06:08","","7657-7664","","","","","","","","","","","","","","","","","","","","ISBN: 9781577358008","","C:\Users\ambreen.hanif\Zotero\storage\CHVSRGSW\Barker et al_2018_Secure and automated enterprise revenue forecasting.pdf; C:\Users\ambreen.hanif\Zotero\storage\TP5EKGD7\11385-Article Text-14913-1-2-20201228.pdf","","","⛔ No DOI found; Deployed Papers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8UUA3N7H","conferencePaper","2016","Ribeiro, Marco Tulio; Singh, Sameer; Guestrin, Carlos","Model-Agnostic Interpretability of Machine Learning","ICML Workshop on Human Interpretability in Machine Learning","","","","","Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to inter-pretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison , and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.","2016","2022-01-06 04:33:03","2022-12-20 05:06:07","2022-01-06","","","","","","","","","","","","","","","","","","","","","arXiv: 1606.05386v1","","C:\Users\ambreen.hanif\Zotero\storage\WIIB32SG\Ribeiro et al_2016_Model-Agnostic Interpretability of Machine Learning.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PWF33V3Y","journalArticle","2017","Smilkov, Daniel; Thorat, Nikhil; Kim, Been; Viégas, Fernanda; Wattenberg, Martin","SmoothGrad: removing noise by adding noise","arXiv preprint arXiv:1706.03825","","","","https://arxiv.org/abs/1706.03825v1","Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.","2017-06-12","2021-10-28 00:12:27","2022-12-20 05:06:07","2021-10-28","","","","","","","","","","","","","","","","","","","","","arXiv: 1706.03825","","C:\Users\ambreen.hanif\Zotero\storage\P5BXJS8N\Smilkov et al_2017_SmoothGrad.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B9X77366","journalArticle","2017","Edwards, Lilian; Veale, Michael; Welbl, Johannes; Kleek, Max Van; Binns, Reuben; Lane, Giles; Henderson, Tristan","SLAVE TO THE ALGORITHM? WHY A 'RIGHT TO AN EXPLANATION' IS PROBABLY NOT THE REMEDY YOU ARE LOOKING FOR","Duke Law and  Technology Review","","","","https://perma.cc/PJX2-XT7X];","Algorithms, particularly machine learning (ML) algorithms, are increasingly important to individuals' lives, but have caused a range of concerns revolving mainly around unfairness, discrimination and opacity. Transparency in the form of a ""right to an explanation"" has emerged as a compellingly attractive remedy since it intuitively promises to open the algorithmic ""black box"" to promote challenge, redress, and hopefully heightened accountability. Amidst the general furore over algorithmic bias we describe, any remedy in a storm has looked attractive. However, we argue that a right to an explanation in the EU General Data Protection Regulation (GDPR) is unlikely to present a complete remedy to algorithmic harms, particularly in some of the core ""algorithmic war stories"" that have shaped recent attitudes in this domain. Firstly, the law is restrictive, unclear, or even paradoxical concerning when any explanation-related right can be triggered. Secondly, even navigating this, the legal conception of explanations as ""meaningful information about the logic of processing"" may not be provided by the kind of ML ""explanations"" computer scientists have developed, partially in response. ML explanations are restricted both by the type of † Professor of Internet Law, explanation sought, the dimensionality of the domain and the type of user seeking an explanation. However, ""subject-centric"" explanations (SCEs) focussing on particular regions of a model around a query show promise for interactive exploration, as do explanation systems based on learning a model from outside rather than taking it apart (pedagogical versus decompositional explanations) in dodging developers' worries of intellectual property or trade secrets disclosure. Based on our analysis, we fear that the search for a ""right to an explanation"" in the GDPR may be at best distracting, and at worst nurture a new kind of ""transparency fallacy."" But all is not lost. We argue that other parts of the GDPR related (i) to the right to erasure (""right to be forgotten"") and the right to data portability; and (ii) to privacy by design, Data Protection Impact Assessments and certification and privacy seals, may have the seeds we can use to make algorithms more responsible, explicable, and human-centered.","2017","2021-10-14 04:15:24","2022-12-20 05:06:07","2021-10-14","","","","16","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\JG8DWAXP\Edwards et al_2017_SLAVE TO THE ALGORITHM.pdf","","","⛔ No DOI found; International","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EGIHUY4E","journalArticle","","Aenugu, Sneha","Perturbation-based exploration methods in deep reinforcement learning","","","","","","Recent research on structured exploration placed emphasis on identifying novel states in the state space and incentivizing the agent to revisit them through intrinsic reward bonuses. In this study, we question whether the performance boost demonstrated through these methods is indeed due to the discovery of structure in exploratory schedule of the agent or is the benefit largely attributed to the perturbations in the policy and reward space manifested in pursuit of structured exploration. In this study we investigate the effect of perturbations in policy and reward spaces on the exploratory behavior of the agent. We proceed to show that simple acts of perturbing the policy just before the softmax layer and introduction of sporadic reward bonuses into the domain can greatly enhance exploration in several domains of the arcade learning environment. In light of these findings, we recommend bench-marking any enhancements to structured exploration research against the backdrop of noisy exploration.","","2021-09-28 05:33:26","2022-12-20 05:06:07","2021-09-28","","","","","","","","","","","","","","","","","","","","","arXiv: 2011.05446v1","","C:\Users\ambreen.hanif\Zotero\storage\U3Q27AYN\Aenugu_Perturbation-based exploration methods in deep reinforcement learning.pdf; C:\Users\ambreen.hanif\Zotero\storage\2C5QATCX\full-text.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5MIMTYDT","journalArticle","","Fotedar, Sonali; Vannisselroij, Koen; Khalil, Shama; Ploeg, Bas","Storytelling AI: A Generative Approach to Story Narration","","","","","https://github.com/hanzhanggit/StackGAN-Pytorch","In this paper, we demonstrate a Storytelling AI system , which is able to generate short stories and complementary illustrated images with minimal input from the user. The system makes use of a text generation model, a text-to-image synthesis network and a neural style transfer model. The final project is deployed as a web page where a user can build their stories.","","2022-07-04 23:45:00","2022-12-20 05:06:06","2022-07-05","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\ZVTH3E3D\Fotedar et al_Storytelling AI.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PV9YUNSU","journalArticle","2019","Schoenborn, Jakob M; Altho↵, Klaus-Dieter","Recent Trends in XAI: A Broad Overview on current Approaches, Methodologies and Interactions","","","","","","The definition of an explainable artificial intelligence heavily depends on the use-case, whether one is focusing on the technical knowledge-management component [30, 33, 37, 43] or rather the more social interaction including speech acts and conversations [27, 31, 33]. Since the uprising debate of the unknown outcome on the development of AI in general using Deep Learning [4, 34, 35, 44] and recent legal restrictions (for example the GDPR [19]), the need on developing an explainable AI is rapidly increasing, especially since the last two years. Additionally, the goal to increase the users trust towards AI has still to be achieved. Thus, this contribution aims to provide an overview on the current topics especially since 2018 with a focus on case-based explanations 3 up until today.","2019","2021-11-25 02:16:56","2022-12-20 05:06:06","2021-11-25","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\Z2BVISBD\Schoenborn_Altho↵_2019_Recent Trends in XAI.pdf","","","⛔ No DOI found; XAI; Case-Based Explanation; Explanation; Framework","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YKXRNDCL","journalArticle","2015","Härle, Philipp; Havas, Andras; Kremer, Andreas; Rona, Daniel; Samandari, Hamid","The future of bank risk management McKinsey Working Papers on Risk","Mckinsey Work. Pap. Risk","","","","","","2015","2021-10-25 23:03:35","2022-12-20 05:06:06","2021-10-26","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\B8VJ4ULN\Härle et al_2015_The future of bank risk management McKinsey Working Papers on Risk.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9MCESPXH","journalArticle","","Langley, Pat","Varieties of Explainable Agency","","","","","www.aaai.org","In this paper, I discuss some varieties of explanation that can arise in intelligent agents. I distinguish between process accounts, which address the detailed decisions made during heuristic search, and preference accounts, which clarify the ordering of alternatives independent of how they were generated. I also hypothesize which types of users will appreciate which types of explanation. In addition, I discuss three facets of multi-step decision making-conceptual inference, plan generation, and plan execution-in which explanations can arise. I also consider alternative ways to present questions to agents and for them provide their answers.","","2021-09-28 05:53:02","2022-12-20 05:06:05","2021-09-28","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\ASW3T9NQ\Langley_Varieties of Explainable Agency.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FMQHRS7G","journalArticle","2017","Doran, Derek; Schulz, Sarah; Besold, Tarek R.","What Does Explainable AI Really Mean? A New Conceptualization of Perspectives","CEUR Workshop Proceedings","","","","https://arxiv.org/abs/1710.00794v1","We characterize three notions of explainable AI that cut across research fields: opaque systems that offer no insight into its algo- rithmic mechanisms; interpretable systems where users can mathemat- ically analyze its algorithmic mechanisms; and comprehensible systems that emit symbols enabling user-driven explanations of how a conclusion is reached. The paper is motivated by a corpus analysis of NIPS, ACL, COGSCI, and ICCV/ECCV paper titles showing differences in how work on explainable AI is positioned in various fields. We close by introducing a fourth notion: truly explainable systems, where automated reasoning is central to output crafted explanations without requiring human post processing as final step of the generative process.","2017-10-02","2021-09-27 04:22:17","2022-12-20 05:06:05","2021-09-27","","","","2071","","","","","","","","","","","","","","","","","arXiv: 1710.00794 Publisher: CEUR-WS","","C:\Users\ambreen.hanif\Zotero\storage\9X92LXHD\Doran et al_2017_What Does Explainable AI Really Mean.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MEI5MUTQ","journalArticle","2018","Devlin, Jacob; Chang, Ming Wei; Lee, Kenton; Toutanova, Kristina","BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference","","","10.48550/arxiv.1810.04805","https://arxiv.org/abs/1810.04805v2","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","2018-10-11","2022-05-17 04:48:50","2022-12-20 04:41:50","2022-05-17","4171-4186","","","1","","","","","","","","","","","","","","","","","arXiv: 1810.04805 Publisher: Association for Computational Linguistics (ACL) ISBN: 9781950737130 QID: Q57267388","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F7ASHF8C","journalArticle","2019","Kelly, Christopher J.; Karthikesalingam, Alan; Suleyman, Mustafa; Corrado, Greg; King, Dominic","Key challenges for delivering clinical impact with artificial intelligence","BMC Medicine","","17417015","10.1186/S12916-019-1426-2/PEER-REVIEW","https://link.springer.com/articles/10.1186/s12916-019-1426-2","Background: Artificial intelligence (AI) research in healthcare is accelerating rapidly, with potential applications being demonstrated across various domains of medicine. However, there are currently limited examples of such techniques being successfully deployed into clinical practice. This article explores the main challenges and limitations of AI in healthcare, and considers the steps required to translate these potentially transformative technologies from research to clinical practice. Main body: Key challenges for the translation of AI systems in healthcare include those intrinsic to the science of machine learning, logistical difficulties in implementation, and consideration of the barriers to adoption as well as of the necessary sociocultural or pathway changes. Robust peer-reviewed clinical evaluation as part of randomised controlled trials should be viewed as the gold standard for evidence generation, but conducting these in practice may not always be appropriate or feasible. Performance metrics should aim to capture real clinical applicability and be understandable to intended users. Regulation that balances the pace of innovation with the potential for harm, alongside thoughtful post-market surveillance, is required to ensure that patients are not exposed to dangerous interventions nor deprived of access to beneficial innovations. Mechanisms to enable direct comparisons of AI systems must be developed, including the use of independent, local and representative test sets. Developers of AI algorithms must be vigilant to potential dangers, including dataset shift, accidental fitting of confounders, unintended discriminatory bias, the challenges of generalisation to new populations, and the unintended negative consequences of new algorithms on health outcomes. Conclusion: The safe and timely translation of AI research into clinically validated and appropriately regulated systems that can benefit everyone is challenging. Robust clinical evaluation, using metrics that are intuitive to clinicians and ideally go beyond measures of technical accuracy to include quality of care and patient outcomes, is essential. Further work is required (1) to identify themes of algorithmic bias and unfairness while developing mitigations to address these, (2) to reduce brittleness and improve generalisability, and (3) to develop methods for improved interpretability of machine learning predictions. If these goals can be achieved, the benefits for patients are likely to be transformational.","2019-10-29","2022-06-06 02:17:30","2022-12-20 04:41:48","2022-06-06","1-9","","1","17","","","","","","","","","","","","","","","","","PMID: 31665002 Publisher: BioMed Central Ltd. QID: Q91004786","","; C:\Users\ambreen.hanif\Zotero\storage\93S68JL9\Kelly et al_2019_Key challenges for delivering clinical impact with artificial intelligence.pdf","https://link.springer.com/article/10.1186/s12916-019-1426-2","","Machine learning; Artificial intelligence; Algorithms; Evaluation; Regulation; Translation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PG58W9LI","journalArticle","2020","Boukhelifa, Nadia; Bezerianos, Anastacia; Chang, Remco; Collins, Christopher; Drucker, Steven; Endert, Alex; Hullman, Jessica; North, Chris; Sedlmair, Michael","Challenges in Evaluating Interactive Visual Machine Learning Systems","IEEE Computer Graphics and Applications","","","10.1109/MCG.2020.3017064ï","https://hal.archives-ouvertes.fr/hal-03133986","","2020","2021-10-28 01:03:16","2022-12-20 04:41:38","2021-10-28","88-96","","6","40","","","","","","","","","","","","","","","","","Publisher: Institute of Electrical and Electronics Engineers QID: Q100953214","","C:\Users\ambreen.hanif\Zotero\storage\5EWPR9MD\Boukhelifa et al_2020_Challenges in Evaluating Interactive Visual Machine Learning Systems.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3PZ7E2Y5","document","2014","Santos, Cicero dos; Gatti, Maíra","Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts","","","","","https://www.aclweb.org/anthology/C14-1008","","2014","2021-06-05 23:16:11","2022-12-20 04:41:38","2021-06-06","","","","","","","","","","","","","","","","","","","","","Pages: 69-78 QID: Q41554085","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FWR9R2XK","journalArticle","2020","Bussmann, Niklas; Giudici, Paolo; Marinelli, Dimitri; Papenbrock, Jochen","Explainable AI in Fintech Risk Management","Frontiers in Artificial Intelligence","","26248212","10.3389/FRAI.2020.00026/BIBTEX","","The paper proposes an explainable AI model that can be used in fintech risk management and, in particular, in measuring the risks that arise when credit is borrowed employing peer to peer lending platforms. The model employs Shapley values, so that AI predictions are interpreted according to the underlying explanatory variables. The empirical analysis of 15,000 small and medium companies asking for peer to peer lending credit reveals that both risky and not risky borrowers can be grouped according to a set of similar financial characteristics, which can be employed to explain and understand their credit score and, therefore, to predict their future behavior.","2020-04-24","2021-11-24 23:44:42","2022-12-20 04:41:35","2021-11-25","26","","","3","","","","","","","","","","","","","","","","","Publisher: Frontiers Media SA QID: Q102633441","","C:\Users\ambreen.hanif\Zotero\storage\V7W4CCH9\Bussmann et al_2020_Explainable AI in Fintech Risk Management.pdf","","","Explainable AI; Credit risk management; Financial technologies; Shapley values; Similarity networks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GF4SUMLT","book","2021","Molnar, Christopher","Interpretable Machine Learning","","","","","https://christophm.github.io/interpretable-ml-book/","","2021","2021-08-05 23:39:35","2022-12-20 04:41:29","2021-08-06","","","","","","","","","","","","Lulu.com","","","","","","","","","QID: Q106987859","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G4P6FS43","journalArticle","2020","Holzinger, Andreas; Carrington, André; Müller, Heimo","Measuring the Quality of Explanations: The System Causability Scale (SCS): Comparing Human and Machine Explanations","KI - Kunstliche Intelligenz","","","10.1007/S13218-020-00636-Z/FIGURES/1","","Recent success in Artificial Intelligence (AI) and Machine Learning (ML) allow problem solving automatically without any human intervention. Autonomous approaches can be very convenient. However, in certain domains, e.g., in the medical domain, it is necessary to enable a domain expert to understand, why an algorithm came up with a certain result. Consequently, the field of Explainable AI (xAI) rapidly gained interest worldwide in various domains, particularly in medicine. Explainable AI studies transparency and traceability of opaque AI/ML and there are already a huge variety of methods. For example with layer-wise relevance propagation relevant parts of inputs to, and representations in, a neural network which caused a result, can be highlighted. This is a first important step to ensure that end users, e.g., medical professionals, assume responsibility for decision making with AI/ML and of interest to professionals and regulators. Interactive ML adds the component of human expertise to AI/ML processes by enabling them to re-enact and retrace AI/ML results, e.g. let them check it for plausibility. This requires new human–AI interfaces for explainable AI. In order to build effective and efficient interactive human–AI interfaces we have to deal with the question of how to evaluate the quality of explanations given by an explainable AI system. In this paper we introduce our System Causability Scale to measure the quality of explanations. It is based on our notion of Causability (Holzinger et al. in Wiley Interdiscip Rev Data Min Knowl Discov 9(4), 2019) combined with concepts adapted from a widely-accepted usability scale.","2020-06-01","2021-09-30 00:20:20","2022-12-20 04:41:28","2021-09-30","193-198","","2","34","","","","","","","","","","","","","","","","","Publisher: Springer QID: Q96439256","","C:\Users\ambreen.hanif\Zotero\storage\QIIWEGKN\Holzinger et al_2020_Measuring the Quality of Explanations.pdf","","","Explainable AI; Human–AI interfaces; System causability scale (SCS)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6MSL26LJ","journalArticle","","Clinciu, Miruna A; Hastie, Helen F","Proceedings of the 1st Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence (NL4XAI 2019)","","","","","","The field of Explainable Artificial Intelligence attempts to solve the problem of algorithmic opacity. Many terms and notions have been introduced recently to define Explainable AI, however, these terms seem to be used interchangeably , which is leading to confusion in this rapidly expanding field. As a solution to overcome this problem, we present an analysis of the existing research literature and examine how key terms, such as transparency, intelli-gibility, interpretability, and explainability are referred to and in what context. This paper, thus, moves towards a standard terminology for Explainable AI.","","2021-11-25 02:12:19","2022-12-20 04:41:27","2021-11-25","8-13","","","","","","","","","","","","","","","","","","","","QID: Q102635093","","C:\Users\ambreen.hanif\Zotero\storage\BDTGZ5ZG\Clinciu_Hastie_Proceedings of the 1st Workshop on Interactive Natural Language Technology for.pdf","","","Explainable AI; Explainability; black-box; Intelligibility; Inter-pretability; NLG; The-oretical Issues; Transparency","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LWZEFYGJ","conferencePaper","2018","Kim, Been; Wattenberg, Martin; Gilmer, Justin; Cai, Carrie; Wexler, James; Viegas, Fernanda; sayres, Rory","Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)","International conference on machine learning","","","","https://proceedings.mlr.press/v80/kim18d.html","The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result-for example , how sensitive a prediction of zebra is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.","2018-07-03","2021-09-28 01:45:04","2022-11-21 00:40:33","2021-09-28","2668-2677","","","6","","","","","","","","PMLR","","","","","","","","","ISSN: 2640-3498","","C:\Users\ambreen.hanif\Zotero\storage\LLL2Z49P\Kim et al_2018_Interpretability Beyond Feature Attribution.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ICML","","","","","","","","","","","","","","",""
"IJGS6BDY","journalArticle","2022","Fu, Tingchen; Gao, Shen; Zhao, Xueliang; Wen, Ji-rong; Yan, Rui","Learning towards conversational AI: A survey","AI Open","","2666-6510","10.1016/j.aiopen.2022.02.001","https://www.sciencedirect.com/science/article/pii/S2666651022000079","Recent years have witnessed a surge of interest in the field of open-domain dialogue. Thanks to the rapid development of social media, large dialogue corpus from the Internet builds up a fundamental premise for data-driven dialogue model. The breakthrough in neural network also brings new ideas to researchers in AI and NLP. A great number of new techniques and methods therefore came into being. In this paper, we review some of the most representative works in recent years and divide existing prevailing frameworks for a dialogue model into three categories. We further analyze the trend of development for open-domain dialogue and summarize the goal of an open-domain dialogue system in two aspects, informative and controllable. The methods we review in this paper are selected according to our unique perspectives and by no means complete. Rather, we hope this servery could benefit NLP community for future research in open-domain dialogue.","2022-01-01","2022-10-08 07:24:33","2022-10-08 07:24:33","2022-10-08 07:25:47","14-28","","","3","","AI Open","Learning towards conversational AI","","","","","","","en","","","","","ScienceDirect","","","","C:\Users\ambreen.hanif\Zotero\storage\D8A2F8BZ\Fu et al. - 2022 - Learning towards conversational AI A survey.html","","","Controllable dialogue; Human-machine conversation; Informativeness dialogue; Response generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"58AXB3XG","conferencePaper","2020","Li, Chi-Hsun; Yeh, Su-Fang; Chang, Tang-Jie; Tsai, Meng-Hsuan; Chen, Ken; Chang, Yung-Ju","A Conversation Analysis of Non-Progress and Coping Strategies with a Banking Task-Oriented Chatbot","Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems","978-1-4503-6708-0","","10.1145/3313831.3376209","https://doi.org/10.1145/3313831.3376209","Task-oriented chatbots are becoming popular alternatives for fulfilling users' needs, but few studies have investigated how users cope with conversational 'non-progress' (NP) in their daily lives. Accordingly, we analyzed a three-month conversation log between 1,685 users and a task-oriented banking chatbot. In this data, we observed 12 types of conversational NP; five types of content that was unexpected and challenging for the chatbot to recognize; and 10 types of coping strategies. Moreover, we identified specific relationships between NP types and strategies, as well as signs that users were about to abandon the chatbot, including 1) three consecutive incidences of NP, 2) consecutive use of message reformulation or switching subjects, and 3) using message reformulation as the final strategy. Based on these findings, we provide design recommendations for task-oriented chatbots, aimed at reducing NP, guiding users through such NP, and improving user experiences to reduce the cessation of chatbot use.","2020-04-21","2022-10-07 00:52:44","2022-10-07 00:52:44","2022-10-06","1–12","","","","","","","CHI '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","false","https://dl.acm.org/doi/pdf/10.1145/3313831.3376209","","breakdowns; chatbot; conversation analysis; coping strategies; non-progress","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B2MDJ9NL","webpage","","","Course intake form  · Microsoft Learn for Educators","","","","","https://fsiwwlprd.powerappsportals.com/en-US/CourseIntakeForm/","","","2022-10-06 05:00:12","2022-10-06 05:00:12","2022-10-06 05:01:25","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NNHUNBGL","webpage","","","","","","","","https://probml.github.io/pml-book/book1.html","","","2022-08-25 06:06:05","2022-08-25 06:06:05","2022-08-25 06:06:05","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\YNW3YQ8J\book1.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YFBKBI9C","journalArticle","2019","Sayres, Rory; Taly, Ankur; Rahimy, Ehsan; Blumer, Katy; Coz, David; Hammel, Naama; Krause, Jonathan; Narayanaswamy, Arunachalam; Rastegar, Zahra; Wu, Derek; Xu, Shawn; Barb, Scott; Joseph, Anthony; Shumski, Michael; Smith, Jesse; Sood, Arjun B.; Corrado, Greg S.; Peng, Lily; Webster, Dale R.","Using a Deep Learning Algorithm and Integrated Gradients Explanation to Assist Grading for Diabetic Retinopathy","Ophthalmology","","0161-6420, 1549-4713","10.1016/j.ophtha.2018.11.016","https://www.aaojournal.org/article/S0161-6420(18)31575-6/fulltext","","2019-04-01","2022-08-25 06:03:39","2022-08-25 06:03:39","2022-08-25 06:03:39","552-564","","4","126","","Ophthalmology","","","","","","","","English","","","","","www.aaojournal.org","","Publisher: Elsevier PMID: 30553900","","; C:\Users\ambreen.hanif\Zotero\storage\2Q9DFXWF\Sayres et al_2019_Using a Deep Learning Algorithm and Integrated Gradients Explanation to Assist.pdf; C:\Users\ambreen.hanif\Zotero\storage\23V5IF9Y\fulltext.html","http://www.ncbi.nlm.nih.gov/pubmed/30553900","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V6UB68ZL","journalArticle","2021","Schnake, Thomas; Eberle, Oliver; Lederer, Jonas; Nakajima, Shinichi; Schütt, Kristof T.; Müller, Klaus-Robert; Montavon, Grégoire","Higher-Order Explanations of Graph Neural Networks via Relevant Walks","IEEE Transactions on Pattern Analysis and Machine Intelligence","","0162-8828, 2160-9292, 1939-3539","10.1109/TPAMI.2021.3115452","http://arxiv.org/abs/2006.03589","Graph Neural Networks (GNNs) are a popular approach for predicting graph structured data. As GNNs tightly entangle the input graph into the neural network structure, common explainable AI approaches are not applicable. To a large extent, GNNs have remained black-boxes for the user so far. In this paper, we show that GNNs can in fact be naturally explained using higher-order expansions, i.e. by identifying groups of edges that jointly contribute to the prediction. Practically, we find that such explanations can be extracted using a nested attribution scheme, where existing techniques such as layer-wise relevance propagation (LRP) can be applied at each step. The output is a collection of walks into the input graph that are relevant for the prediction. Our novel explanation method, which we denote by GNN-LRP, is applicable to a broad range of graph neural networks and lets us extract practically relevant insights on sentiment analysis of text data, structure-property relationships in quantum chemistry, and image classification.","2021","2022-08-25 06:01:00","2022-08-25 06:01:00","2022-08-25 06:01:00","1-1","","","","","IEEE Trans. Pattern Anal. Mach. Intell.","","","","","","","","","","","","","arXiv.org","","arXiv:2006.03589 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\P92P4XZZ\2006.html; C:\Users\ambreen.hanif\Zotero\storage\B2J5ZMNV\Schnake et al_2021_Higher-Order Explanations of Graph Neural Networks via Relevant Walks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LBAGBNV6","conferencePaper","2019","Liu, Frederick; Avci, Besim","Incorporating Priors with Feature Attribution on Text Classification","Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics","","","10.18653/v1/P19-1631","https://aclanthology.org/P19-1631","Feature attribution methods, proposed recently, help users interpret the predictions of complex models. Our approach integrates feature attributions into the objective function to allow machine learning practitioners to incorporate priors in model building. To demonstrate the effectiveness our technique, we apply it to two tasks: (1) mitigating unintended bias in text classifiers by neutralizing identity terms; (2) improving classifier performance in scarce data setting by forcing model to focus on toxic terms. Our approach adds an L2 distance loss between feature attributions and task-specific prior values to the objective. Our experiments show that i) a classifier trained with our technique reduces undesired model biases without a tradeoff on the original task; ii) incorporating prior helps model performance in scarce data settings.","2019-07","2022-08-25 05:59:29","2022-08-25 05:59:29","2022-08-25 05:59:29","6274–6283","","","","","","","","","","","Association for Computational Linguistics","Florence, Italy","","","","","","ACLWeb","","","","C:\Users\ambreen.hanif\Zotero\storage\K9X48CUE\Liu_Avci_2019_Incorporating Priors with Feature Attribution on Text Classification.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ACL 2019","","","","","","","","","","","","","","",""
"FDAMR7N4","journalArticle","2021","Sassi, Imad; Anter, Samir; Bekkhoucha, Abdelkrim","A graph-based big data optimization approach using hidden Markov model and constraint satisfaction problem","Journal of Big Data","","2196-1115","10.1186/s40537-021-00485-z","https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00485-z","To address the challenges of big data analytics, several works have focused on big data optimization using metaheuristics. The constraint satisfaction problem (CSP) is a fundamental concept of metaheuristics that has shown great efficiency in several fields. Hidden Markov models (HMMs) are powerful machine learning algorithms that are applied especially frequently in time series analysis. However, one issue in forecasting time series using HMMs is how to reduce the search space (state and observation space). To address this issue, we propose a graph-based big data optimization approach using a CSP to enhance the results of learning and prediction tasks of HMMs. This approach takes full advantage of both HMMs, with the richness of their algorithms, and CSPs, with their many powerful and efficient solver algorithms. To verify the validity of the model, the proposed approach is evaluated on real-world data using the mean absolute percentage error (MAPE) and other metrics as measures of the prediction accuracy. The conducted experiments show that the proposed model outperforms the conventional model. It reduces the MAPE by 0.71% and offers a particularly good trade-off between computational costs and the quality of results for large datasets. It is also competitive with benchmark models in terms of the running time and prediction accuracy. Further comparisons substantiate these experimental findings.","2021-12","2022-08-23 00:24:32","2022-08-23 00:24:32","2022-08-23 00:24:32","93","","1","8","","J Big Data","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\ambreen.hanif\Zotero\storage\VHWRS4AX\Sassi et al. - 2021 - A graph-based big data optimization approach using.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V3TNIEX7","preprint","2017","Bojanowski, Piotr; Grave, Edouard; Joulin, Armand; Mikolov, Tomas","Enriching Word Vectors with Subword Information","","","","10.48550/arXiv.1607.04606","http://arxiv.org/abs/1607.04606","Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.","2017-06-19","2022-08-23 00:00:36","2022-08-23 00:00:36","2022-08-23 00:00:36","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1607.04606 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\ZEAQ2M37\1607.html; C:\Users\ambreen.hanif\Zotero\storage\79WCFCAS\Bojanowski et al_2017_Enriching Word Vectors with Subword Information.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1607.04606","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UI6VWF4F","preprint","2014","Maleki, Sasan; Tran-Thanh, Long; Hines, Greg; Rahwan, Talal; Rogers, Alex","Bounding the Estimation Error of Sampling-based Shapley Value Approximation","","","","10.48550/arXiv.1306.4265","http://arxiv.org/abs/1306.4265","The Shapley value is arguably the most central normative solution concept in cooperative game theory. It specifies a unique way in which the reward from cooperation can be ""fairly"" divided among players. While it has a wide range of real world applications, its use is in many cases hampered by the hardness of its computation. A number of researchers have tackled this problem by (i) focusing on classes of games where the Shapley value can be computed efficiently, or (ii) proposing representation formalisms that facilitate such efficient computation, or (iii) approximating the Shapley value in certain classes of games. For the classical \textit{characteristic function} representation, the only attempt to approximate the Shapley value for the general class of games is due to Castro \textit{et al.} \cite{castro}. While this algorithm provides a bound on the approximation error, this bound is \textit{asymptotic}, meaning that it only holds when the number of samples increases to infinity. On the other hand, when a finite number of samples is drawn, an unquantifiable error is introduced, meaning that the bound no longer holds. With this in mind, we provide non-asymptotic bounds on the estimation error for two cases: where (i) the \textit{variance}, and (ii) the \textit{range}, of the players' marginal contributions is known. Furthermore, for the second case, we show that when the range is significantly large relative to the Shapley value, the bound can be improved (from $O(\frac{r}{m})$ to $O(\sqrt{\frac{r}{m}})$). Finally, we propose, and demonstrate the effectiveness of using stratified sampling for improving the bounds further.","2014-02-12","2022-08-18 02:35:53","2022-08-18 02:35:53","2022-08-18 02:35:53","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1306.4265 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\YR5IRED5\1306.html; C:\Users\ambreen.hanif\Zotero\storage\CXASFGEU\Maleki et al_2014_Bounding the Estimation Error of Sampling-based Shapley Value Approximation.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1306.4265","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RE5EU3UK","preprint","2014","Maleki, Sasan; Tran-Thanh, Long; Hines, Greg; Rahwan, Talal; Rogers, Alex","Bounding the Estimation Error of Sampling-based Shapley Value Approximation","","","","10.48550/arXiv.1306.4265","http://arxiv.org/abs/1306.4265","The Shapley value is arguably the most central normative solution concept in cooperative game theory. It specifies a unique way in which the reward from cooperation can be ""fairly"" divided among players. While it has a wide range of real world applications, its use is in many cases hampered by the hardness of its computation. A number of researchers have tackled this problem by (i) focusing on classes of games where the Shapley value can be computed efficiently, or (ii) proposing representation formalisms that facilitate such efficient computation, or (iii) approximating the Shapley value in certain classes of games. For the classical \textit{characteristic function} representation, the only attempt to approximate the Shapley value for the general class of games is due to Castro \textit{et al.} \cite{castro}. While this algorithm provides a bound on the approximation error, this bound is \textit{asymptotic}, meaning that it only holds when the number of samples increases to infinity. On the other hand, when a finite number of samples is drawn, an unquantifiable error is introduced, meaning that the bound no longer holds. With this in mind, we provide non-asymptotic bounds on the estimation error for two cases: where (i) the \textit{variance}, and (ii) the \textit{range}, of the players' marginal contributions is known. Furthermore, for the second case, we show that when the range is significantly large relative to the Shapley value, the bound can be improved (from $O(\frac{r}{m})$ to $O(\sqrt{\frac{r}{m}})$). Finally, we propose, and demonstrate the effectiveness of using stratified sampling for improving the bounds further.","2014-02-12","2022-08-18 02:35:51","2022-08-18 02:35:51","2022-08-18 02:35:51","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1306.4265 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\ENH23FG7\1306.html; C:\Users\ambreen.hanif\Zotero\storage\2LYP7B7G\Maleki et al_2014_Bounding the Estimation Error of Sampling-based Shapley Value Approximation.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1306.4265","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QBFMSIIU","journalArticle","2018","Montavon, Grégoire; Samek, Wojciech; Müller, Klaus-Robert","Methods for interpreting and understanding deep neural networks","Digital Signal Processing","","1051-2004","10.1016/j.dsp.2017.10.011","https://www.sciencedirect.com/science/article/pii/S1051200417302385","This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to make most efficient use of it on real data.","2018-02-01","2023-04-06 03:28:39","2023-04-06 03:28:42","2023-04-06 03:28:39","1-15","","","73","","Digital Signal Processing","","","","","","","","en","","","","","ScienceDirect","","1050 citations (Crossref) [2023-04-06]","","C:\Users\ambreen.hanif\Zotero\storage\DX5SWZ4C\Montavon et al. - 2018 - Methods for interpreting and understanding deep ne.pdf; C:\Users\ambreen.hanif\Zotero\storage\QDP8WZR5\S1051200417302385.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FVBVMPHY","book","2019","Ribera Turró, Mireia; Lapedriza, Agata","Can we do better explanations? A proposal of User-Centered Explainable AI","","","","","","Artificial Intelligence systems are spreading to multiple applications and they are used by a more diverse audience. With this change of the use scenario, AI users will increasingly require explanations. The first part of this paper makes a review of the state of the art of Explainable AI and highlights how the current research is not paying enough attention to whom the explanations are targeted. In the second part of the paper, it is suggested a new explainability pipeline, where users are classified in three main groups (developers or AI researchers, domain experts and lay users). Inspired by the cooperative principles of conversations, it is discussed how creating different explanations for each of the targeted groups can overcome some of the difficulties related to creating good explanations and evaluating them.","2019-03-12","2023-04-06 03:21:00","2023-04-06 03:21:00","","","","","","","","Can we do better explanations?","","","","","","","","","","","","ResearchGate","","","","","https://www.researchgate.net/profile/Mireia-Ribera-Turro/publication/339390078_Can_we_do_better_explanations_A_proposal_of_User-Centered_Explainable_AI/links/5e4ede8692851c7f7f48f8a7/Can-we-do-better-explanations-A-proposal-of-User-Centered-Explainable-AI.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3BLNF3T9","journalArticle","2020","Akata, Zeynep; Balliet, Dan; de Rijke, Maarten; Dignum, Frank; Dignum, Virginia; Eiben, Guszti; Fokkens, Antske; Grossi, Davide; Hindriks, Koen; Hoos, Holger; Hung, Hayley; Jonker, Catholijn; Monz, Christof; Neerincx, Mark; Oliehoek, Frans; Prakken, Henry; Schlobach, Stefan; van der Gaag, Linda; van Harmelen, Frank; van Hoof, Herke; van Riemsdijk, Birna; van Wynsberghe, Aimee; Verbrugge, Rineke; Verheij, Bart; Vossen, Piek; Welling, Max","A Research Agenda for Hybrid Intelligence: Augmenting Human Intellect With Collaborative, Adaptive, Responsible, and Explainable Artificial Intelligence","Computer","","0018-9162","10.1109/MC.2020.2996587","https://doi.org/10.1109/MC.2020.2996587","We define hybrid intelligence (HI) as the combination of human and machine intelligence, augmenting human intellect and capabilities instead of replacing them and achieving goals that were unreachable by either humans or machines. HI is an important new research focus for artificial intelligence, and we set a research agenda for HI by formulating four challenges.","2020-08-01","2023-04-06 00:28:16","2023-04-06 00:28:19","2023-04-06 00:28:16","18–28","","8","53","","Computer","A Research Agenda for Hybrid Intelligence","","","","","","","","","","","","ACM Digital Library","","66 citations (Crossref) [2023-04-06]","","C:\Users\ambreen.hanif\Zotero\storage\5BTHTQPF\Akata et al_2020_A Research Agenda for Hybrid Intelligence.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7H72IBTW","journalArticle","2021","Markus, Aniek F.; Kors, Jan A.; Rijnbeek, Peter R.","The role of explainability in creating trustworthy artificial intelligence for health care: A comprehensive survey of the terminology, design choices, and evaluation strategies","Journal of Biomedical Informatics","","1532-0464","10.1016/j.jbi.2020.103655","https://www.sciencedirect.com/science/article/pii/S1532046420302835","Artificial intelligence (AI) has huge potential to improve the health and well-being of people, but adoption in clinical practice is still limited. Lack of transparency is identified as one of the main barriers to implementation, as clinicians should be confident the AI system can be trusted. Explainable AI has the potential to overcome this issue and can be a step towards trustworthy AI. In this paper we review the recent literature to provide guidance to researchers and practitioners on the design of explainable AI systems for the health-care domain and contribute to formalization of the field of explainable AI. We argue the reason to demand explainability determines what should be explained as this determines the relative importance of the properties of explainability (i.e. interpretability and fidelity). Based on this, we propose a framework to guide the choice between classes of explainable AI methods (explainable modelling versus post-hoc explanation; model-based, attribution-based, or example-based explanations; global and local explanations). Furthermore, we find that quantitative evaluation metrics, which are important for objective standardized evaluation, are still lacking for some properties (e.g. clarity) and types of explanations (e.g. example-based methods). We conclude that explainable modelling can contribute to trustworthy AI, but the benefits of explainability still need to be proven in practice and complementary measures might be needed to create trustworthy AI in health care (e.g. reporting data quality, performing extensive (external) validation, and regulation).","2021-01-01","2023-04-06 00:24:53","2023-04-06 00:24:56","2023-04-06 00:24:53","103655","","","113","","Journal of Biomedical Informatics","The role of explainability in creating trustworthy artificial intelligence for health care","","","","","","","en","","","","","ScienceDirect","","112 citations (Crossref) [2023-04-06]","","C:\Users\ambreen.hanif\Zotero\storage\YHM95ZUY\Markus et al_2021_The role of explainability in creating trustworthy artificial intelligence for.pdf; C:\Users\ambreen.hanif\Zotero\storage\H724PW3Q\S1532046420302835.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZC3AEXK8","journalArticle","2018","Yu, Rulei; Shi, Lei","A user-based taxonomy for deep learning visualization","Visual Informatics","","2468-502X","10.1016/j.visinf.2018.09.001","https://www.sciencedirect.com/science/article/pii/S2468502X1830038X","Deep learning has achieved impressive success in a variety of tasks and is developing rapidly in recent years. The problem of understanding the deep learning models has become an issue for the development of deep learning, for example, in domains like medicine and finance which require interpretable models. While it is challenging to analyze and interpret complicated deep neural networks, visualization is good at bridging between abstract data and intuitive representations. Visual analytics for deep learning is a rapidly growing research field. To help users better understand this field, we present a mini-survey including a user-based taxonomy that covers state-of-the-art works of the field. Regarding the requirements of different types of users (beginners, practitioners, developers, and experts), we categorize the methods and tools by four visualization goals respectively focusing on teaching deep learning concepts, architecture assessment, tools for debugging and improving models, and visual explanation. Notably, we present a table consisting of the name of the method or tool, the year, the visualization goal, and the types of networks to which the method or tool can be applied, to assist users in finding available tools and methods quickly. To emphasize the importance of visual explanation for deep learning, we introduce the studies in this research field in detail.","2018-09-01","2023-04-06 00:13:32","2023-04-06 00:13:35","2023-04-06 00:13:32","147-154","","3","2","","Visual Informatics","","","","","","","","en","","","","","ScienceDirect","","15 citations (Crossref) [2023-04-06]","","C:\Users\ambreen.hanif\Zotero\storage\GXEMVXXX\S2468502X1830038X.html; C:\Users\ambreen.hanif\Zotero\storage\54TBQQIL\Yu_Shi_2018_A user-based taxonomy for deep learning visualization.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QRB77DJ6","bookSection","2022","Pathak, Sandeep","Explainable AI for ML Ops","World of Business with Data and Analytics","978-981-19568-9-8","","","https://doi.org/10.1007/978-981-19-5689-8_12","This chapter explains the significance of blending two emerging technologies in AI/ML—Explainable AI (XAI) and Machine Learning Operations (ML Ops) and demonstrates a focused use case that derives value from leveraging XAI to enhance ML Ops. The chapter starts by laying out the “growing pains” problems that enterprises are encountering to scale AI. We highlight the relatively low maturity of post-production ML processes thus exposing enterprises to reputation, compliance, and hence financial risk. We give a historical perspective on the rise of AI. Subsequent gain in mindshare of ML Ops is explained. XAI as a solution is introduced. After a brief explanation on XAI, we delve into the experimental setup and detail the results that demonstrate the potential of using XAI to enhance ML Ops. We end the white paper by reiterating the benefits, opportunities, and market potential and finally some recommendations.","2022","2023-04-05 04:32:53","2023-04-05 04:32:53","2023-04-05 04:32:53","187-201","","","","","","","Studies in Autonomic, Data-driven and Industrial Computing","","","","Springer Nature","Singapore","en","","","","","Springer Link","","DOI: 10.1007/978-981-19-5689-8_12","","","","","","Sharma, Neha; Bhatavdekar, Mandar","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VDRQU6AP","conferencePaper","2018","Alvarez-Melis, David; Jaakkola, Tommi S.","Towards robust interpretability with self-explaining neural networks","Proceedings of the 32nd International Conference on Neural Information Processing Systems","","","","","Most recent work on interpretability of complex machine learning models has focused on estimating a posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general – explicitness, faithfulness, and stability – and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.","2018-12-03","2023-04-04 05:47:03","2023-04-04 05:47:03","2023-04-03","7786–7795","","","","","","","NIPS'18","","","","Curran Associates Inc.","Red Hook, NY, USA","","","","","","ACM Digital Library","","","","C:\Users\ambreen.hanif\Zotero\storage\YTWSJQQA\Alvarez-Melis_Jaakkola_2018_Towards robust interpretability with self-explaining neural networks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YF6JIDJP","computerProgram","2023","","Machine Learning Interpretability (MLI)","","","","","https://github.com/h2oai/mli-resources","H2O.ai Machine Learning Interpretability Resources","2023-03-21","2023-04-04 05:22:46","2023-04-04 05:22:46","2023-04-04 05:22:45","","","","","","","","","","","","H2O.ai","","","","","","","GitHub","","original-date: 2017-10-19T20:56:24Z","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Jupyter Notebook","","","","","","","","",""
"ZV6KXF9N","preprint","2021","Jin, Ming; Chang, Heng; Zhu, Wenwu; Sojoudi, Somayeh","Power up! Robust Graph Convolutional Network via Graph Powering","","","","","http://arxiv.org/abs/1905.10029","Graph convolutional networks (GCNs) are powerful tools for graph-structured data. However, they have been recently shown to be vulnerable to topological attacks. To enhance adversarial robustness, we go beyond spectral graph theory to robust graph theory. By challenging the classical graph Laplacian, we propose a new convolution operator that is provably robust in the spectral domain and is incorporated in the GCN architecture to improve expressivity and interpretability. By extending the original graph to a sequence of graphs, we also propose a robust training paradigm that encourages transferability across graphs that span a range of spatial and spectral characteristics. The proposed approaches are demonstrated in extensive experiments to simultaneously improve performance in both benign and adversarial situations.","2021-09-21","2023-04-04 04:24:56","2023-04-04 04:24:56","2023-04-04 04:24:56","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1905.10029 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\7W2ZXZIX\1905.html; C:\Users\ambreen.hanif\Zotero\storage\7L5A76VR\Jin et al_2021_Power up.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1905.10029","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I6USFPVF","preprint","2016","Hendricks, Lisa Anne; Akata, Zeynep; Rohrbach, Marcus; Donahue, Jeff; Schiele, Bernt; Darrell, Trevor","Generating Visual Explanations","","","","10.48550/arXiv.1603.08507","http://arxiv.org/abs/1603.08507","Clearly explaining a rationale for a classification decision to an end-user can be as important as the decision itself. Existing approaches for deep visual recognition are generally opaque and do not output any justification text; contemporary vision-language models can describe image content but fail to take into account class-discriminative image aspects which justify visual predictions. We propose a new model that focuses on the discriminating properties of the visible object, jointly predicts a class label, and explains why the predicted label is appropriate for the image. We propose a novel loss function based on sampling and reinforcement learning that learns to generate sentences that realize a global sentence property, such as class specificity. Our results on a fine-grained bird species classification dataset show that our model is able to generate explanations which are not only consistent with an image but also more discriminative than descriptions produced by existing captioning methods.","2016-03-28","2023-04-04 01:39:29","2023-04-04 01:39:29","2023-04-04 01:39:29","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1603.08507 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\M9L6QB2D\1603.html; C:\Users\ambreen.hanif\Zotero\storage\ZTGKVNDR\Hendricks et al_2016_Generating Visual Explanations.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1603.08507","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IA3ERF95","webpage","","","[1603.08507] Generating Visual Explanations","","","","","https://arxiv.org/abs/1603.08507","","","2023-04-04 01:36:58","2023-04-04 01:36:58","2023-04-04 01:36:58","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"57NKP66V","webpage","","","oracle/skater: Python Library for Model Interpretation/Explanations","","","","","https://github.com/oracle/Skater","","","2023-04-03 04:08:46","2023-04-04 00:30:21","2023-04-03 04:08:46","","","","","","","","","","","","","","","","code","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\JIBYKRKN\Skater.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LBKTB5I9","computerProgram","","","h2oai / h2o-3","","","","","https://github.com/h2oai/h2o-3","","","2023-04-04 00:03:23","2023-04-04 00:04:28","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VH8HK6IZ","computerProgram","2023","","explAIner","","","","","https://github.com/dbvis-ukon/explainer","The official repository containing the source code to the explAIner publication.","2023-03-30","2023-04-03 04:20:29","2023-04-03 04:20:29","2023-04-03 04:20:29","","","","","","","","","","","","Data Analysis & Visualization Group","","","","","","","GitHub","","original-date: 2021-02-16T09:04:55Z","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","TypeScript","","","","","","","","",""
"5EHHVIDA","computerProgram","2023","","fat-forensics/fat-forensics","","","","","https://github.com/fat-forensics/fat-forensics","Modular Python Toolbox for Fairness, Accountability and Transparency Forensics","2023-03-14","2023-04-03 04:09:34","2023-04-03 04:09:34","2023-04-03 04:09:33","","","","","","","","","","","","FAT Forensics","","","BSD-3-Clause","","","","GitHub","","original-date: 2018-08-30T16:29:23Z","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Python","","","","","","","","",""
"9SV3ICEQ","computerProgram","2021","Meudec, Raphael","tf-explain","","","","","https://github.com/sicara/tf-explain","Interpretability Methods for tf.keras models with Tensorflow 2.x","2021-02","2023-04-03 04:08:57","2023-04-03 04:08:57","2023-04-03 04:08:57","","","","","","","","","","","","","","","MIT","","","","GitHub","","DOI: 10.5281/zenodo.5711704","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Python","0.3.1","","","","","","","",""
"YDH497HR","webpage","","","interpretml/interpret: Fit interpretable models. Explain blackbox machine learning.","","","","","https://github.com/interpretml/interpret","","","2023-04-03 04:08:43","2023-04-03 04:08:43","2023-04-03 04:08:43","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\SSMUHSV9\interpret.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5KHZLX79","webpage","","","albermax/innvestigate: A toolbox to iNNvestigate neural networks' predictions!","","","","","https://github.com/albermax/innvestigate","","","2023-04-03 04:08:40","2023-04-03 04:08:40","2023-04-03 04:08:40","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\SVWV8ZYQ\innvestigate.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UY468UK6","webpage","","","h2oai/mli-resources: H2O.ai Machine Learning Interpretability Resources","","","","","https://github.com/h2oai/mli-resources","","","2023-04-03 04:08:38","2023-04-03 04:08:38","2023-04-03 04:08:38","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\C9M8WAEX\mli-resources.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KNE5W6MH","computerProgram","2023","","moDel Agnostic Language for Exploration and eXplanation","","","","","https://github.com/ModelOriented/DALEX","moDel Agnostic Language for Exploration and eXplanation","2023-04-01","2023-04-03 04:08:36","2023-04-03 04:08:36","2023-04-03 04:08:35","","","","","","","","","","","","Model Oriented","","","GPL-3.0","","","","GitHub","","original-date: 2018-02-18T03:24:12Z","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Python","","","","","","","","",""
"AHBGXZ3Q","webpage","","","EthicalML/xai: XAI - An eXplainability toolbox for machine learning","","","","","https://github.com/EthicalML/xai","","","2023-04-03 04:08:33","2023-04-03 04:08:33","2023-04-03 04:08:33","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\ITKIEIY9\xai.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UWGHRQ9P","computerProgram","2023","","XAI - An eXplainability toolbox for machine learning","","","","","https://github.com/EthicalML/xai","XAI - An eXplainability toolbox for machine learning","2023-03-31","2023-04-03 03:21:03","2023-04-03 03:21:03","2023-04-03 03:21:03","","","","","","","","","","","","The Institute for Ethical Machine Learning","","","MIT","","","","GitHub","","original-date: 2019-01-11T20:00:09Z","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Python","","","","","","","","",""
"3YWJE2YT","webpage","","","Descriptive mAchine Learning EXplanations • DALEX2","","","","","https://modeloriented.github.io/DALEX2/","","","2023-04-03 03:20:39","2023-04-03 03:20:39","2023-04-03 03:20:39","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\S9YC28VQ\DALEX2.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AQP5TCEA","journalArticle","2021","Klaise, Janis; Looveren, Arnaud Van; Vacanti, Giovanni; Coca, Alexandru","Alibi Explain: Algorithms for Explaining Machine Learning Models","Journal of Machine Learning Research","","1533-7928","","http://jmlr.org/papers/v22/21-0017.html","We introduce Alibi Explain, an open-source Python library for explaining predictions of machine learning models (https://github.com/SeldonIO/alibi). The library features state-of-the-art explainability algorithms for classification and regression models. The algorithms cover both the model-agnostic (black-box) and model-specific (white-box) setting, cater for multiple data types (tabular, text, images) and explanation scope (local and global explanations). The library exposes a unified API enabling users to work with explanations in a consistent way. Alibi adheres to best development practices featuring extensive testing of code correctness and algorithm convergence in a continuous integration environment. The library comes with extensive documentation of both usage and theoretical background of methods, and a suite of worked end-to-end use cases. Alibi aims to be a production-ready toolkit with integrations into machine learning deployment platforms such as Seldon Core and KFServing, and distributed explanation capabilities using Ray.","2021","2023-04-03 03:17:51","2023-04-03 03:18:02","2023-04-03 03:17:51","1-7","","181","22","","","Alibi Explain","","","","","","","","","","","","jmlr.org","","","","C:\Users\ambreen.hanif\Zotero\storage\MICCHL9B\Klaise et al_2021_Alibi Explain.pdf; C:\Users\ambreen.hanif\Zotero\storage\BLPCRJ58\alibi.html","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"68J5F6BJ","conferencePaper","2021","Jacovi, Alon; Marasović, Ana; Miller, Tim; Goldberg, Yoav","Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI","Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency","978-1-4503-8309-7","","10.1145/3442188.3445923","https://dl.acm.org/doi/10.1145/3442188.3445923","Trust is a central component of the interaction between people and AI, in that 'incorrect' levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in AI? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we promote them, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, interpersonal trust (i.e., trust between people) as defined by sociologists. This model rests on two key properties: the vulnerability of the user; and the ability to anticipate the impact of the AI model's decisions. We incorporate a formalization of 'contractual trust', such that trust between a user and an AI model is trust that some implicit or explicit contract will hold, and a formalization of 'trustworthiness' (that detaches from the notion of trustworthiness in sociology), and with it concepts of 'warranted' and 'unwarranted' trust. We present the possible causes of warranted trust as intrinsic reasoning and extrinsic behavior, and discuss how to design trustworthy AI, how to evaluate whether trust has manifested, and whether it is warranted. Finally, we elucidate the connection between trust and XAI using our formalization.","2021-03-01","2023-04-03 02:49:20","2023-04-03 02:49:22","2023-04-02","624–635","","","","","","Formalizing Trust in Artificial Intelligence","FAccT '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","77 citations (Crossref) [2023-04-03]","","C:\Users\ambreen.hanif\Zotero\storage\JHNL2W3U\Jacovi et al_2021_Formalizing Trust in Artificial Intelligence.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4J3IND9L","conferencePaper","2020","Kumar, Abhishek; Braud, Tristan; Tarkoma, Sasu; Hui, Pan","Trustworthy AI in the Age of Pervasive Computing and Big Data","2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)","","","10.1109/PerComWorkshops48775.2020.9156127","","The era of pervasive computing has resulted in countless devices that continuously monitor users and their environment, generating an abundance of user behavioural data. Such data may support improving the quality of service, but may also lead to adverse usages such as surveillance and advertisement. In parallel, Artificial Intelligence (AI) systems are being applied to sensitive fields such as healthcare, justice, or human resources, raising multiple concerns on the trustworthiness of such systems. Trust in AI systems is thus intrinsically linked to ethics, including the ethics of algorithms, the ethics of data, or the ethics of practice. In this paper, we formalise the requirements of trustworthy AI systems through an ethics perspective. We specifically focus on the aspects that can be integrated into the design and development of AI systems. After discussing the state of research and the remaining challenges, we show how a concrete use-case in smart cities can benefit from these methods.","2020-03","2023-04-03 02:46:09","2023-04-03 02:46:12","","1-6","","","","","","","","","","","","","","","","","","IEEE Xplore","","19 citations (Crossref) [2023-04-03]","","C:\Users\ambreen.hanif\Zotero\storage\SN88GYDI\9156127.html; C:\Users\ambreen.hanif\Zotero\storage\3YC5F259\Kumar et al_2020_Trustworthy AI in the Age of Pervasive Computing and Big Data.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)","","","","","","","","","","","","","","",""
"FJJQ4XGN","journalArticle","2021","Kute, Dattatray Vishnu; Pradhan, Biswajeet; Shukla, Nagesh; Alamri, Abdullah","Deep Learning and Explainable Artificial Intelligence Techniques Applied for Detecting Money Laundering–A Critical Review","IEEE Access","","2169-3536","10.1109/ACCESS.2021.3086230","","Money laundering has been a global issue for decades, which is one of the major threat for economy and society. Government, regulatory and financial institutions are combating it together in their respective capacity, however still billions of dollars in fines by authorities make the headlines in the news. High-speed internet services have enabled financial institutions to deliver better customer experience through multi-channel engagements, which has led to exponential growth in transactions and new avenues for laundering the money for fraudsters. Literature shows the usage of statistical methods, data mining and Machine Learning (ML) techniques for money laundering detection, but limited research on Deep Learning (DL) techniques, primarily due to lack of model interpretability and explainability of the decisions made. Several studies are conducted on application of ML for Anti-Money Laundering (AML), and Explainable Artificial Intelligence (XAI) techniques in general, but lacks the study on usage of DL techniques together with XAI. This paper aims to review the current state-of-the-art literature on DL together with XAI for identifying suspicious money laundering transactions and identify future research areas. Key findings of the review are, researchers have preferred variants of Convolutional Neural Networks, and AutoEncoder; graph deep learning together with natural language processing is emerging as an important technology for AML; XAI use is not seen in AML domain; 51% ML methods used in AML are non-interpretable, 58% studies used sample of old real data; key challenges for researchers are access to recent real transaction data and scarcity of labelled training data; and data being highly imbalanced. Future research directions are, application of XAI techniques to bring-out explainability, graph deep learning using natural language processing (NLP), unsupervised and reinforcement learning to handle lack of labelled data; and joint research programs between research community and industry to benefit from domain knowledge and controlled access to data.","2021","2023-04-01 00:31:12","2023-04-02 23:08:18","","82300-82317","","","9","","","","","","","","","","","","","","","IEEE Xplore","","19 citations (Crossref) [2023-04-03] Conference Name: IEEE Access","","C:\Users\ambreen.hanif\Zotero\storage\IN4I5RWZ\Kute et al. - 2021 - Deep Learning and Explainable Artificial Intellige.pdf","","","Deep learning; Machine learning; Artificial intelligence; Data mining; explainable AI; machine learning; Australia; deep learning; Finance; Money laundering; Statistical analysis; suspicious transaction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZU4XNLE3","journalArticle","2022","Adak, Anirban; Pradhan, Biswajeet; Shukla, Nagesh","Sentiment Analysis of Customer Reviews of Food Delivery Services Using Deep Learning and Explainable Artificial Intelligence: Systematic Review","Foods","","2304-8158","10.3390/foods11101500","https://www.mdpi.com/2304-8158/11/10/1500","During the COVID-19 crisis, customers’ preference in having food delivered to their doorstep instead of waiting in a restaurant has propelled the growth of food delivery services (FDSs). With all restaurants going online and bringing FDSs onboard, such as UberEATS, Menulog or Deliveroo, customer reviews on online platforms have become an important source of information about the company’s performance. FDS organisations aim to gather complaints from customer feedback and effectively use the data to determine the areas for improvement to enhance customer satisfaction. This work aimed to review machine learning (ML) and deep learning (DL) models and explainable artificial intelligence (XAI) methods to predict customer sentiments in the FDS domain. A literature review revealed the wide usage of lexicon-based and ML techniques for predicting sentiments through customer reviews in FDS. However, limited studies applying DL techniques were found due to the lack of the model interpretability and explainability of the decisions made. The key findings of this systematic review are as follows: 77% of the models are non-interpretable in nature, and organisations can argue for the explainability and trust in the system. DL models in other domains perform well in terms of accuracy but lack explainability, which can be achieved with XAI implementation. Future research should focus on implementing DL models for sentiment analysis in the FDS domain and incorporating XAI techniques to bring out the explainability of the models.","2022-01","2023-04-01 00:31:16","2023-04-02 23:08:13","2023-04-01 00:31:16","1500","","10","11","","","Sentiment Analysis of Customer Reviews of Food Delivery Services Using Deep Learning and Explainable Artificial Intelligence","","","","","","","en","http://creativecommons.org/licenses/by/3.0/","","","","www.mdpi.com","","7 citations (Crossref) [2023-04-03] Number: 10 Publisher: Multidisciplinary Digital Publishing Institute","","C:\Users\ambreen.hanif\Zotero\storage\S56G4CAW\Adak et al. - 2022 - Sentiment Analysis of Customer Reviews of Food Del.pdf","","","deep learning; explainable artificial intelligence; food delivery services; lime; sentiment analysis; shapley","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MB9YHJEE","conferencePaper","2022","Vaidya, Aditya R.; Jain, Shailee; Huth, Alexander","Self-Supervised Models of Audio Effectively Explain Human Cortical Responses to Speech","Proceedings of the 39th International Conference on Machine Learning","","","","https://proceedings.mlr.press/v162/vaidya22a.html","Self-supervised language models are very effective at predicting high-level cortical responses during language comprehension. However, the best current models of lower-level auditory processing in the human brain rely on either hand-constructed acoustic filters or representations from supervised audio neural networks. In this work, we capitalize on the progress of self-supervised speech representation learning (SSL) to create new state-of-the-art models of the human auditory system. Compared against acoustic baselines, phonemic features, and supervised models, representations from the middle layers of self-supervised models (APC, wav2vec, wav2vec 2.0, and HuBERT) consistently yield the best prediction performance for fMRI recordings within the auditory cortex (AC). Brain areas involved in low-level auditory processing exhibit a preference for earlier SSL model layers, whereas higher-level semantic areas prefer later layers. We show that these trends are due to the models’ ability to encode information at multiple linguistic levels (acoustic, phonetic, and lexical) along their representation depth. Overall, these results show that self-supervised models effectively capture the hierarchy of information relevant to different stages of speech processing in human cortex.","2022-06-28","2023-03-31 02:45:45","2023-03-31 02:45:45","2023-03-31 02:45:45","21927-21944","","","","","","","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 2640-3498","","C:\Users\ambreen.hanif\Zotero\storage\5E6VQDYI\Vaidya et al. - 2022 - Self-Supervised Models of Audio Effectively Explai.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Machine Learning","","","","","","","","","","","","","","",""
"RGA7BQWT","journalArticle","","Guillot, Calvin","Human-in-the-loop Hyperparameter Tuning of Deep Nets to Improve Explainability of Classifications","","","","","","Artificial Intelligence methods, especially the fields of deep-learning and other neuralnetwork based architectures have seen an increasing amount of development and deployment over the last decade. These architectures are especially suited to learning from large volumes of labelled data, and even though we know how they are constructed, they turn out to be equivalent to black boxes when it comes to understanding the basis upon which they produce predictions, especially as size of the network increases. Explainable AI (xAI) methods aim to disclose the key features and values that influence the prediction of black-box classifiers in a manner that is understandable to humans. In this project, the first steps are taken towards developing an interactive xAI system that places a human in the loop; here, a user’s ratings on the sensibility of explanations of individual classifications are used to iteratively find Hyperparameters of the neural net classifier (VGG-16), image segmentator (Felzenszwalb), and xAI (SHAP), to improve the sensibility of the explanations produced without affecting classification accuracy of the classifier in the training set. The users are asked to rate the sensibility of explanation from 1-10. The rating from the users is fed back to the Bayesian optimization algorithm that suggests new Hyperparameters values for the classifier, segmentator, and SHAP modules. The results of the user study suggests that the Hyperparameters which produced higher ratings on explanations tended to also improve the explainability of the images, thus generally improving the explainability for the image class. Improvement in the out-of-sample accuracy of the classifier (for the same class) was observed in some scenarios, but this still needs more comprehensive evaluation. More sensitive queries for the users, explore a variety of xAI methods, a variety of datasets, as well as conduct larger-scale experiments with users would be required to jointly improve explanations of multiple classes.","","2023-03-27 02:05:46","2023-03-31 01:53:24","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\V7ND9K69\Guillot - Human-in-the-loop Hyperparameter Tuning of Deep Ne.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NKXY3PZX","journalArticle","","Srihari, Sargur N","Machine Learning: Generative and Discriminative Models","Machine Learning","","","","","","","2023-03-30 01:02:43","2023-03-31 01:53:03","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\FSRS3LSS\Srihari - Machine Learning Generative and Discriminative Mo.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JFBHW8XY","journalArticle","2021","Vilone, Giulia; Longo, Luca","Classification of Explainable Artificial Intelligence Methods through Their Output Formats","Machine Learning and Knowledge Extraction","","","10.3390/make3030032","","Machine and deep learning have proven their utility to generate data-driven models with high accuracy and precision. However, their non-linear, complex structures are often difficult to interpret. Consequently, many scholars have developed a plethora of methods to explain their functioning and the logic of their inferences. This systematic review aimed to organise these methods into a hierarchical classification system that builds upon and extends existing taxonomies by adding a significant dimension—the output formats. The reviewed scientific papers were retrieved by conducting an initial search on Google Scholar with the keywords “explainable artificial intelligence”; “explainable machine learning”; and “interpretable machine learning”. A subsequent iterative search was carried out by checking the bibliography of these articles. The addition of the dimension of the explanation format makes the proposed classification system a practical tool for scholars, supporting them to select the most suitable type of explanation format for the problem at hand. Given the wide variety of challenges faced by researchers, the existing XAI methods provide several solutions to meet the requirements that differ considerably between the users, problems and application fields of artificial intelligence (AI). The task of identifying the most appropriate explanation can be daunting, thus the need for a classification system that helps with the selection of methods. This work concludes by critically identifying the limitations of the formats of explanations and by providing recommendations and possible future research directions on how to build a more generally applicable XAI method. Future work should be flexible enough to meet the many requirements posed by the widespread use of AI in several fields, and the new regulations.","2021-08-04","2023-03-30 01:06:07","2023-03-30 01:06:09","","615","","","3","","Machine Learning and Knowledge Extraction","","","","","","","","","","","","","ResearchGate","","24 citations (Crossref) [2023-03-30]","","; C:\Users\ambreen.hanif\Zotero\storage\GGSIMW8H\Vilone_Longo_2021_Classification of Explainable Artificial Intelligence Methods through Their.pdf","https://www.researchgate.net/publication/353687759_Classification_of_Explainable_Artificial_Intelligence_Methods_through_Their_Output_Formats","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2DVY4WJ9","conferencePaper","2022","Yu, Jialin; Cristea, Alexandra I.; Harit, Anoushka; Sun, Zhongtian; Aduragba, Olanrewaju Tahir; Shi, Lei; Moubayed, Noura Al","INTERACTION: A Generative XAI Framework for Natural Language Inference Explanations","2022 International Joint Conference on Neural Networks (IJCNN)","978-1-72818-671-9","","10.1109/IJCNN55064.2022.9892336","https://ieeexplore.ieee.org/document/9892336/","XAI with natural language processing aims to produce human-readable explanations as evidence for AI decisionmaking, which addresses explainability and transparency. However, from an HCI perspective, the current approaches only focus on delivering a single explanation, which fails to account for the diversity of human thoughts and experiences in language. This paper thus addresses this gap, by proposing a generative XAI framework, INTERACTION (explaIn aNd predicT thEn queRy with contextuAl CondiTional varIational autO-eNcoder). Our novel framework presents explanation in two steps: (step one) Explanation and Label Prediction; and (step two) Diverse Evidence Generation. We conduct intensive experiments with the Transformer architecture on a benchmark dataset, e-SNLI [1]. Our method achieves competitive or better performance against state-of-the-art baseline models on explanation generation (up to 4.7% gain in BLEU) and prediction (up to 4.4% gain in accuracy) in step one; it can also generate multiple diverse explanations in step two.","2022-07-18","2023-03-30 01:04:53","2023-03-30 01:04:58","2023-03-30 01:04:53","1-8","","","","","","INTERACTION","","","","","IEEE","Padua, Italy","en","","","","","DOI.org (Crossref)","","","","C:\Users\ambreen.hanif\Zotero\storage\PGFZQJ4N\Yu et al. - 2022 - INTERACTION A Generative XAI Framework for Natura.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2022 International Joint Conference on Neural Networks (IJCNN)","","","","","","","","","","","","","","",""
"F8RRPKX4","webpage","2022","Sun, Jiao; Liao, Q. Vera; Muller, Michael; Agarwal, Mayank; Houde, Stephanie; Talamadupula, Kartik; Weisz, Justin D.","Investigating Explainability of Generative AI for Code through Scenario-based Design","arXiv.org","","","","https://arxiv.org/abs/2202.04903v1","What does it mean for a generative AI model to be explainable? The emergent discipline of explainable AI (XAI) has made great strides in helping people understand discriminative models. Less attention has been paid to generative models that produce artifacts, rather than decisions, as output. Meanwhile, generative AI (GenAI) technologies are maturing and being applied to application domains such as software engineering. Using scenario-based design and question-driven XAI design approaches, we explore users' explainability needs for GenAI in three software engineering use cases: natural language to code, code translation, and code auto-completion. We conducted 9 workshops with 43 software engineers in which real examples from state-of-the-art generative AI models were used to elicit users' explainability needs. Drawing from prior work, we also propose 4 types of XAI features for GenAI for code and gathered additional design ideas from participants. Our work explores explainability needs for GenAI for code and demonstrates how human-centered approaches can drive the technical development of XAI in novel domains.","2022-02-10","2023-03-30 01:03:40","2023-03-30 01:03:40","2023-03-30 01:03:40","","","","","","","","","","","","","","en","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\RW2NEB96\Sun et al_2022_Investigating Explainability of Generative AI for Code through Scenario-based.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TK2788JY","webpage","2021","Schwalbe, Gesina; Finzel, Bettina","A Comprehensive Taxonomy for Explainable Artificial Intelligence: A Systematic Survey of Surveys on Methods and Concepts","arXiv.org","","","","https://arxiv.org/abs/2105.07190v4","In the meantime, a wide variety of terminologies, motivations, approaches, and evaluation criteria have been developed within the research field of explainable artificial intelligence (XAI). With the amount of XAI methods vastly growing, a taxonomy of methods is needed by researchers as well as practitioners: To grasp the breadth of the topic, compare methods, and to select the right XAI method based on traits required by a specific use-case context. Many taxonomies for XAI methods of varying level of detail and depth can be found in the literature. While they often have a different focus, they also exhibit many points of overlap. This paper unifies these efforts and provides a complete taxonomy of XAI methods with respect to notions present in the current state of research. In a structured literature analysis and meta-study, we identified and reviewed more than 50 of the most cited and current surveys on XAI methods, metrics, and method traits. After summarizing them in a survey of surveys, we merge terminologies and concepts of the articles into a unified structured taxonomy. Single concepts therein are illustrated by more than 50 diverse selected example methods in total, which we categorize accordingly. The taxonomy may serve both beginners, researchers, and practitioners as a reference and wide-ranging overview of XAI method traits and aspects. Hence, it provides foundations for targeted, use-case-oriented, and context-sensitive future research.","2021-05-15","2023-03-30 00:39:46","2023-03-30 00:39:48","2023-03-30 00:39:46","","","","","","","A Comprehensive Taxonomy for Explainable Artificial Intelligence","","","","","","","en","","","","","","","DOI: 10.1007/s10618-022-00867-8","","C:\Users\ambreen.hanif\Zotero\storage\QM638882\Schwalbe_Finzel_2021_A Comprehensive Taxonomy for Explainable Artificial Intelligence.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D6N2APXM","preprint","2023","Longa, Antonio; Lachi, Veronica; Santin, Gabriele; Bianchini, Monica; Lepri, Bruno; Lio, Pietro; Scarselli, Franco; Passerini, Andrea","Graph Neural Networks for temporal graphs: State of the art, open challenges, and opportunities","","","","10.48550/arXiv.2302.01018","http://arxiv.org/abs/2302.01018","Graph Neural Networks (GNNs) have become the leading paradigm for learning on (static) graph-structured data. However, many real-world systems are dynamic in nature, since the graph and node/edge attributes change over time. In recent years, GNN-based models for temporal graphs have emerged as a promising area of research to extend the capabilities of GNNs. In this work, we provide the first comprehensive overview of the current state-of-the-art of temporal GNN, introducing a rigorous formalization of learning settings and tasks and a novel taxonomy categorizing existing approaches in terms of how the temporal aspect is represented and processed. We conclude the survey with a discussion of the most relevant open challenges for the field, from both research and application perspectives.","2023-02-03","2023-03-29 23:11:19","2023-03-29 23:11:27","2023-03-29 23:11:19","","","","","","","Graph Neural Networks for temporal graphs","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2302.01018 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\DHPWP549\2302.html; C:\Users\ambreen.hanif\Zotero\storage\T4UH3GIN\Longa et al_2023_Graph Neural Networks for temporal graphs.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2302.01018","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R73ETPEV","preprint","2022","He, Wenchong; Vu, Minh N.; Jiang, Zhe; Thai, My T.","An Explainer for Temporal Graph Neural Networks","","","","10.48550/arXiv.2209.00807","http://arxiv.org/abs/2209.00807","Temporal graph neural networks (TGNNs) have been widely used for modeling time-evolving graph-related tasks due to their ability to capture both graph topology dependency and non-linear temporal dynamic. The explanation of TGNNs is of vital importance for a transparent and trustworthy model. However, the complex topology structure and temporal dependency make explaining TGNN models very challenging. In this paper, we propose a novel explainer framework for TGNN models. Given a time series on a graph to be explained, the framework can identify dominant explanations in the form of a probabilistic graphical model in a time period. Case studies on the transportation domain demonstrate that the proposed approach can discover dynamic dependency structures in a road network for a time period.","2022-09-02","2023-03-29 23:10:55","2023-03-29 23:10:57","2023-03-29 23:10:55","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2209.00807 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\J9T2BWCR\2209.html; C:\Users\ambreen.hanif\Zotero\storage\4ND93W94\He et al_2022_An Explainer for Temporal Graph Neural Networks.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2209.00807","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5ZCYGF35","webpage","","","Graphemd (Chami et. al) - The AI Search Engine You Control | AI Chat & Apps","","","","","https://you.com/s/Graphemd%20(Chami%20et.%20al)","You.com is a search engine built on artificial intelligence that provides users with a customized search experience while keeping their data 100% private. Try it today.","","2023-03-28 23:35:03","2023-03-28 23:35:03","2023-03-28 23:35:03","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\Z3Y8XLGJ\search.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MQV9HK9Z","preprint","2022","Leventi-Peetz, A.-M.; Östreich, T.","Deep Learning Reproducibility and Explainable AI (XAI)","","","","","http://arxiv.org/abs/2202.11452","The nondeterminism of Deep Learning (DL) training algorithms and its inﬂuence on the explainability of neural network (NN) models are investigated in this work with the help of image classiﬁcation examples. To discuss the issue, two convolutional neural networks (CNN) have been trained and their results compared. The comparison serves the exploration of the feasibility of creating deterministic, robust DL models and deterministic explainable artiﬁcial intelligence (XAI) in practice. Successes and limitation of all here carried out eﬀorts are described in detail. The source code of the attained deterministic models has been listed in this work. Reproducibility is indexed as a development-phase-component of the Model Governance Framework, proposed by the EU within their excellence in AI approach. Furthermore, reproducibility is a requirement for establishing causality for the interpretation of model results and building of trust towards the overwhelming expansion of AI systems applications. Problems that have to be solved on the way to reproducibility and ways to deal with some of them, are examined in this work.","2022-03-02","2023-03-27 06:19:27","2023-03-27 06:19:29","2023-03-27 06:19:27","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2202.11452 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\SEYWB2XR\Leventi-Peetz and Östreich - 2022 - Deep Learning Reproducibility and Explainable AI (.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2202.11452","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6C9CMTYR","webpage","","","A Feature Selection Method Based on Shapley Value to False Alarm Reduction in ICUs A Genetic-Algorithm Approach - PubMed","","","","","https://pubmed.ncbi.nlm.nih.gov/30440402/","","","2023-03-24 06:08:18","2023-03-24 06:08:18","2023-03-24 06:08:18","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\6HZWIZMF\30440402.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JDD4Z2E6","journalArticle","2020","Chu, Carlin Chun Fai; Chan, David Po Kin","Feature Selection Using Approximated High-Order Interaction Components of the Shapley Value for Boosted Tree Classifier","IEEE Access","","2169-3536","10.1109/ACCESS.2020.3002665","","The Shapely value originates from the cooperative game theory and has been widely used in solving machine learning problems due to its high interpretability and consistency. The typical Shapley value evaluates the importance score of a feature as its average marginal contribution to a fully parameterized model under all possible feature combinations; as a result, its value includes the influences from both selected features and unselected ones. To better separate the corresponding sources, it is suggested to decompose the Shapley value into high-order interaction effect components such that the influences from each individual feature can be effectively untangled. A feature's contribution is decomposed into distinct interaction components, and each component corresponds to a joint contribution resulting from a particular feature combination. The feature ranking is therefore evaluated with respect to the selected feature subset and calculated as the total incremental contribution by summing up its corresponding decomposed interaction values. In this study, a computationally efficient model-dependent greedy search algorithm on the high-order interaction components is proposed to solve an optimal subset selection problem with hundreds of time-lagged interrelated input features. Our algorithm extends a recently developed low-order polynomial time method for calculating the interaction component values. The empirical analysis demonstrates that the proposed method always outperforms other methods that are based on the typical Shapley value, the gain or the split count criteria, in terms of in-sample representativeness and out-of-sample forecasting performance for handling a problem with hundreds of time-lagged input features.","2020","2023-03-24 06:08:02","2023-03-24 06:08:02","","112742-112750","","","8","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Access","","C:\Users\ambreen.hanif\Zotero\storage\8T9RD46J\Chu_Chan_2020_Feature Selection Using Approximated High-Order Interaction Components of the.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M62V8SLW","webpage","","","Feature Selection Using Approximated High-Order Interaction Components of the Shapley Value for Boosted Tree Classifier | IEEE Journals & Magazine | IEEE Xplore","","","","","https://ieeexplore-ieee-org.simsrad.net.ocs.mq.edu.au/document/9117109","","","2023-03-24 06:04:26","2023-03-24 06:04:50","2023-03-24 06:04:26","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\QQUC2RCQ\9117109.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2XYCJJGS","journalArticle","2021","Fryer, Daniel; Strümke, Inga; Nguyen, Hien","Shapley Values for Feature Selection: The Good, the Bad, and the Axioms","IEEE Access","","2169-3536","10.1109/ACCESS.2021.3119110","","The Shapley value has become popular in the Explainable AI (XAI) literature, thanks, to a large extent, to a solid theoretical foundation, including four “favourable and fair” axioms for attribution in transferable utility games. The Shapley value is probably the only solution concept satisfying these axioms. In this paper, we introduce the Shapley value and draw attention to its recent uses as a feature selection tool. We call into question this use of the Shapley value, using simple, abstract “toy” counterexamples to illustrate that the axioms may work against the goals of feature selection. From this, we develop a number of insights that are then investigated in concrete simulation settings, with a variety of Shapley value formulations, including SHapley Additive exPlanations (SHAP) and Shapley Additive Global importancE (SAGE). The aim is not to encourage any use of the Shapley value for feature selection, but we aim to clarify various limitations around their current use in the literature. In so doing, we hope to help demystify certain aspects of the Shapley value axioms that are viewed as “favourable”. In particular, we wish to highlight that the favourability of the axioms depends non-trivially on the way in which the Shapley value is appropriated in the XAI application.","2021","2023-03-24 06:02:30","2023-03-24 06:02:30","","144352-144360","","","9","","","Shapley Values for Feature Selection","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Access","","C:\Users\ambreen.hanif\Zotero\storage\BLBCV2R4\Fryer et al_2021_Shapley Values for Feature Selection.pdf; C:\Users\ambreen.hanif\Zotero\storage\FKDZPGRD\9565902.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GQHAV4J3","preprint","2017","Holzinger, Andreas; Biemann, Chris; Pattichis, Constantinos S.; Kell, Douglas B.","What do we need to build explainable AI systems for the medical domain?","","","","10.48550/arXiv.1712.09923","http://arxiv.org/abs/1712.09923","Artificial intelligence (AI) generally and machine learning (ML) specifically demonstrate impressive practical success in many different application domains, e.g. in autonomous driving, speech recognition, or recommender systems. Deep learning approaches, trained on extremely large data sets or using reinforcement learning methods have even exceeded human performance in visual tasks, particularly on playing games such as Atari, or mastering the game of Go. Even in the medical domain there are remarkable results. The central problem of such models is that they are regarded as black-box models and even if we understand the underlying mathematical principles, they lack an explicit declarative knowledge representation, hence have difficulty in generating the underlying explanatory structures. This calls for systems enabling to make decisions transparent, understandable and explainable. A huge motivation for our approach are rising legal and privacy aspects. The new European General Data Protection Regulation entering into force on May 25th 2018, will make black-box approaches difficult to use in business. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make the results re-traceable on demand. In this paper we outline some of our research topics in the context of the relatively new area of explainable-AI with a focus on the application in medicine, which is a very special domain. This is due to the fact that medical professionals are working mostly with distributed heterogeneous and complex sources of data. In this paper we concentrate on three sources: images, *omics data and text. We argue that research in explainable-AI would generally help to facilitate the implementation of AI/ML in the medical domain, and specifically help to facilitate transparency and trust.","2017-12-28","2023-03-24 05:40:02","2023-03-24 05:40:08","2023-03-24 05:40:02","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1712.09923 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\8ZLINF9H\1712.html; C:\Users\ambreen.hanif\Zotero\storage\5EH5TGCQ\Holzinger et al_2017_What do we need to build explainable AI systems for the medical domain.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1712.09923","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RC94YI6F","conferencePaper","2017","Lundberg, Scott M; Allen, Paul G; Lee, Su-In","A Unified Approach to Interpreting Model Predictions","Advances in Neural Information Processing Systems","","","","https://github.com/slundberg/shap","Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.","2017","2021-06-05 09:21:03","2023-03-24 05:19:29","2021-06-05","","","","30","","","","","","","","","Long Beach, CA","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\VW8UE5A6\Lundberg et al_2017_A Unified Approach to Interpreting Model Predictions.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","CoRR","","","","","","","","","","","","","","",""
"ID2XNEYA","journalArticle","2018","Liu, Mengchen; Shi, Jiaxin; Cao, Kelei; Zhu, Jun; Liu, Shixia","Analyzing the Training Processes of Deep Generative Models","IEEE Transactions on Visualization and Computer Graphics","","1941-0506","10.1109/TVCG.2017.2744938","","Among the many types of deep models, deep generative models (DGMs) provide a solution to the important problem of unsupervised and semi-supervised learning. However, training DGMs requires more skill, experience, and know-how because their training is more complex than other types of deep models such as convolutional neural networks (CNNs). We develop a visual analytics approach for better understanding and diagnosing the training process of a DGM. To help experts understand the overall training process, we first extract a large amount of time series data that represents training dynamics (e.g., activation changes over time). A blue-noise polyline sampling scheme is then introduced to select time series samples, which can both preserve outliers and reduce visual clutter. To further investigate the root cause of a failed training process, we propose a credit assignment algorithm that indicates how other neurons contribute to the output of the neuron causing the training failure. Two case studies are conducted with machine learning experts to demonstrate how our approach helps understand and diagnose the training processes of DGMs. We also show how our approach can be directly used to analyze other types of deep models, such as CNNs.","2018-01","2023-03-24 02:38:24","2023-03-24 02:38:28","","77-87","","1","24","","","","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Transactions on Visualization and Computer Graphics","","C:\Users\ambreen.hanif\Zotero\storage\BM925PGV\8019879.html; C:\Users\ambreen.hanif\Zotero\storage\88WN6EU6\Liu et al_2018_Analyzing the Training Processes of Deep Generative Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MGZ2SURR","preprint","2022","Li, Yiqiao; Zhou, Jianlong; Verma, Sunny; Chen, Fang","A Survey of Explainable Graph Neural Networks: Taxonomy and Evaluation Metrics","","","","","http://arxiv.org/abs/2207.12599","Graph neural networks (GNNs) have demonstrated a significant boost in prediction performance on graph data. At the same time, the predictions made by these models are often hard to interpret. In that regard, many efforts have been made to explain the prediction mechanisms of these models from perspectives such as GNNExplainer, XGNN and PGExplainer. Although such works present systematic frameworks to interpret GNNs, a holistic review for explainable GNNs is unavailable. In this survey, we present a comprehensive review of explainability techniques developed for GNNs. We focus on explainable graph neural networks and categorize them based on the use of explainable methods. We further provide the common performance metrics for GNNs explanations and point out several future research directions.","2022-07-25","2023-03-23 05:26:16","2023-03-23 05:26:18","2023-03-23 05:26:16","","","","","","","A Survey of Explainable Graph Neural Networks","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2207.12599 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\X97XDG7N\2207.html; C:\Users\ambreen.hanif\Zotero\storage\ZH7UDULN\Li et al_2022_A Survey of Explainable Graph Neural Networks.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2207.12599","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RQEN7Q9J","conferencePaper","2017","Howard, Ayanna; Zhang, Cha; Horvitz, Eric","Addressing bias in machine learning algorithms: A pilot study on emotion recognition for intelligent systems","2017 IEEE workshop on advanced robotics and its social impacts (ARSO)","","","10.1109/ARSO.2017.8025197","","","2017","2023-03-22 23:51:43","2023-03-23 04:31:24","","1-7","","","","","","","","","","","","","","","","","","","","Citation Key: 8025197","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YT2AMRTU","journalArticle","2018","Tavana, Madjid; Abtahi, Amir-Reza; Di Caprio, Debora; Poortarigh, Maryam","An Artificial Neural Network and Bayesian Network model for liquidity risk assessment in banking","Neurocomputing","","0925-2312","10.1016/j.neucom.2017.11.034","https://www.sciencedirect.com/science/article/pii/S0925231217317939","Liquidity risk represent a devastating financial threat to banks and may lead to irrecoverable consequences in case of underestimation or negligence. The optimal control of a phenomenon such as liquidity risk requires a precise measurement method. However, liquidity risk is complicated and providing a suitable definition for it constitutes a serious obstacle. In addition, the problem of defining the related determining factors and formulating an appropriate functional form to approximate and predict its value is a difficult and complex task. To deal with these issues, we propose a model that uses Artificial Neural Networks and Bayesian Networks. The implementation of these two intelligent systems comprises several algorithms and tests for validating the proposed model. A real-world case study is presented to demonstrate applicability and exhibit the efficiency, accuracy and flexibility of data mining methods when modeling ambiguous occurrences related to bank liquidity risk measurement.","2018-01-31","2021-02-10 01:08:08","2023-03-23 04:31:15","2021-02-10 01:07:05","2525-2554","","","275","","Neurocomputing","","","","","","","","en","","","","","ScienceDirect","","","","C:\Users\ambreen.hanif\Zotero\storage\DSWQEAJH\Tavana et al. - 2018 - An Artificial Neural Network and Bayesian Network .pdf; C:\Users\ambreen.hanif\Zotero\storage\B2X4Q5S7\Tavana et al. - 2018 - An Artificial Neural Network and Bayesian Network .html","","Financial Risk","Artificial Neural Network; Banking; Bayesian Network; Intelligent systems; Liquidity risk","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JF2SWX6H","webpage","","","OECD Principles on Artificial Intelligence - Organisation for Economic Co-operation and Development","","","","","https://www.oecd.org/going-digital/ai/principles/","","","2021-09-09 20:59:23","2023-03-23 04:31:09","2021-09-10","","","","","","","","","","","","","","","","","","","","","","","false","https://www.oecd.org/going-digital/ai/principles/","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D88H64BI","conferencePaper","2011","Cortez, Paulo; Embrechts, Mark J.","Opening black box data mining models using sensitivity analysis","2011 IEEE symposium on computational intelligence and data mining (CIDM)","","","10.1109/CIDM.2011.5949423","","","2011","2023-03-22 23:51:44","2023-03-23 04:31:01","","341-348","","","","","","","","","","","","","","","","","","","","Citation Key: 5949423","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"36REKX4X","conferencePaper","2017","Zhang, Yu; Sreedharan, Sarath; Kulkarni, Anagha; Chakraborti, Tathagata; Zhuo, Hankz Hankui; Kambhampati, Subbarao","Plan explicability and predictability for robot task planning","2017 IEEE international conference on robotics and automation (ICRA)","","","10.1109/ICRA.2017.7989155","","","2017","2023-03-22 23:51:44","2023-03-23 04:30:56","","1313-1320","","","","","","","","","","","","","","","","","","","","Citation Key: 7989155","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RB33QNK5","conferencePaper","2020","Beheshti, A.; Moraveji-Hashemi, V.; Yakhchi, S.; Motahari-Nezhad, H.R.; Ghafari, S.M.; Yang, J.","Personality2Vec: Enabling the analysis of behavioral disorders in social networks","","","","10.1145/3336191.3371865","","Enabling the analysis of behavioral disorders over time in social networks, can help in suicide prevention, (school) bullying detection and extremist/criminal activity prediction. In this paper, we present a novel data analytics pipeline to enable the analysis of patterns of behavioral disorders on social networks. We present a Social Behavior Graph (sbGraph) model, to enable the analysis of factors that are driving behavior disorders over time. We use the golden standards in personality, behavior and attitude to build a domain specific Knowledge Base (KB). We use this domain knowledge to design cognitive services to automatically contextualize the raw social data and to prepare them for behavioral analytics. Then we introduce a pattern-based word embedding technique, namely personality2vec, on each feature extracted to build the sbGraph. The goal is to use mathematical embedding from a space with a dimension per feature to a continuous vector space which can be mapped to classes of behavioral disorders (such as cyber-bullying and radicalization) in the domain specific KB. We implement an interactive dashboard to enable social network analysts to analyze and understand the patterns of behavioral disorders over time. We focus on a motivating scenario in Australian government’s office of the e-Safety commissioner, where the goal is to empowering all citizens to have safer, more positive experiences online. © 2020 Association for Computing Machinery.","2020","2021-02-10 02:56:43","2023-03-23 04:30:52","","825-828","","","","","","Personality2Vec","","","","","","","","","","Scopus","","Scopus","","","","C:\Users\ambreen.hanif\Zotero\storage\P2IZ2K7U\3336191.3371865.pdf; C:\Users\ambreen.hanif\Zotero\storage\U2GQQD9K\Beheshti et al. - 2020 - Personality2Vec Enabling the analysis of behavior.html; C:\Users\ambreen.hanif\Zotero\storage\4RWPPXMX\Beheshti et al. - 2020 - personality2Vec enabling the analysis of behavior.html","","","Deep learning; Behavioural analytics; Cognitive analytics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","WSDM 2020 - Proceedings of the 13th International Conference on Web Search and Data Mining","","","","","","","","","","","","","","",""
"XCBH2GCZ","webpage","","","ePlanning Spatial Viewer","","","","","https://www.planningportal.nsw.gov.au/spatialviewer/#/find-a-property/address","","","2022-04-07 05:53:24","2023-03-23 04:30:45","2022-04-07","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\3KIV5YAQ\spatialviewer.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QNKXUDPP","conferencePaper","2013","Andrzejak, Artur; Langner, Felix; Zabala, Silvestre","Interpretable models from distributed data via merging of decision trees","2013 IEEE symposium on computational intelligence and data mining (CIDM)","","","10.1109/CIDM.2013.6597210","","","2013","2023-03-22 23:51:43","2023-03-23 04:30:27","","1-9","","","","","","","","","","","","","","","","","","","","Citation Key: 6597210","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6PVSCUEV","conferencePaper","2013","Backhaus, Andreas; Seiffert, Udo","Quantitative Measurements of model interpretability for the analysis of spectral data","2013 IEEE symposium on computational intelligence and data mining (CIDM)","","","10.1109/CIDM.2013.6597212","","","2013","2023-03-22 23:51:44","2023-03-23 04:30:23","","18-25","","","","","","","","","","","","","","","","","","","","Citation Key: 6597212","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QZDPNRHB","report","","CSIRO; Meteorology, Australian Government Bureau of","State of The Climate 2020","","","","","www.esrl.noaa.gov/psd/","","","2022-04-06 01:01:15","2023-03-23 04:30:17","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3S8V9TBR","book","2020","Parsons, Melissa; Reeve, Ian; McGregor, James; Marshall, Graham; Stayner, Richard; McNeill, Judith; Hastings, Peter; Glavac, Sonya; Morley, Phil","THE AUSTRALIAN DISASTER RESILIENCE INDEX VOLUME I-STATE OF DISASTER RESILIENCE REPORT","","978-0-648-27561-9","","","","","2020","2022-04-06 01:01:14","2023-03-23 04:29:58","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9HMHQS5D","conferencePaper","2017","Kaptein, Frank; Broekens, Joost; Hindriks, Koen; Neerincx, Mark","The role of emotion in self-explanations by cognitive agents","2017 seventh international conference on affective computing and intelligent interaction workshops and demos (ACIIW)","","","10.1109/ACIIW.2017.8272595","","","2017","2023-03-22 23:51:44","2023-03-23 04:29:46","","88-93","","","","","","","","","","","","","","","","","","","","Citation Key: 8272595","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XYCC7WZ8","journalArticle","","Ghorbani, Amirata; Wexler Google Brain, James; Zou, James; Kim Google Brain, Been","Towards Automatic Concept-based Explanations","","","","","https://github.com/amiratag/ACE","Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions. Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for concept based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that ACE discovers concepts that are human-meaningful, coherent and important for the neural network's predictions.","","2022-01-16 13:31:56","2023-03-23 04:29:35","2022-01-17","","","","","","","","","","","","","","","","","","","","","QID: Q76472349","","C:\Users\ambreen.hanif\Zotero\storage\28KMI3KB\Ghorbani et al_Towards Automatic Concept-based Explanations.pdf; C:\Users\ambreen.hanif\Zotero\storage\KHBFHSDP\full-text.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FX4TBG2U","journalArticle","2017","Weller, Adrian","Transparency: Motivations and Challenges","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","","16113349","10.1007/978-3-030-28954-6_2","https://arxiv.org/abs/1708.01870v2","Transparency is often deemed critical to enable effective real-world deployment of intelligent systems. Yet the motivations for and benefits of different types of transparency can vary significantly depending on context, and objective measurement criteria are difficult to identify. We provide a brief survey, suggesting challenges and related concerns. We highlight and review settings where transparency may cause harm, discussing connections across privacy, multi-agent game theory, economics, fairness and trust.","2017-07-29","2021-08-27 02:31:36","2023-03-23 04:29:28","2021-08-27","23-40","","","11700 LNCS","","","","","","","","","","","","","","","","","31 citations (Crossref) [2022-12-20] arXiv: 1708.01870 Publisher: Springer Verlag QID: Q102633391","","C:\Users\ambreen.hanif\Zotero\storage\XEFM929Y\Weller_2017_Transparency.pdf","","","Transparency; Explainable; Interpretability; Social good","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FTVAMJD2","journalArticle","2017","Doshi-Velez, Finale; Kim, Been","Towards A Rigorous Science of Interpretable Machine Learning","arXiv preprint arXiv:1702.08608","","","","https://arxiv.org/abs/1702.08608v2","As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.","2017-02-28","2021-08-05 23:46:27","2023-03-23 04:27:53","2021-08-06","","","","","","","","","","","","","","","","","","","","","arXiv: 1702.08608 QID: Q50745352","","C:\Users\ambreen.hanif\Zotero\storage\YQF9EHA9\Doshi-Velez_Kim_2017_Towards A Rigorous Science of Interpretable Machine Learning.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RHF6X37M","webpage","","","Explainable AI: from black box to glass box | SpringerLink","","","","","https://link.springer.com/article/10.1007/s11747-019-00710-5","","","2023-03-23 04:20:17","2023-03-23 04:20:17","2023-03-23 04:20:17","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\DSRVF5AN\s11747-019-00710-5.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T8AJ4WDX","webpage","2022","Dai, Enyan; Zhao, Tianxiang; Zhu, Huaisheng; Xu, Junjie; Guo, Zhimeng; Liu, Hui; Tang, Jiliang; Wang, Suhang","A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability","arXiv.org","","","","https://arxiv.org/abs/2204.08570v1","Graph Neural Networks (GNNs) have made rapid developments in the recent years. Due to their great ability in modeling graph-structured data, GNNs are vastly used in various applications, including high-stakes scenarios such as financial analysis, traffic predictions, and drug discovery. Despite their great potential in benefiting humans in the real world, recent study shows that GNNs can leak private information, are vulnerable to adversarial attacks, can inherit and magnify societal bias from training data and lack interpretability, which have risk of causing unintentional harm to the users and society. For example, existing works demonstrate that attackers can fool the GNNs to give the outcome they desire with unnoticeable perturbation on training graph. GNNs trained on social networks may embed the discrimination in their decision process, strengthening the undesirable societal bias. Consequently, trustworthy GNNs in various aspects are emerging to prevent the harm from GNN models and increase the users' trust in GNNs. In this paper, we give a comprehensive survey of GNNs in the computational aspects of privacy, robustness, fairness, and explainability. For each aspect, we give the taxonomy of the related methods and formulate the general frameworks for the multiple categories of trustworthy GNNs. We also discuss the future research directions of each aspect and connections between these aspects to help achieve trustworthiness.","2022-04-18","2023-03-23 00:33:45","2023-03-23 00:33:45","2023-03-23 00:33:45","","","","","","","A Comprehensive Survey on Trustworthy Graph Neural Networks","","","","","","","en","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\VEBHID5H\Dai et al_2022_A Comprehensive Survey on Trustworthy Graph Neural Networks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5VV3HQ43","journalArticle","2020","Lecue, Freddy","On the role of knowledge graphs in explainable AI","Semantic Web","","1570-0844","10.3233/SW-190374","https://content.iospress.com/articles/semantic-web/sw190374","The current hype of Artificial Intelligence (AI) mostly refers to the success of machine learning and its sub-domain of deep learning. However, AI is also about other areas, such as Knowledge Representation and Reasoning, or Distributed AI, i.e., are","2020-01-01","2023-03-23 00:33:10","2023-03-23 00:33:10","2023-03-23 00:33:10","41-51","","1","11","","","","","","","","","","en","","","","","content.iospress.com","","Publisher: IOS Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A77Y6VHK","conferencePaper","2019","Pope, Phillip E.; Kolouri, Soheil; Rostami, Mohammad; Martin, Charles E.; Hoffmann, Heiko","Explainability Methods for Graph Convolutional Neural Networks","","","","","https://openaccess.thecvf.com/content_CVPR_2019/html/Pope_Explainability_Methods_for_Graph_Convolutional_Neural_Networks_CVPR_2019_paper.html","","2019","2023-03-23 00:32:49","2023-03-23 00:32:49","2023-03-23 00:32:49","10772-10781","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","C:\Users\ambreen.hanif\Zotero\storage\FHB9ZAUV\Pope et al_2019_Explainability Methods for Graph Convolutional Neural Networks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","","","","","","","","","","","","","","",""
"F6N2NR8L","conferencePaper","2021","Yuan, Hao; Yu, Haiyang; Wang, Jie; Li, Kang; Ji, Shuiwang","On Explainability of Graph Neural Networks via Subgraph Explorations","Proceedings of the 38th International Conference on Machine Learning","","","","https://proceedings.mlr.press/v139/yuan21c.html","We consider the problem of explaining the predictions of graph neural networks (GNNs), which otherwise are considered as black boxes. Existing methods invariably focus on explaining the importance of graph nodes or edges but ignore the substructures of graphs, which are more intuitive and human-intelligible. In this work, we propose a novel method, known as SubgraphX, to explain GNNs by identifying important subgraphs. Given a trained GNN model and an input graph, our SubgraphX explains its predictions by efficiently exploring different subgraphs with Monte Carlo tree search. To make the tree search more effective, we propose to use Shapley values as a measure of subgraph importance, which can also capture the interactions among different subgraphs. To expedite computations, we propose efficient approximation schemes to compute Shapley values for graph data. Our work represents the first attempt to explain GNNs via identifying subgraphs explicitly and directly. Experimental results show that our SubgraphX achieves significantly improved explanations, while keeping computations at a reasonable level.","2021-07-01","2023-03-23 00:29:47","2023-03-23 00:29:47","2023-03-23 00:29:47","12241-12252","","","","","","","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 2640-3498","","C:\Users\ambreen.hanif\Zotero\storage\HRL4W9C8\Yuan et al. - 2021 - On Explainability of Graph Neural Networks via Sub.pdf; C:\Users\ambreen.hanif\Zotero\storage\ZHWN4HWA\Yuan et al_2021_On Explainability of Graph Neural Networks via Subgraph Explorations.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Machine Learning","","","","","","","","","","","","","","",""
"CRVR5U8N","conferencePaper","2010","Harbers, Maaike; van den Bosch, Karel; Meyer, John-Jules","Design and evaluation of explainable BDI agents","2010 IEEE/WIC/ACM international conference on web intelligence and intelligent agent technology","","","10.1109/WI-IAT.2010.115","","","2010","2023-03-23 00:03:57","2023-03-23 00:03:57","","125-132","","","2","","","","","","","","","","","","","","","","","Citation Key: 5614190","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GR49BP6M","conferencePaper","2017","Bau, David; Zhou, Bolei; Khosla, Aditya; Oliva, Aude; Torralba, Antonio","Network dissection: Quantifying interpretability of deep visual representations","2017 IEEE conference on computer vision and pattern recognition (CVPR)","","","10.1109/CVPR.2017.354","","","2017","2023-03-23 00:03:56","2023-03-23 00:03:56","","3319-3327","","","","","","","","","","","","","","","","","","","","Citation Key: 8099837","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CUPFZ4KV","journalArticle","2006","Etchells, T.A.; Lisboa, P.J.G.","Orthogonal search-based rule extraction (OSRE) for trained neural networks: a practical and efficient approach","IEEE Transactions on Neural Networks","","","10.1109/TNN.2005.863472","","","2006","2023-03-23 00:03:56","2023-03-23 00:03:56","","374-384","","2","17","","","","","","","","","","","","","","","","","Citation Key: 1603623","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QF3VP99P","journalArticle","2007","Schetinin, Vitaly; Fieldsend, Jonathan E.; Partridge, Derek; Coats, Timothy J.; Krzanowski, Wojtek J.; Everson, Richard M.; Bailey, Trevor C.; Hernandez, Adolfo","Confident interpretation of bayesian decision tree ensembles for clinical applications","IEEE Transactions on Information Technology in Biomedicine","","","10.1109/TITB.2006.880553","","","2007","2023-03-23 00:03:56","2023-03-23 00:03:56","","312-319","","3","11","","","","","","","","","","","","","","","","","Citation Key: 4167900","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D7SZ7SX5","conferencePaper","2016","Zhou, Bolei; Khosla, Aditya; Lapedriza, Agata; Oliva, Aude; Torralba, Antonio","Learning deep features for discriminative localization","2016 IEEE conference on computer vision and pattern recognition (CVPR)","","","10.1109/CVPR.2016.319","","","2016","2023-03-23 00:03:56","2023-03-23 00:03:56","","2921-2929","","","","","","","","","","","","","","","","","","","","Citation Key: 7780688","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MSY3RCXL","conferencePaper","2017","Dong, Yinpeng; Su, Hang; Zhu, Jun; Zhang, Bo","Improving interpretability of deep neural networks with semantic information","2017 IEEE conference on computer vision and pattern recognition (CVPR)","","","10.1109/CVPR.2017.110","","","2017","2023-03-23 00:03:56","2023-03-23 00:03:56","","975-983","","","","","","","","","","","","","","","","","","","","Citation Key: 8099593","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y6LWRNPR","journalArticle","2017","Krening, Samantha; Harrison, Brent; Feigh, Karen M.; Isbell, Charles Lee; Riedl, Mark; Thomaz, Andrea","Learning from explanations using sentiment and advice in RL","IEEE Transactions on Cognitive and Developmental Systems","","","10.1109/TCDS.2016.2628365","","","2017","2023-03-23 00:03:56","2023-03-23 00:03:56","","44-55","","1","9","","","","","","","","","","","","","","","","","Citation Key: 7742965","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BNN6QAEH","conferencePaper","2017","Fong, Ruth C.; Vedaldi, Andrea","Interpretable explanations of black boxes by meaningful perturbation","2017 IEEE international conference on computer vision (ICCV)","","","10.1109/ICCV.2017.371","","","2017","2023-03-23 00:03:56","2023-03-23 00:03:56","","3449-3457","","","","","","","","","","","","","","","","","","","","Citation Key: 8237633","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VQ9D9NTT","journalArticle","2008","Robnik-Šikonja, Marko; Kononenko, Igor","Explaining classifications for individual instances","IEEE Transactions on Knowledge and Data Engineering","","","10.1109/TKDE.2007.190734","","","2008","2023-03-23 00:03:56","2023-03-23 00:03:56","","589-600","","5","20","","","","","","","","","","","","","","","","","Citation Key: 4407709","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XLYJTRS7","conferencePaper","2015","Mahendran, Aravindh; Vedaldi, Andrea","Understanding deep image representations by inverting them","2015 IEEE conference on computer vision and pattern recognition (CVPR)","","","10.1109/CVPR.2015.7299155","","","2015","2023-03-23 00:03:56","2023-03-23 00:03:56","","5188-5196","","","","","","","","","","","","","","","","","","","","Citation Key: 7299155","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B26XUG4K","conferencePaper","2010","Harbers, Maaike; van den Bosch, Karel; Meyer, John-Jules","Design and evaluation of explainable BDI agents","2010 IEEE/WIC/ACM international conference on web intelligence and intelligent agent technology","","","10.1109/WI-IAT.2010.115","","","2010","2023-03-22 23:51:44","2023-03-22 23:51:44","","125-132","","","2","","","","","","","","","","","","","","","","","Citation Key: 5614190","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8LW3IWHL","journalArticle","2007","Schetinin, Vitaly; Fieldsend, Jonathan E.; Partridge, Derek; Coats, Timothy J.; Krzanowski, Wojtek J.; Everson, Richard M.; Bailey, Trevor C.; Hernandez, Adolfo","Confident interpretation of bayesian decision tree ensembles for clinical applications","IEEE Transactions on Information Technology in Biomedicine","","","10.1109/TITB.2006.880553","","","2007","2023-03-22 23:51:44","2023-03-22 23:51:44","","312-319","","3","11","","","","","","","","","","","","","","","","","Citation Key: 4167900","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y9U63SX4","conferencePaper","2016","Bach, Sebastian; Binder, Alexander; Müller, Klaus-Robert; Samek, Wojciech","Controlling explanatory heatmap resolution and semantics via decomposition depth","2016 IEEE international conference on image processing (ICIP)","","","10.1109/ICIP.2016.7532763","","","2016","2023-03-22 23:51:44","2023-03-22 23:51:44","","2271-2275","","","","","","","","","","","","","","","","","","","","Citation Key: 7532763","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TDFGXL28","journalArticle","2008","Robnik-Šikonja, Marko; Kononenko, Igor","Explaining classifications for individual instances","IEEE Transactions on Knowledge and Data Engineering","","","10.1109/TKDE.2007.190734","","","2008","2023-03-22 23:51:43","2023-03-22 23:51:43","","589-600","","5","20","","","","","","","","","","","","","","","","","Citation Key: 4407709","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"23XVGZZ3","journalArticle","2017","Krening, Samantha; Harrison, Brent; Feigh, Karen M.; Isbell, Charles Lee; Riedl, Mark; Thomaz, Andrea","Learning from explanations using sentiment and advice in RL","IEEE Transactions on Cognitive and Developmental Systems","","","10.1109/TCDS.2016.2628365","","","2017","2023-03-22 23:51:43","2023-03-22 23:51:43","","44-55","","1","9","","","","","","","","","","","","","","","","","Citation Key: 7742965","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SXZ2NKTB","conferencePaper","2015","Tan, Shawn; Sim, Khe Chai; Gales, Mark","Improving the interpretability of deep neural networks with stimulated learning","2015 IEEE workshop on automatic speech recognition and understanding (ASRU)","","","10.1109/ASRU.2015.7404853","","","2015","2023-03-22 23:51:43","2023-03-22 23:51:43","","617-623","","","","","","","","","","","","","","","","","","","","Citation Key: 7404853","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3S3LZ8CK","webpage","","","OpenXAI","OpenXAI","","","","http://localhost:4000/","OpenXAI | OpenXAI is an open-source library for benchmarking explanations for structure data","","2023-03-22 00:21:02","2023-03-22 00:21:02","2023-03-22 00:21:02","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\FYQC69DY\leaderboard.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IR6L34QN","preprint","2022","Krishna, Satyapriya; Han, Tessa; Gu, Alex; Pombra, Javin; Jabbari, Shahin; Wu, Steven; Lakkaraju, Himabindu","The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective","","","","","http://arxiv.org/abs/2202.01602","As various post hoc explanation methods are increasingly being leveraged to explain complex models in high-stakes settings, it becomes critical to develop a deeper understanding of if and when the explanations output by these methods disagree with each other, and how such disagreements are resolved in practice. However, there is little to no research that provides answers to these critical questions. In this work, we introduce and study the disagreement problem in explainable machine learning. More specifically, we formalize the notion of disagreement between explanations, analyze how often such disagreements occur in practice, and how do practitioners resolve these disagreements. To this end, we first conduct interviews with data scientists to understand what constitutes disagreement between explanations generated by different methods for the same model prediction, and introduce a novel quantitative framework to formalize this understanding. We then leverage this framework to carry out a rigorous empirical analysis with four real-world datasets, six state-of-the-art post hoc explanation methods, and eight different predictive models, to measure the extent of disagreement between the explanations generated by various popular explanation methods. In addition, we carry out an online user study with data scientists to understand how they resolve the aforementioned disagreements. Our results indicate that state-of-the-art explanation methods often disagree in terms of the explanations they output. Our findings also underscore the importance of developing principled evaluation metrics that enable practitioners to effectively compare explanations.","2022-02-08","2023-03-22 00:05:21","2023-03-22 00:05:21","2023-03-22 00:05:21","","","","","","","The Disagreement Problem in Explainable Machine Learning","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2202.01602 [cs] version: 3","","C:\Users\ambreen.hanif\Zotero\storage\57PVQY54\2202.html; C:\Users\ambreen.hanif\Zotero\storage\G4K6JPCN\Krishna et al_2022_The Disagreement Problem in Explainable Machine Learning.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2202.01602","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RLQ2MME3","journalArticle","2014","Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua","Neural Machine Translation by Jointly Learning to Align and Translate","3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings","","","","https://arxiv.org/abs/1409.0473v7","Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.","2014-09-01","2021-08-26 01:38:40","2023-03-21 06:27:39","2021-08-26","","","","","","","","","","","","","","","","","","","","","arXiv: 1409.0473 Publisher: International Conference on Learning Representations, ICLR Citation Key: Bahdanau2014 QID: Q29996006 cites: a_holzinger_interactive_2019 cites: aenugu_perturbation-based_nodate","","C:\Users\ambreen.hanif\Zotero\storage\X6A475SF\Bahdanau et al_2014_Neural Machine Translation by Jointly Learning to Align and Translate.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IG47WP27","journalArticle","2022","Kaur, Davinder; Uslu, Suleyman; Rittichier, Kaley J.; Durresi, Arjan","Trustworthy Artificial Intelligence: A Review","ACM Computing Surveys","","0360-0300","10.1145/3491209","https://dl.acm.org/doi/10.1145/3491209","Artificial intelligence (AI) and algorithmic decision making are having a profound impact on our daily lives. These systems are vastly used in different high-stakes applications like healthcare, business, government, education, and justice, moving us toward a more algorithmic society. However, despite so many advantages of these systems, they sometimes directly or indirectly cause harm to the users and society. Therefore, it has become essential to make these systems safe, reliable, and trustworthy. Several requirements, such as fairness, explainability, accountability, reliability, and acceptance, have been proposed in this direction to make these systems trustworthy. This survey analyzes all of these different requirements through the lens of the literature. It provides an overview of different approaches that can help mitigate AI risks and increase trust and acceptance of the systems by utilizing the users and society. It also discusses existing strategies for validating and verifying these systems and the current standardization efforts for trustworthy AI. Finally, we present a holistic view of the recent advancements in trustworthy AI to help the interested researchers grasp the crucial facets of the topic efficiently and offer possible future research directions.","2022-01-18","2023-03-16 12:33:25","2023-03-16 12:33:25","2023-03-16 12:33:24","39:1–39:38","","2","55","","ACM Comput. Surv.","Trustworthy Artificial Intelligence","","","","","","","","","","","","ACM Digital Library","","","","C:\Users\ambreen.hanif\Zotero\storage\RJZSQDJ6\Kaur et al. - 2022 - Trustworthy Artificial Intelligence A Review.pdf","","","Artificial intelligence; explainable AI; machine learning; acceptance; accountability; black-box problem; explainability; fairness; privacy; trustworthy AI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LWFFLN9G","journalArticle","","Phuong, Mary; Hutter, Marcus","Formal Algorithms for Transformers","","","","","","This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (not results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.","","2023-02-13 01:30:07","2023-02-13 22:39:18","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\A32XQ979\Phuong and Hutter - Formal Algorithms for Transformers.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"23T2DMAM","preprint","2022","Krzyziński, Mateusz; Spytek, Mikołaj; Baniecki, Hubert; Biecek, Przemysław","SurvSHAP(t): Time-dependent explanations of machine learning survival models","","","","10.48550/arXiv.2208.11080","http://arxiv.org/abs/2208.11080","Machine and deep learning survival models demonstrate similar or even improved time-to-event prediction capabilities compared to classical statistical learning methods yet are too complex to be interpreted by humans. Several model-agnostic explanations are available to overcome this issue; however, none directly explain the survival function prediction. In this paper, we introduce SurvSHAP(t), the first time-dependent explanation that allows for interpreting survival black-box models. It is based on SHapley Additive exPlanations with solid theoretical foundations and a broad adoption among machine learning practitioners. The proposed methods aim to enhance precision diagnostics and support domain experts in making decisions. Experiments on synthetic and medical data confirm that SurvSHAP(t) can detect variables with a time-dependent effect, and its aggregation is a better determinant of the importance of variables for a prediction than SurvLIME. SurvSHAP(t) is model-agnostic and can be applied to all models with functional output. We provide an accessible implementation of time-dependent explanations in Python at http://github.com/MI2DataLab/survshap.","2022-09-07","2023-01-30 22:53:57","2023-01-30 22:53:57","2023-01-30 22:53:57","","","","","","","SurvSHAP(t)","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2208.11080 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\CJRGKNE9\2208.html; C:\Users\ambreen.hanif\Zotero\storage\H93NN3XK\Krzyziński et al_2022_SurvSHAP(t).pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2208.11080","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QLSUB56G","conferencePaper","2019","Teso, Stefano; Kersting, Kristian","Explanatory interactive machine learning","AIES 2019 - Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society","","","10.1145/3306618.3314293","https://doi.org/10.1145/3306618.3314293","Although interactive learning puts the user into the loop, the learner remains mostly a black box for the user. Understanding the reasons behind predictions and queries is important when assessing how the learner works and, in turn, trust. Consequently, we propose the novel framework of explanatory interactive learning where, in each step, the learner explains its query to the user, and the user interacts by both answering the query and correcting the explanation. We demonstrate that this can boost the predictive and explanatory powers of, and the trust into, the learned model, using text (e.g. SVMs) and image classification (e.g. neural networks) experiments as well as a user study.","2019-01-27","2021-10-26 06:02:30","2023-01-30 05:19:07","2021-10-26","239-245","","","","","","","","","","","ACM","Honolulu HI USA","","","","","","","","45 citations (Crossref) [2022-12-20] Publisher: Association for Computing Machinery, Inc","","C:\Users\ambreen.hanif\Zotero\storage\T2DB5IQZ\full-text.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","AIES '19: AAAI/ACM Conference on AI, Ethics, and Society","","","","","","","","","","","","","","",""
"7YDYS2CR","conferencePaper","2016","Ribeiro, Marco Tulio; Singh, Sameer; Guestrin, Carlos","""Why should i trust you?"" Explaining the predictions of any classifier","Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","978-1-4503-4232-2","","10.1145/2939672.2939778","http://dx.doi.org/10.1145/2939672.2939778","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.","2016-08-13","2021-06-05 23:07:19","2023-01-30 05:14:59","2021-06-06","1135-1144","","","13-17-August-2016","","","","KDD '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","","","3777 citations (Crossref) [2022-12-20] arXiv: 1602.04938","","C:\Users\ambreen.hanif\Zotero\storage\59WTRS8L\Ribeiro et al_2016_Why should i trust you.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V4B4CKGE","conferencePaper","2018","Holzinger, Andreas","From Machine Learning to Explainable AI","2018 world symposium on digital intelligence for systems and machines (DISA)","","","10.1109/DISA.2018.8490530","https://hci-kdd.org","The success of statistical machine learning (ML) methods made the field of Artificial Intelligence (AI) so popular again, after the last AI winter. Meanwhile deep learning approaches even exceed human performance in particular tasks. However, such approaches have some disadvantages besides of needing big quality data, much computational power and engineering effort; those approaches are becoming increasingly opaque, and even if we understand the underlying mathematical principles of such models they still lack explicit declarative knowledge. For example, words are mapped to high-dimensional vectors, making them unintelligible to humans. What we need in the future are context-adaptive procedures, i.e. systems that construct contextual explanatory models for classes of real-world phenomena. This is the goal of explainable AI, which is not a new field; rather, the problem of explainability is as old as AI itself. While rule-based approaches of early AI were com-prehensible ""glass-box"" approaches at least in narrow domains, their weakness was in dealing with uncertainties of the real world. Maybe one step further is in linking probabilistic learning methods with large knowledge representations (ontologies) and logical approaches, thus making results re-traceable, explainable and comprehensible on demand.","2018","2021-09-30 01:45:47","2023-01-30 05:03:58","2021-09-30","55-66","","","","","","","","","","","IEEE","Technical University of Košice, Slovakia","","","","","","","","115 citations (Crossref) [2022-12-20]","","C:\Users\ambreen.hanif\Zotero\storage\L2P22WJA\Holzinger_2018_From Machine Learning to Explainable AI.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LEJG54DE","journalArticle","","Ehsan, Upol; Riedl, Mark O","Human-centered Explainable AI: Towards a Reflective Sociotechnical Approach","","","","","","Explanations-a form of post-hoc interpretability-play an instrumental role in making systems accessible as AI continues to proliferate complex and sensitive sociotechnical systems. In this paper, we introduce Human-centered Explainable AI (HCXAI) as an approach that puts the human at the center of technology design. It develops a holis-tic understanding of ""who"" the human is by considering the interplay of values, interpersonal dynamics, and the socially situated nature of AI systems. In particular, we advocate for a reflective sociotechnical approach. We illustrate HCXAI through a case study of an explanation system for non-technical end-users that shows how technical advancements and the understanding of human factors co-evolve. Building on the case study, we lay out open research questions pertaining to further refining our understanding of ""who"" the human is and extending beyond 1-to-1 human-computer interactions. Finally, we propose that a reflective HCXAI paradigm-mediated through the perspective of Critical Technical Practice and supplemented with strategies from HCI, such as value-sensitive design and participatory design-not only helps us understand our intellectual blind spots, but it can also open up new design and research spaces.","","2021-11-25 02:21:18","2023-01-27 00:13:27","2021-11-25","","","","","","","","","","","","","","","","","","","","","17 citations (Crossref) [2022-12-20] arXiv: 2002.01092v2 QID: Q102636987","","C:\Users\ambreen.hanif\Zotero\storage\FMAT39L5\Ehsan_Riedl_Human-centered Explainable AI.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F5U9MKWD","journalArticle","","","EvidenceCap: Towards trustworthy medical image segmentation via evidential identity cap. (arXiv:2301.00349v1 [eess.IV])","arXiv Computer Science","","","arXiv:2301.00349v1","https://arxiv.org/abs/2301.00349?utm_source=researcher_app&utm_medium=referral&utm_campaign=RESR_MRKT_Researcher_inbound","Medical image segmentation (MIS) is essential for supporting disease diagnosis and treatment effect assessment. Despite considerable advances in artificial intelligence (AI) for MIS, clinicians remain skeptical of its utility, maintaining low confidence in such black box systems, with this problem being exacerbated by low generalization for out-of-distribution (OOD) data. To move towards effective clinical utilization, we propose a foundation model named EvidenceCap, which makes the box transparent in a quantifiable way by uncertainty estimation. EvidenceCap not only makes AI visible in regions of uncertainty and OOD data, but also enhances the reliability, robustness, and computational efficiency of MIS. Uncertainty is modeled explicitly through subjective logic theory to gather strong evidence from features. We show the effectiveness of EvidenceCap in three segmentation datasets and apply it to the clinic. Our work sheds light on clinical safe applications and explainable AI, and can contribute towards trustworthiness in the medical domain.","","2023-01-03 22:30:52","2023-01-03 22:31:22","","","","","","","","","","","","","","","","","","","","","","","","","","Researcher App","⚠️ Invalid DOI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XTM7H9SI","journalArticle","","","Data Provenance and Reproducibility with Pachyderm","Data Skeptic","","","7770.28197.d2f07c45-87cd-4754-a200-ec7e990a0f11.1634800158","https://dataskeptic.com/blog/episodes/2017/data-provenance-and-reproducibility-with-pachyderm?utm_source=researcher_app&utm_medium=referral&utm_campaign=RESR_MRKT_Researcher_inbound","Versioning isn't just for source code. Being able to track changes to data is critical for answering questions about data provenance, quality, and reproducibility. Daniel Whitenack joins me this week to talk about these concepts and share his work on Pachyderm. Pachyderm is an open source containerized data lake.  During the show, Daniel mentioned the Gopher Data Science github repo as a great resource for any data scientists interested in the Go language. Although we didn't mention it, Daniel also did an interesting analysis on the 2016 world chess championship that complements our recent episode on chess well. You can find that post  here  Supplemental music is Lee Rosevere's Let's Start at the Beginning.     Thanks to Periscope Data for sponsoring this episode. More about them at periscopedata.com/skeptics","","2023-01-03 22:01:11","2023-01-03 22:01:41","","","","","","","","","","","","","","","","","","","","","","","","","","Researcher App","⚠️ Invalid DOI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S3RD3IMB","journalArticle","","","The World of Graph Databases from An Industry Perspective. (arXiv:2211.13170v1 [cs.DB])","arXiv Computer Science","","","arXiv:2211.13170v1","https://arxiv.org/abs/2211.13170?utm_source=researcher_app&utm_medium=referral&utm_campaign=RESR_MRKT_Researcher_inbound","Rapidly growing social networks and other graph data have created a high demand for graph technologies in the market. A plethora of graph databases, systems, and solutions have emerged, as a result. On the other hand, graph has long been a well studied area in the database research community. Despite the numerous surveys on various graph research topics, there is a lack of survey on graph technologies from an industry perspective. The purpose of this paper is to provide the research community with an industrial perspective on the graph database landscape, so that graph researcher can better understand the industry trend and the challenges that the industry is facing, and work on solutions to help address these problems.","","2023-01-03 05:56:32","2023-01-03 05:57:01","","","","","","","","","","","","","","","","","","","","","","","","","","Researcher App","⚠️ Invalid DOI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P9FTB378","journalArticle","","","Data Provenance","Foundations and Trends in Databases","","","10.1007/springerreference_64204","http://www.nowpublishers.com/article/Details/DBS-068?utm_source=researcher_app&utm_medium=referral&utm_campaign=RESR_MRKT_Researcher_inbound","Data provenance has evolved from a niche topic to a mainstream area of research in databases and other research communities. This article gives a comprehensive introduction to data provenance. The main focus is on provenance in the context of databases. However, it will be insightful to also consider connections to related research in programming languages, software engineering, semantic web, formal logic, and other communities. The target audience are researchers and practitioners that want to gain a solid understanding of data provenance and the state-of-the-art in this research area. The article only assumes that the reader has a basic understanding of database concepts, but not necessarily any prior exposure to provenance.<h3>Suggested Citation</h3>Boris Glavic (2021), ""Data Provenance"", Foundations and Trends® in Databases: Vol. 9: No. 3-4, pp 209-441. http://dx.doi.org/10.1561/1900000068","","2023-01-03 05:56:02","2023-01-03 05:56:02","","","","","","","","","","","","","","","","","","","","","","","","","","Researcher App","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U6ARAFWX","journalArticle","","","Connecting Algorithmic Research and Usage Contexts: A Perspective of Contextualized Evaluation for Explainable AI. (arXiv:2206.10847v2 [cs.AI] UPDATED)","arXiv Computer Science","","","arXiv:2206.10847v2","https://arxiv.org/abs/2206.10847?utm_source=researcher_app&utm_medium=referral&utm_campaign=RESR_MRKT_Researcher_inbound","Recent years have seen a surge of interest in the field of explainable AI (XAI), with a plethora of algorithms proposed in the literature. However, a lack of consensus on how to evaluate XAI hinders the advancement of the field. We highlight that XAI is not a monolithic set of technologies -- researchers and practitioners have begun to leverage XAI algorithms to build XAI systems that serve different usage contexts, such as model debugging and decision-support. Algorithmic research of XAI, however, often does not account for these diverse downstream usage contexts, resulting in limited effectiveness or even unintended consequences for actual users, as well as difficulties for practitioners to make technical choices. We argue that one way to close the gap is to develop evaluation methods that account for different user requirements in these usage contexts. Towards this goal, we introduce a perspective of contextualized XAI evaluation by considering the relative importance of XAI evaluation criteria for prototypical usage contexts of XAI. To explore the context dependency of XAI evaluation criteria, we conduct two survey studies, one with XAI topical experts and another with crowd workers. Our results urge for responsible AI research with usage-informed evaluation practices, and provide a nuanced understanding of user requirements for XAI in different usage contexts.","","2023-01-03 05:55:31","2023-01-03 05:56:01","","","","","","","","","","","","","","","","","","","","","","","","","","Researcher App","⚠️ Invalid DOI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BWFPSJC8","journalArticle","","","Ethical artificial intelligence framework for a good AI society: principles, opportunities and perils","AI & Society","","","10.1007/s00146-022-01458-3","https://link.springer.com/article/10.1007/s00146-022-01458-3?utm_source=researcher_app&utm_medium=referral&utm_campaign=RESR_MRKT_Researcher_inbound","The justification and rationality of this paper is to present some fundamental principles, theories, and concepts that we believe moulds the nucleus of a good artificial intelligence (AI) society. The morally accepted significance and utilitarian concerns that stems from the inception and realisation of an AI's structural foundation are displayed in this study. This paper scrutinises the structural foundation, fundamentals, and cardinal righteous remonstrations, as well as the gaps in mechanisms towards novel prospects and perils in determining resilient fundamentals, accountability, and AI's convoluted and responsible implications. We outline a number of salient and practical benefits, in which to place moral norms within the mise en scène of AI, to delineate the rudimentary ethical dilemmas and decorous directions within the realms of AI.","","2023-01-03 05:51:07","2023-01-03 05:51:07","","","","","","","","","","","","","","","","","","","","","","","","","","Researcher App","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JTG8HRCL","journalArticle","2021","Moscato, Vincenzo; Picariello, Antonio; Sperlí, Giancarlo","A benchmark of machine learning approaches for credit score prediction","Expert Systems with Applications","","09574174","10.1016/j.eswa.2020.113986","https://doi.org/10.1016/j.eswa.2020.113986","Credit risk assessment plays a key role for correctly supporting financial institutes in defining their bank policies and commercial strategies. Over the last decade, the emerging of social lending platforms has disrupted traditional services for credit risk assessment. Through these platforms, lenders and borrowers can easily interact among them without any involvement of financial institutes. In particular, they support borrowers in the fundraising process, enabling the participation of any number and size of lenders. However, the lack of lenders’ experience and missing or uncertain information about borrower's credit history can increase risks in social lending platforms, requiring an accurate credit risk scoring. To overcome such issues, the credit risk assessment problem of financial operations is usually modeled as a binary problem on the basis of debt's repayment and proper machine learning techniques can be consequently exploited. In this paper, we propose a benchmarking study of some of the most used credit risk scoring models to predict if a loan will be repaid in a P2P platform. We deal with a class imbalance problem and leverage several classifiers among the most used in the literature, which are based on different sampling techniques. A real social lending platform (Lending Club) data-set, composed by 877,956 samples, has been used to perform the experimental analysis considering different evaluation metrics (i.e. AUC, Sensitivity, Specificity), also comparing the obtained outcomes with respect to the state-of-the-art approaches. Finally, the three best approaches have also been evaluated in terms of their explainability by means of different eXplainable Artificial Intelligence (XAI) tools.","2021","2021-03-17 02:53:30","2022-12-20 05:08:43","2021-03-17","113986","","","165","","","","","","","","","","","","","","","","","34 citations (Crossref) [2022-12-20]","","C:\Users\ambreen.hanif\Zotero\storage\TWDCUXVA\Moscato et al_2021_A benchmark of machine learning approaches for credit score prediction.pdf","","","Machine learning; Supervised learning; Benchmark; Credit score prediction; Explainable artificial intelligence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"63F5IUSF","journalArticle","2022","Gil, Maitê; Sylla, Cristina","A close look into the storytelling process: The procedural nature of interactive digital narratives as learning opportunity","Entertainment Computing","","1875-9521","10.1016/J.ENTCOM.2021.100466","","Differently from traditional narratives, which focus on the output, i.e. the oral or written text, interactive digital narratives provide a more holistic view of the storytelling process, considering as integral part of it the system, the user, the process and the output. In this framework, the procedural nature of IDN as a reactive and generative system becomes prominent. Such an approach is particularly interesting when considering educational applications of IDN and how they can support early literacy practices in pre-and primary school children. Here, we take a close look into the procedural nature of IDN, presenting observations and results from two pilot studies carried out with six to seven-years old children, arguing that interactive digital narratives can provide a window into (i) how the children plan their story, (ii) how, along the storytelling process, the children learn the rules and constraints provided by the IDN system, which they appropriate and incorporate in their storytelling to achieve a certain output, (iii) how the children empathize with the story characters, diving into the story world and (iv) how the system provides opportunities for mediating new knowledge in a meaningful way, which was visible e.g. in the way the children immediately appropriated and used the new conveyed vocabulary.","2022-03-01","2022-05-03 03:53:10","2022-12-20 05:08:42","2022-05-03","100466","","","41","","","","","","","","","","","","","","","","","2 citations (Crossref) [2022-12-20] Publisher: Elsevier","","","","","Children; Digital manipulatives; Interactive Digital Narratives; Interactive process; Learning; Storytelling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M5CVDHVK","journalArticle","2022","Gil, Maitê; Sylla, Cristina","A close look into the storytelling process: The procedural nature of interactive digital narratives as learning opportunity","Entertainment Computing","","1875-9521","10.1016/J.ENTCOM.2021.100466","","Differently from traditional narratives, which focus on the output, i.e. the oral or written text, interactive digital narratives provide a more holistic view of the storytelling process, considering as integral part of it the system, the user, the process and the output. In this framework, the procedural nature of IDN as a reactive and generative system becomes prominent. Such an approach is particularly interesting when considering educational applications of IDN and how they can support early literacy practices in pre-and primary school children. Here, we take a close look into the procedural nature of IDN, presenting observations and results from two pilot studies carried out with six to seven-years old children, arguing that interactive digital narratives can provide a window into (i) how the children plan their story, (ii) how, along the storytelling process, the children learn the rules and constraints provided by the IDN system, which they appropriate and incorporate in their storytelling to achieve a certain output, (iii) how the children empathize with the story characters, diving into the story world and (iv) how the system provides opportunities for mediating new knowledge in a meaningful way, which was visible e.g. in the way the children immediately appropriated and used the new conveyed vocabulary.","2022-03-01","2022-05-17 04:06:13","2022-12-20 05:08:41","2022-05-17","100466","","","41","","","","","","","","","","","","","","","","","2 citations (Crossref) [2022-12-20] Publisher: Elsevier","","","","","Children; Digital manipulatives; Interactive Digital Narratives; Interactive process; Learning; Storytelling","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PPJHUY84","conferencePaper","2014","Kalchbrenner, Nal; Grefenstette, Edward; Blunsom, Phil","A convolutional neural network for modelling sentences","52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference","978-1-937284-72-5","","10.3115/v1/p14-1062","https://arxiv.org/abs/1404.2188v1","The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline. © 2014 Association for Computational Linguistics.","2014-04-08","2021-06-05 23:15:56","2022-12-20 05:08:41","2021-06-06","655-665","","","1","","","","","","","","Association for Computational Linguistics (ACL)","","","","","","","","","1409 citations (Crossref) [2022-12-20] arXiv: 1404.2188","","C:\Users\ambreen.hanif\Zotero\storage\95VSUPFI\Kalchbrenner et al_2014_A convolutional neural network for modelling sentences.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WE6RXRT5","journalArticle","2004","Brabazon, Anthony; Keenan, Peter B.","A hybrid genetic model for the prediction of corporate failure","Computational Management Science","","1619-697X","10.1007/s10287-004-0017-6","","This study examines the potential of a neural network (NN) model, whose inputs and structure are automatically selected by means of a genetic algo-rithm (GA), for the prediction of corporate failure using information drawn from financial statements. The results of this model are compared with those of a linear discriminant analysis (LDA) model. Data from a matched sample of 178 publicly quoted, failed and non-failed, US firms, drawn from the period 1991 to 2000 is used to train and test the models. The best evolved neural network correctly classified 86.7 (76.6)% of the firms in the training set, one (three) year(s) prior to failure, and 80.7 (66.0)% in the out-of-sample validation set. The LDA model correctly cate-gorised 81.7 (75.0)% and 76.0 (64.7)% respectively. The results provide support for a hypothesis that corporate failure can be anticipated, and that a hybrid GA/NN model can outperform an LDA model in this domain.","2004","2021-02-10 03:45:06","2022-12-20 05:08:40","","293-310","","3-4","1","","","","","","","","","","","","","","","","","30 citations (Crossref) [2022-12-20]","","C:\Users\ambreen.hanif\Zotero\storage\5UNCMYPW\Brabazon_Keenan_2004_A hybrid genetic model for the prediction of corporate failure.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VSHFU8B8","journalArticle","2008","Kim, Kyoung jae; Ahn, Hyunchul","A recommender system using GA K-means clustering in an online shopping market","Expert Systems with Applications","","09574174","10.1016/j.eswa.2006.12.025","","The Internet is emerging as a new marketing channel, so understanding the characteristics of online customers' needs and expectations is considered a prerequisite for activating the consumer-oriented electronic commerce market. In this study, we propose a novel clustering algorithm based on genetic algorithms (GAs) to effectively segment the online shopping market. In general, GAs are believed to be effective on NP-complete global optimization problems, and they can provide good near-optimal solutions in reasonable time. Thus, we believe that a clustering technique with GA can provide a way of finding the relevant clusters more effectively. The research in this paper applied K-means clustering whose initial seeds are optimized by GA, which is called GA K-means, to a real-world online shopping market segmentation case. In this study, we compared the results of GA K-means to those of a simple K-means algorithm and self-organizing maps (SOM). The results showed that GA K-means clustering may improve segmentation performance in comparison to other typical clustering algorithms. In addition, our study validated the usefulness of the proposed model as a preprocessing tool for recommendation systems. © 2007 Elsevier Ltd. All rights reserved.","2008-02","2021-03-15 17:57:35","2022-12-20 05:08:38","2021-03-16","1200-1209","","2","34","","","","","","","","","","","","","","","","","205 citations (Crossref) [2022-12-20]","","","","","Genetic algorithms; Case-based reasoning; Market segmentation; Recommender system; Self-organizing maps","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8KRWMJQ6","conferencePaper","2014","Khalid, Samina; Khalil, Tehmina; Nasreen, Shamila","A survey of feature selection and feature extraction techniques in machine learning","Proceedings of 2014 Science and Information Conference, SAI 2014","978-0-9893193-1-7","","10.1109/SAI.2014.6918213","","Dimensionality reduction as a preprocessing step to machine learning is effective in removing irrelevant and redundant data, increasing learning accuracy, and improving result comprehensibility. However, the recent increase of dimensionality of data poses a severe challenge to many existing feature selection and feature extraction methods with respect to efficiency and effectiveness. In the field of machine learning and pattern recognition, dimensionality reduction is important area, where many approaches have been proposed. In this paper, some widely used feature selection and feature extraction techniques have analyzed with the purpose of how effectively these techniques can be used to achieve high performance of learning algorithms that ultimately improves predictive accuracy of classifier. An endeavor to analyze dimensionality reduction techniques briefly with the purpose to investigate strengths and weaknesses of some widely used dimensionality reduction methods is presented.","2014-10-07","2021-03-26 15:04:07","2022-12-20 05:08:37","2021-03-27","372-378","","","","","","","","","","","Institute of Electrical and Electronics Engineers Inc.","","","","","","","","","348 citations (Crossref) [2022-12-20]","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QAQXJJF8","journalArticle","2019","Tjoa, Erico; Guan, Cuntai","A Survey on Explainable Artificial Intelligence (XAI): Towards Medical XAI","IEEE Transactions on Neural Networks and Learning Systems","","","10.1109/tnnls.2020.3027314","https://arxiv.org/abs/1907.07374v5","Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning. Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the deep learning is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide ""obviously"" interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that (1) clinicians and practitioners can subsequently approach these methods with caution, (2) insights into interpretability will be born with more considerations for medical practices, and (3) initiatives to push forward data-based, mathematically- and technically-grounded medical education are encouraged.","2019-07-17","2021-09-27 03:59:52","2022-12-20 05:08:36","2021-09-27","1-21","","8","14","","","","","","","","","","","","","","","","","260 citations (Crossref) [2022-12-20] arXiv: 1907.07374 Publisher: Institute of Electrical and Electronics Engineers (IEEE) QID: Q100739578","","C:\Users\ambreen.hanif\Zotero\storage\GVLNRVUZ\full-text.pdf","","","Index Terms-Explainable Artificial Intelligence; Interpretability; Ma-chine Learning; Medical Information System; Survey","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G6ZKJY3W","journalArticle","2020","Yujie, Yang","A Survey on Information Diffusion in Online Social Networks","ACM International Conference Proceeding Series","","","10.1145/3393822.3432322","","Nowadays, social networks have become a critical data dissemination platform with the drastic proliferation of social networks and the growing recipient of data. In this paper, recent successful works of information diffusion in online social networks are introduced. We summarize several classic information diffusion models like an explanatory model: the SI Model, the SIS Model, the SIRS Model; predictive Model: Independent Cascade Model, Linear Threshold Model. Then, we discuss some applications of those information diffusion model in different social networks.","2020","2021-02-17 22:27:20","2022-12-20 05:08:35","","181-186","","","8","","","","","","","","","","","","","","","","","1 citations (Crossref) [2022-12-20] ISBN: 9781450377621 QID: Q99625320","","C:\Users\ambreen.hanif\Zotero\storage\T3PMSPY9\ghafari_2018_TrustPrediction.pdf; C:\Users\ambreen.hanif\Zotero\storage\MWR9L7LI\Yujie_2020_A Survey on Information Diffusion in Online Social Networks.pdf","","","Cascade model; Disease model; Information diffusion; Online social networks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X64RMGZZ","journalArticle","2010","Namvar, Morteza; Gholamian, Mohammad R.; Khakabi, Sahand","A two phase clustering method for intelligent customer segmentation","ISMS 2010 - UKSim/AMSS 1st International Conference on Intelligent Systems, Modelling and Simulation","","","10.1109/ISMS.2010.48","","Customer Segmentation is an increasingly significant issue in today's competitive commercial area. Many literatures have reviewed the application of data mining technology in customer segmentation, and achieved sound effectives. But in the most cases, it is performed using customer data from a special point of view, rather than from systematical method considering all stages of CRM. This paper, with the aid of data mining tools, constructs a new customer segmentation method based on RFM, demographic and LTV data. The new customer segmentation method consists of two phases. Firstly, with K-means clustering, customers are clustered into different segments regarding their RFM. Secondly, using demographic data, each cluster again is partitioned into new clusters. Finally, using LTV, a profile for customer is created. The method has been applied to a dataset from Iranian bank, which resulted in some useful management measures and suggestions. © 2010 IEEE.","2010","2021-09-28 06:05:43","2022-12-20 05:08:33","2021-09-28","215-219","","","","","","","","","","","","","","","","","","","","33 citations (Crossref) [2022-12-20]","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CIIAYLSW","conferencePaper","2020","Panigutti, Cecilia; Perotti, Alan; Pedreschi, Dino; Pedreschi, Dino 2020","An ontology-based approach to black-box sequential data classification explanations","Proceedings of the 2020 conference on fairness, accountability, and transparency","978-1-4503-6936-7","","10.1145/3351095.3372855","https://doi.org/10.1145/3351095.3372855","Several recent advancements in Machine Learning involve black-box models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multi-label classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations. CCS CONCEPTS • Computing methodologies → Artificial intelligence; Machine learning; • Applied computing → Health care information systems. KEYWORDS explainable artificial intelligence, machine learning, healthcare data ACM Reference Format:","2020","2021-10-07 01:25:46","2022-12-20 05:08:29","2021-10-07","629-639","","","","","","","","","","","","","","","","","","","","45 citations (Crossref) [2022-12-20]","","C:\Users\ambreen.hanif\Zotero\storage\HFXVT4AP\full-text.pdf","","","machine learning; explainable artificial intelligence; healthcare data","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N9FQGSDU","journalArticle","2021","Vassiliades, Alexandros; Bassiliades, Nick; Patkos, Theodore","Argumentation and explainable artificial intelligence: a survey","The Knowledge Engineering Review","","0269-8889","10.1017/S0269888921000011","https://www.cambridge.org/core/journals/knowledge-engineering-review/article/abs/argumentation-and-explainable-artificial-intelligence-a-survey/DC6841ED8C7A80DC9EFADF87E4558A1F","Argumentation and eXplainable Artificial Intelligence (XAI) are closely related, as in the recent years, Argumentation has been used for providing Explainability to AI. Argumentation can show step by step how an AI System reaches a decision; it can provide reasoning over uncertainty and can find solutions when conflicting information is faced. In this survey, we elaborate over the topics of Argumentation and XAI combined, by reviewing all the important methods and studies, as well as implementations that use Argumentation to provide Explainability in AI. More specifically, we show how Argumentation can enable Explainability for solving various types of problems in decision-making, justification of an opinion, and dialogues. Subsequently, we elaborate on how Argumentation can help in constructing explainable systems in various applications domains, such as in Medical Informatics, Law, the Semantic Web, Security, Robotics, and some general purpose systems. Finally, we present approaches that combine Machine Learning and Argumentation Theory, toward more interpretable predictive models.","2021","2021-09-30 00:49:06","2022-12-20 05:08:28","2021-09-30","","","","36","","","","","","","","","","","","","","","","","20 citations (Crossref) [2022-12-20] Publisher: Cambridge University Press","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2LJBHGLJ","journalArticle","2018","Veloso de Melo, Vinícius; Banzhaf, Wolfgang","Automatic feature engineering for regression models with machine learning: An evolutionary computation and statistics hybrid","Information Sciences","","00200255","10.1016/j.ins.2017.11.041","","Symbolic Regression (SR) is a well-studied task in Evolutionary Computation (EC), where adequate free-form mathematical models must be automatically discovered from observed data. Statisticians, engineers, and general data scientists still prefer traditional regression methods over EC methods because of the solid mathematical foundations, the interpretability of the models, and the lack of randomness, even though such deterministic methods tend to provide lower quality prediction than stochastic EC methods. On the other hand, while EC solutions can be big and uninterpretable, they can be created with less bias, finding high-quality solutions that would be avoided by human researchers. Another interesting possibility is using EC methods to perform automatic feature engineering for a deterministic regression method instead of evolving a single model; this may lead to smaller solutions that can be easy to understand. In this contribution, we evaluate an approach called Kaizen Programming (KP) to develop a hybrid method employing EC and Statistics. While the EC method builds the features, the statistical method efficiently builds the models, which are also used to provide the importance of the features; thus, features are improved over the iterations resulting in better models. Here we examine a large set of benchmark SR problems known from the EC literature. Our experiments show that KP outperforms traditional Genetic Programming - a popular EC method for SR - and also shows improvements over other methods, including other hybrids and well-known statistical and Machine Learning (ML) ones. More in line with ML than EC approaches, KP is able to provide high-quality solutions while requiring only a small number of function evaluations.","2018-03-01","2021-04-21 23:31:17","2022-12-20 05:08:26","2021-04-22","287-313","","","430-431","","","","","","","","","","","","","","","","","18 citations (Crossref) [2022-12-20] Publisher: Elsevier Inc.","","C:\Users\ambreen.hanif\Zotero\storage\2YBGBPS7\Veloso de Melo_Banzhaf_2018_Automatic feature engineering for regression models with machine learning.pdf","","","Machine learning; Feature engineering; Genetic programming; Hybrid; Kaizen programming; Linear regression; Symbolic regression","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MVSKYTTW","journalArticle","2011","Chen, Mu Yen","Bankruptcy prediction in firms with statistical and intelligent techniques and a comparison of evolutionary computation approaches","Computers and Mathematics with Applications","","08981221","10.1016/j.camwa.2011.10.030","http://dx.doi.org/10.1016/j.camwa.2011.10.030","In this paper, we compare some traditional statistical methods for predicting financial distress to some more ""unconventional"" methods, such as decision tree classification, neural networks, and evolutionary computation techniques, using data collected from 200 Taiwan Stock Exchange Corporation (TSEC) listed companies. Empirical experiments were conducted using a total of 42 ratios including 33 financial, 8 non-financial and 1 combined macroeconomic index, using principle component analysis (PCA) to extract suitable variables. This paper makes four critical contributions: (1) with nearly 80% fewer financial ratios by the PCA method, the prediction performance is still able to provide highly-accurate forecasts of financial bankruptcy; (2) we show that traditional statistical methods are better able to handle large datasets without sacrificing prediction performance, while intelligent techniques achieve better performance with smaller datasets and would be adversely affected by huge datasets; (3) empirical results show that C5.0 and CART provide the best prediction performance for imminent bankruptcies; and (4) Support Vector Machines (SVMs) with evolutionary computation provide a good balance of high-accuracy short- and long-term performance predictions for healthy and distressed firms. Therefore, the experimental results show that the Particle Swarm Optimization (PSO) integrated with SVM (PSOSVM) approach could be considered for predicting potential financial distress. © 2011 Elsevier Ltd. All rights reserved.","2011","2021-02-10 03:45:06","2022-12-20 05:08:25","","4514-4524","","12","62","","","","","","","","","","","","","","","","","63 citations (Crossref) [2022-12-20] Publisher: Elsevier Ltd","","C:\Users\ambreen.hanif\Zotero\storage\IB4QQ6E6\Chen_2011_Bankruptcy prediction in firms with statistical and intelligent techniques and.pdf","","","Decision tree classification; Financial bankruptcy prediction; Support vector machine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JQ4F7VIU","journalArticle","2021","Yu, Lean; Zhang, Xiaoming","Can small sample dataset be used for efficient internet loan credit risk assessment? Evidence from online peer to peer lending","Finance Research Letters","","1544-6123","10.1016/J.FRL.2020.101521","","The emerging online peer to peer (P2P) lending platforms have only a small number of samples in the early stage, it is thus unable to conduct an efficient credit risk assessment on internet loan applicants. In order to solve the sample shortage issue, a virtual sample generation (VSG) methodology integrating multi-distribution mega-trend-diffusion (MD-MTD) and particle swarm optimization (PSO) algorithm is proposed for internet loan credit risk evaluation with small samples. The empirical results indicate that the proposed VSG methodology can greatly help to improve performance of the internet loan credit risk evaluation with small sample datasets.","2021-01-01","2021-10-26 00:23:23","2022-12-20 05:08:23","2021-10-26","101521","","","38","","","","","","","","","","","","","","","","","8 citations (Crossref) [2022-12-20] Publisher: Elsevier","","","","","Bootstrapping; Internet loan credit risk evaluation; mega-trend-diffusion; Particle swarm optimization; Peer to peer lending; Small sample; Virtual sample generation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QV32WTSJ","journalArticle","2022","Kiefer, Sebastian","CaSE: Explaining Text Classifications by Fusion of Local Surrogate Explanation Models with Contextual and Semantic Knowledge","Information Fusion","","15662535","10.1016/j.inffus.2021.07.014","","Generating explanations within a local and model-agnostic explanation scenario for text classification is often accompanied by a local approximation task. In order to create a local neighborhood for a document, whose classification shall be explained, sampling techniques are used that most often treat the according features at least semantically independent from each other. Hence, contextual as well as semantic information is lost and therefore cannot be used to update a human's mental model within the according explanation task. In case of dependent features, such explanation techniques are prone to extrapolation to feature areas with low data density, therefore causing misleading interpretations. Additionally, the ”the whole is greater than the sum of its parts” phenomenon is disregarded when using explanations that treat the according words independently from each other. In this paper, an architecture named CaSE is proposed that either uses Semantic Feature Arrangements or Semantic Interrogations to overcome these drawbacks. Combined with a modified version of Local interpretable model-agnostic explanations (LIME), a state of the art local explanation framework, it is capable of generating meaningful and coherent explanations. The approach utilizes contextual and semantic knowledge from unsupervised topic models in order to enable realistic and semantic sampling and based on that generate understandable explanations for any text classifier. The key concepts of CaSE that are deemed essential for providing humans with high quality explanations are derived from findings of psychology. In a nutshell, CaSE shall enable Semantic Alignment between humans and machines and thus further improve the basis for Interactive Machine Learning. An extensive experimental validation of CaSE is conducted, showing its effectiveness by generating reliable and meaningful explanations whose elements are made of contextually coherent words and therefore are suitable to update human mental models in an appropriate way. In the course of a quantitative analysis, the proposed architecture is evaluated w.r.t. a consistency property and to Local Fidelity of the resulting explanation models. According to that, CaSE generates more realistic explanation models leading to higher Local Fidelity compared to LIME.","2022-01-01","2021-10-26 06:26:49","2022-12-20 05:08:22","2021-10-26","184-195","","","77","","","","","","","","","","","","","","","","","6 citations (Crossref) [2022-12-20] Publisher: Elsevier Citation Key: Kiefer2022CaSE:Knowledge","","C:\Users\ambreen.hanif\Zotero\storage\5RAI32SD\Kiefer_2022_CaSE.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4I65LV6Q","journalArticle","2017","Mauša, Goran; Galinac Grbac, Tihana","Co-evolutionary multi-population genetic programming for classification in software defect prediction: An empirical case study","Applied Soft Computing Journal","","15684946","10.1016/j.asoc.2017.01.050","","Evolving diverse ensembles using genetic programming has recently been proposed for classification problems with unbalanced data. Population diversity is crucial for evolving effective algorithms. Multilevel selection strategies that involve additional colonization and migration operations have shown better performance in some applications. Therefore, in this paper, we are interested in analysing the performance of evolving diverse ensembles using genetic programming for software defect prediction with unbalanced data by using different selection strategies. We use colonization and migration operators along with three ensemble selection strategies for the multi-objective evolutionary algorithm. We compare the performance of the operators for software defect prediction datasets with varying levels of data imbalance. Moreover, to generalize the results, gain a broader view and understand the underlying effects, we replicated the same experiments on UCI datasets, which are often used in the evolutionary computing community. The use of multilevel selection strategies provides reliable results with relatively fast convergence speeds and outperforms the other evolutionary algorithms that are often used in this research area and investigated in this paper. This paper also presented a promising ensemble strategy based on a simple convex hull approach and at the same time it raised the question whether ensemble strategy based on the whole population should also be investigated.","2017-06-01","2021-03-10 23:22:09","2022-12-20 05:08:19","2021-03-11","331-351","","","55","","","","","","","","","","","","","","","","","36 citations (Crossref) [2022-12-20] Publisher: Elsevier Ltd","","","","","Classification; Genetic programming; Coevolution; Software defect prediction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RY5ZECE8","journalArticle","2010","Begel, Andrew; Phang, Khoo Yit; Zimmermann, Thomas","Codebook: Discovering and exploiting relationships in software repositories","Proceedings - International Conference on Software Engineering","","02705257","10.1145/1806799.1806821","","Large-scale software engineering requires communication and collaboration to successfully build and ship products. We conducted a survey with Microsoft engineers on inter-team coordination and found that the most impactful problems concerned finding and keeping track of other engineers. Since engineers are connected by their shared work, a tool that discovers connections in their work-related repositories can help. Here we describe the Codebook framework for mining software repositories. It is flexible enough to address all of the problems identified by our survey with a single data structure (graph of people and artifacts) and a single algorithm (regular language reachability). Codebook handles a larger variety of problems than prior work, analyzes more kinds of work artifacts, and can be customized by and for end-users. To evaluate our framework's flexibility, we built two applications, Hoozizat and Deep Intellisense. We evaluated these applications with engineers to show effectiveness in addressing multiple inter-team coordination problems. © 2010 ACM.","2010","2021-01-29 01:59:42","2022-12-20 05:08:18","","125-134","","","1","","","","","","","","","","","","","","","","","109 citations (Crossref) [2022-12-20] Citation Key: Begel2010 ISBN: 9781605587196","","C:\Users\ambreen.hanif\Zotero\storage\EFWKRUZ8\Begel et al_2010_Codebook.pdf","","","inter-team coordination; knowledge management; mining software repositories; regular expression; regular language reachability; social networking","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"55P4WW6W","journalArticle","","Lyons, Henrietta; Velloso, Eduardo","Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions; Conceptualising Contestability: Perspectives on Contesting Algorithmic Decisions","","","","10.1145/3449180","https://doi.org/10.1145/3449180","","","2022-05-17 04:19:51","2022-12-20 05:08:13","2022-05-17","","","","","","","","","","","","","","","","","","","","","9 citations (Crossref) [2022-12-20] arXiv: 2103.01774v1","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DRYIMTEF","journalArticle","2006","Kim, Su Yeon; Jung, Tae Soo; Suh, Eui Ho; Hwang, Hyun Seok","Customer segmentation and strategy development based on customer lifetime value: A case study","Expert Systems with Applications","","09574174","10.1016/j.eswa.2005.09.004","","The more a marketing paradigm evolves, the more long-term relationship with customers gains its importance. CRM, a recent marketing paradigm, pursues long-term relationship with profitable customers. It can be a starting point of relationship management to understand and measure the true value of customers since marketing management as a whole is to be deployed toward the targeted customers and profitable customers, to foster customers' full profit potential. Corporate success depends on an organization's ability to build and maintain loyal and valued customer relationships. Therefore, it is essential to build refined strategies for customers based on their value. In this paper, we propose a framework for analyzing customer value and segmenting customers based on their value. After segmenting customers based on their value, strategies building according to customer segment will be illustrated through a case study on a wireless telecommunication company. © 2005 Elsevier Ltd. All rights reserved.","2006-07","2021-03-15 17:57:35","2022-12-20 05:08:09","2021-03-16","101-107","","1","31","","","","","","","","","","","","","","","","","172 citations (Crossref) [2022-12-20]","","","","","Customer segmentation; Data mining; Customer lifetime value","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BJHKA3X8","journalArticle","2019","Beheshti, Amin; Benatallah, Boualem; Tabebordbar, Alireza; Motahari-Nezhad, Hamid Reza; Barukh, Moshe Chai; Nouri, Reza","DataSynapse: A Social Data Curation Foundry","Distributed and Parallel Databases","","15737578","10.1007/s10619-018-7245-1","https://doi.org/10.1007/s10619-018-7245-1","Social data analytics have become a vital asset for organizations and governments. For example, over the last few years, governments started to extract knowledge and derive insights from vastly growing open data to personalize the advertisements in elections, improve government services, predict intelligence activities, as well as to improve national security and public health. A key challenge in analyzing social data is to transform the raw data generated by social actors into curated data, i.e., contextualized data and knowledge that is maintained and made available for use by end-users and applications. To address this challenge, we present the notion of knowledge lake, i.e., a contextualized Data Lake, to provide the foundation for big data analytics by automatically curating the raw social data and to prepare them for deriving insights. We present a social data curation foundry, namely DataSynapse, to enable analysts engage with social data to uncover hidden patterns and generate insight. In DataSynapse, we present a scalable algorithm to transform social items (e.g., a Tweet in Twitter) into semantic items, i.e., contextualized and curated items. This algorithm offers customizable feature extraction to harness desired features from diverse data sources. To link contextualized information items to the domain knowledge, we present a scalable technique which leverages cross document coreference resolution assisting analysts to derive targeted insights. DataSynapse is offered as an extensible and scalable microservice-based architecture that are publicly available on GitHub supporting networks such as Twitter, Facebook, GooglePlus and LinkedIn. We adopt a typical scenario for analyzing urban social issues from Twitter as it relates to the government budget, to highlight how DataSynapse significantly improves the quality of extracted knowledge compared to the classical curation pipeline (in the absence of feature extraction, enrichment and domain-linking contextualization).","2019","2021-02-10 03:05:13","2022-12-20 05:08:08","","351-384","","3","37","","","","","","","","","","","","","","","","","23 citations (Crossref) [2022-12-20] Publisher: Springer US","","C:\Users\ambreen.hanif\Zotero\storage\PYUIG9BT\Beheshti et al_2019_DataSynapse.pdf","","","Feature engineering; Big data analytics; Data curation; Knowledge lake; Social networks analytics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YC8KAZ2B","journalArticle","2017","Lee, Eun Jae; Kim, Yong Hwan; Kim, Namkug; Kang, Dong Wha","Deep into the Brain: Artificial Intelligence in Stroke Imaging","Journal of Stroke","","22876405","10.5853/JOS.2017.02054","/pmc/articles/PMC5647643/","Artificial intelligence (AI), a computer system aiming to mimic human intelligence, is gaining increasing interest and is being incorporated into many fields, including medicine. Stroke medicine is one such area of application of AI, for improving the accuracy of diagnosis and the quality of patient care. For stroke management, adequate analysis of stroke imaging is crucial. Recently, AI techniques have been applied to decipher the data from stroke imaging and have demonstrated some promising results. In the very near future, such AI techniques may play a pivotal role in determining the therapeutic methods and predicting the prognosis for stroke patients in an individualized manner. In this review, we offer a glimpse at the use of AI in stroke imaging, specifically focusing on its technical principles, clinical application, and future perspectives.","2017-09-01","2022-06-06 02:01:36","2022-12-20 05:08:07","2022-06-06","277","","3","19","","","","","","","","","","","","","","","","","132 citations (Crossref) [2022-12-20] PMID: 29037014 Publisher: Korean Stroke Society QID: Q42380291","","; ; C:\Users\ambreen.hanif\Zotero\storage\HAJIWKW2\Lee et al_2017_Deep into the Brain.pdf","/pmc/articles/PMC5647643/?report=abstract; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5647643/","","Machine learning; Artificial intelligence; Stroke","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CC6AYL8B","journalArticle","2021","Zhao, Jiejie; Du, Bowen; Sun, Leilei; Lv, Weifeng; Liu, Yanchi; Xiong, Hui","Deep multi-task learning with relational attention for business success prediction","Pattern Recognition","","00313203","10.1016/j.patcog.2020.107469","https://doi.org/10.1016/j.patcog.2020.107469","Multi-task learning is a promising machine learning branch, which aims to improve the generalization of the prediction models by sharing knowledge among tasks. Most of the existing multi-task learning methods rely on predefined task relationships and guide the learning process of models by linear regularization terms. On the one hand, improper setting of task relationships may result in negative knowledge transfer; on the other hand, these methods also suffer from the insufficiency of representation ability. To overcome these problems, this paper focuses on attention-based deep multi-task learning method, and provides a novel deep multi-task learning method, namely, Deep Multi-task Learning with Relational Attention (DMLRA). In particular, we first provide a task-specific attention module to specify features for different learning tasks, because different prediction tasks may rely on different parts of the shared feature set. Then, we design a relational attention module to learn relationships among multiple tasks automatically, and transfer positive and negative knowledge among multiple tasks accordingly. Moreover, we provide a joint deep multi-task learning framework to combine task-specific module and relational attention module. Finally, we apply our method on a multi-criteria business success assessment problem, both classical and the state-of-the-art multi-task learning methods are employed to provide baseline performance. The experiments are conducted on real-world datasets, results demonstrate the superiority of our method over the existing methods.","2021","2021-01-29 01:59:42","2022-12-20 05:08:06","","107469","","xxxx","110","","","","","","","","","","","","","","","","","11 citations (Crossref) [2022-12-20] Publisher: Elsevier Ltd Citation Key: Zhao2021DeepPrediction","","C:\Users\ambreen.hanif\Zotero\storage\JQUV3AMY\Deep multi-task learning with relational attention for business suc.pdf","","","Attention; Multi-task learning; Site selection","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NR6ULVWD","journalArticle","2016","Zilke, Jan Ruben; Mencía, Eneldo Loza; Janssen, Frederik","DeepRED – Rule extraction from deep neural networks","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","","","10.1007/978-3-319-46307-0_29","","Neural network classifiers are known to be able to learn very accurate models. In the recent past, researchers have even been able to train neural networks with multiple hidden layers (deep neural networks) more effectively and efficiently. However, the major downside of neural networks is that it is not trivial to understand the way how they derive their classification decisions. To solve this problem, there has been research on extracting better understandable rules from neural networks. However, most authors focus on nets with only one single hidden layer. The present paper introduces a new decompositional algorithm – DeepRED – that is able to extract rules from deep neural networks. The evaluation of the proposed algorithm shows its ability to outperform a pedagogical baseline on several tasks, including the successful extraction of rules from a neural network realizing the XOR function.","2016","2021-08-27 21:43:33","2022-12-20 05:08:05","2021-08-28","457-473","","","9956 LNAI","","","","","","","","","","","","","","","","","72 citations (Crossref) [2022-12-20] Publisher: Springer Verlag","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TAREZ3SD","journalArticle","","Beheshti, Amin; Boualem Benatallah, ·; Alireza Tabebordbar, ·; Hamid, ·; Motahari-Nezhad, Reza; Chai Barukh, Moshe; Nouri, Reza; Amin Beheshti, B","Distributed and Parallel Databases DataSynapse: A Social Data Curation Foundry","Distributed and Parallel Databases","","","10.1007/s10619-018-7245-1","https://doi.org/10.1007/s10619-018-7245-1","Social data analytics have become a vital asset for organizations and governments. For example, over the last few years, governments started to extract knowledge and derive insights from vastly growing open data to personalize the advertisements in elections, improve government services, predict intelligence activities, as well as to improve national security and public health. A key challenge in analyzing social data is to transform the raw data generated by social actors into curated data, i.e., contextu-alized data and knowledge that is maintained and made available for use by end-users and applications. To address this challenge, we present the notion of knowledge lake, i.e., a contextualized Data Lake, to provide the foundation for big data analytics by automatically curating the raw social data and to prepare them for deriving insights. We present a social data curation foundry, namely DataSynapse, to enable analysts engage with social data to uncover hidden patterns and generate insight. In DataSy-napse, we present a scalable algorithm to transform social items (e.g., a Tweet in Twitter) into semantic items, i.e., contextualized and curated items. This algorithm offers customizable feature extraction to harness desired features from diverse data sources. To link contextualized information items to the domain knowledge, we present a scalable technique which leverages cross document coreference resolution assisting analysts to derive targeted insights. DataSynapse is offered as an extensible and scalable microservice-based architecture that are publicly available on GitHub supporting networks such as Twitter, Facebook, GooglePlus and LinkedIn. We adopt a typical scenario for analyzing urban social issues from Twitter as it relates to the government budget, to highlight how DataSynapse significantly improves the quality of extracted knowledge compared to the classical curation pipeline (in the absence of feature extraction, enrichment and domain-linking contextualization).","","2022-05-17 04:22:25","2022-12-20 05:08:04","2022-05-17","","","","","","","","","","","","","","","","","","","","","23 citations (Crossref) [2022-12-20]","","C:\Users\ambreen.hanif\Zotero\storage\JAXGFBNR\Beheshti et al_Distributed and Parallel Databases DataSynapse.pdf","","","Feature engineering; Big data analytics; Data curation; Knowledge lake; Social networks analytics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V28GMCNC","journalArticle","","Beheshti, Amin; Boualem Benatallah, ·; Alireza Tabebordbar, ·; Hamid, ·; Motahari-Nezhad, Reza; Chai Barukh, Moshe; Nouri, Reza; Amin Beheshti, B","Distributed and Parallel Databases DataSynapse: A Social Data Curation Foundry","Distributed and Parallel Databases","","","10.1007/s10619-018-7245-1","https://doi.org/10.1007/s10619-018-7245-1","Social data analytics have become a vital asset for organizations and governments. For example, over the last few years, governments started to extract knowledge and derive insights from vastly growing open data to personalize the advertisements in elections, improve government services, predict intelligence activities, as well as to improve national security and public health. A key challenge in analyzing social data is to transform the raw data generated by social actors into curated data, i.e., contextu-alized data and knowledge that is maintained and made available for use by end-users and applications. To address this challenge, we present the notion of knowledge lake, i.e., a contextualized Data Lake, to provide the foundation for big data analytics by automatically curating the raw social data and to prepare them for deriving insights. We present a social data curation foundry, namely DataSynapse, to enable analysts engage with social data to uncover hidden patterns and generate insight. In DataSy-napse, we present a scalable algorithm to transform social items (e.g., a Tweet in Twitter) into semantic items, i.e., contextualized and curated items. This algorithm offers customizable feature extraction to harness desired features from diverse data sources. To link contextualized information items to the domain knowledge, we present a scalable technique which leverages cross document coreference resolution assisting analysts to derive targeted insights. DataSynapse is offered as an extensible and scalable microservice-based architecture that are publicly available on GitHub supporting networks such as Twitter, Facebook, GooglePlus and LinkedIn. We adopt a typical scenario for analyzing urban social issues from Twitter as it relates to the government budget, to highlight how DataSynapse significantly improves the quality of extracted knowledge compared to the classical curation pipeline (in the absence of feature extraction, enrichment and domain-linking contextualization).","","2022-06-21 01:23:24","2022-12-20 05:08:03","2022-06-21","","","","","","","","","","","","","","","","","","","","","23 citations (Crossref) [2022-12-20]","","","","","Feature engineering; Big data analytics; Data curation; Knowledge lake; Social networks analytics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q5E4MFMN","journalArticle","2019","J Wiens, S Saria, M Sendak, M Ghassemi, VX Liu, F Doshi-Velez, K Jung, K Heller, D Kale, M Saeed, PN Ossorio, S Thadaney-Israni, A Goldenberg","Do no harm: a roadmap for responsible machine learning for health care","Nat Med","","","10.1038/s41591-019-0548-6","","Interest in machine-learning applications within medicine has been growing, but few studies have progressed to deployment in patient care. We present a framework, context and ultimately guidelines for accelerating the translation of machine-learning-based interventions in health care. To be successful, translation will require a team of engaged stakeholders and a systematic process from beginning (problem formulation) to end (widespread deployment).","2019","2021-09-30 00:20:20","2022-12-20 05:08:02","2021-09-30","1337-1340","","9","25","","","","","","","","","","","","","","","","","268 citations (Crossref) [2022-12-20] Publisher: Nature Publishing Group QID: Q92710894","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EKVAXL7G","journalArticle","","Jonathon Phillips, P; Hahn, Carina A; Fontana, Peter C; Broniatowski, David A","Draft NISTIR 8312 - Four Principles of Explainable Artificial Intelligence","","","","10.6028/NIST.IR.8312-draft","https://doi.org/10.6028/NIST.IR.8312-draft","","","2021-11-25 02:34:23","2022-12-20 05:08:01","2021-11-25","","","","","","","","","","","","","","","","","","","","","26 citations (Crossref) [2022-12-20] QID: Q102634055","","C:\Users\ambreen.hanif\Zotero\storage\3NBZYHLQ\Jonathon Phillips et al_Draft NISTIR 8312 - Four Principles of Explainable Artificial Intelligence.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2N54R375","journalArticle","2020","Wilson, Campbell; Dalins, Janis; Rolan, Gregory","Effective, Explainable and Ethical: AI for Law Enforcement and Community Safety","2020 IEEE / ITU International Conference on Artificial Intelligence for Good, AI4G 2020","","","10.1109/AI4G50087.2020.9311021","","We describe the Artificial Intelligence for Law Enforcement and Community Safety (AiLECS) research laboratory, a collaboration between the Australian Federal Police and Monash University. The laboratory was initially motivated by work towards countering online child exploitation material. It now offers a platform for further research and development in AI that will benefit policing and mitigating threats to community wellbeing more broadly. We outline the work the laboratory has undertaken, results to date, and discuss our agenda for scaling up its work into the future.","2020-09-21","2021-09-27 04:26:30","2022-12-20 05:08:01","2021-09-27","186-191","","","","","","","","","","","","","","","","","","","","0 citations (Crossref) [2022-12-20] Publisher: Institute of Electrical and Electronics Engineers Inc.","","","","","artificial intelligence; law enforcement","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2I24NEXM","journalArticle","2017","Church, Kenneth Ward","Emerging Trends: Word2Vec","Natural Language Engineering","","14698110","10.1017/S1351324916000334","","My last column ended with some comments about Kuhn and word2vec. Word2vec has racked up plenty of citations because it satisifies both of Kuhn's conditions for emerging trends: (1) a few initial (promising, if not convincing) successes that motivate early adopters (students) to do more, as well as (2) leaving plenty of room for early adopters to contribute and benefit by doing so. The fact that Google has so much to say on 'How does word2vec work' makes it clear that the definitive answer to that question has yet to be written. It also helps citation counts to distribute code and data to make it that much easier for the next generation to take advantage of the opportunities (and cite your work in the process).","2017","2021-02-10 03:45:07","2022-12-20 05:08:00","","155-162","","1","23","","","","","","","","","","","","","","","","","191 citations (Crossref) [2022-12-20] QID: Q113506666","","C:\Users\ambreen.hanif\Zotero\storage\H35YB2NS\Church_2017_Emerging Trends.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HUTCC3Z6","journalArticle","2021","Yang, Yonghui; Wu, Le; Hong, Richang; Zhang, Kun; Wang, Meng","Enhanced Graph Learning for Collaborative Filtering via Mutual Information Maximization","SIGIR 2021 - Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval","","","10.1145/3404835.3462928","","Neural graph based Collaborative Filtering (CF) models learn user and item embeddings based on the user-item bipartite graph structure, and have achieved state-of-the-art recommendation performance. In the ubiquitous implicit feedback based CF, users' unobserved behaviors are treated as unlinked edges in the user-item bipartite graph. As users' unobserved behaviors are mixed with dislikes and unknown positive preferences, the fixed graph structure input is missing with potential positive preference links. In this paper, we study how to better learn enhanced graph structure for CF. We argue that node embedding learning and graph structure learning can mutually enhance each other in CF, as updated node embeddings are learned from previous graph structure, and vice versa ∼(i.e., newly updated graph structure are optimized based on current node embedding results). Some previous works provided approaches to refine the graph structure. However, most of these graph learning models relied on node features for modeling, which are not available in CF. Besides, nearly all optimization goals tried to compare the learned adaptive graph and the original graph from a local reconstruction perspective, whether the global properties of the adaptive graph structure are modeled in the learning process is still unknown. To this end, in this paper, we propose an enhanced graph learning network EGLN approach for CF via mutual information maximization. The key idea of EGLN is two folds: First, we let the enhanced graph learning module and the node embedding module iteratively learn from each other without any feature input. Second, we design a local-global consistency optimization function to capture the global properties in the enhanced graph learning process. Finally, extensive experimental results on three real-world datasets clearly show the effectiveness of our proposed model.","2021-07-11","2022-03-27 19:59:02","2022-12-20 05:07:59","2022-03-28","71-80","","","","","","","","","","","","","","","","","","","","13 citations (Crossref) [2022-12-20] Publisher: Association for Computing Machinery, Inc ISBN: 9781450380379","","","","","collaborative filtering; graph learning; mutual information maximization; recommendation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UST2M8U3","journalArticle","2009","Talbot, Justin; Lee, Bongshin; Kapoor, Ashish; Tan, Desney S.","EnsembleMatrix: Interactive visualization to support machine learning with multiple classifiers","Conference on Human Factors in Computing Systems - Proceedings","","","10.1145/1518701.1518895","","Machine learning is an increasingly used computational tool within human-computer interaction research. While most researchers currently utilize an iterative approach to refining classifier models and performance, we propose that ensemble classification techniques may be a viable and even preferable alternative. In ensemble learning, algorithms combine multiple classifiers to build one that is superior to its components. In this paper, we present EnsembleMatrix, an interactive visualization system that presents a graphical view of confusion matrices to help users understand relative merits of various classifiers. EnsembleMatrix allows users to directly interact with the visualizations in order to explore and build combination models. We evaluate the efficacy of the system and the approach in a user study. Results show that users are able to quickly combine multiple classifiers operating on multiple feature sets to produce an ensemble classifier with accuracy that approaches best-reported performance classifying images in the CalTech-101 dataset. Copyright 2009 ACM.","2009","2021-11-25 02:32:05","2022-12-20 05:07:58","2021-11-25","1283-1292","","","","","","","","","","","","","","","","","","","","125 citations (Crossref) [2022-12-20] ISBN: 9781605582474","","","","","Visualization; Caltech-101; Ensemble classifiers; Interactive machine learning; Object recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JSUKJ99D","journalArticle","2019","Fernandez, Alberto; Herrera, Francisco; Cordon, Oscar; Jose Del Jesus, Maria; Marcelloni, Francesco","Evolutionary fuzzy systems for explainable artificial intelligence: Why, when, what for, and where to?","IEEE Computational Intelligence Magazine","","15566048","10.1109/MCI.2018.2881645","","Evolutionary fuzzy systems are one of the greatest advances within the area of computational intelligence. They consist of evolutionary algorithms applied to the design of fuzzy systems. Thanks to this hybridization, superb abilities are provided to fuzzy modeling in many different data science scenarios. This contribution is intended to comprise a position paper developing a comprehensive analysis of the evolutionary fuzzy systems research field. To this end, the »4 W» questions are posed and addressed with the aim of understanding the current context of this topic and its significance. Specifically, it will be pointed out why evolutionary fuzzy systems are important from an explainable point of view, when they began, what they are used for, and where the attention of researchers should be directed to in the near future in this area. They must play an important role for the emerging area of eXplainable Artificial Intelligence (XAI) learning from data.","2019-02-01","2021-11-25 02:13:58","2022-12-20 05:07:57","2021-11-25","69-81","","1","14","","","","","","","","","","","","","","","","","111 citations (Crossref) [2022-12-20] Publisher: Institute of Electrical and Electronics Engineers Inc.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HFYBDU4U","journalArticle","2017","Khan, Basit Tanvir; Javed, Noman; Hanif, Ambreen; Raja, Muhammad Adil","Evolving technical trading strategies using genetic algorithms: A case about Pakistan stock exchange","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","","16113349","10.1007/978-3-319-68935-7_37","","Finding optimum trading strategies that maximize profit has been a human desire since the inception of the first stock market. Many techniques have been employed ever since to accomplish this goal without sacrificing much computational power and time. In this paper, Genetic Algorithms (GAs) are used to achieve the aforementioned objectives. The performances of trading strategies devised by the GA are compared with the performance of the infamous Buy and Hold (B&H) Strategy. The stocks on which the performances are compared belong to Pakistan Stock Exchange (PSX). The strategies generated by GA outperform the B&H strategies on these stocks.","2017","2021-01-29 00:57:21","2022-12-20 05:07:57","","335-344","","November","10585 LNCS","","","","","","","","","","","","","","","","","1 citations (Crossref) [2022-12-20] ISBN: 9783319689340","","C:\Users\ambreen.hanif\Zotero\storage\BCA7XW9U\Khan et al_2017_Evolving technical trading strategies using genetic algorithms.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"72JVD886","journalArticle","2021","Bauer, Kevin; von Zahn, Moritz; Hinz, Oliver","Expl(Ai)Ned: The Impact of Explainable Artificial Intelligence on Cognitive Processes","SSRN Electronic Journal","","","10.2139/SSRN.3872711","https://papers.ssrn.com/abstract=3872711","","2021-06-16","2021-09-28 05:54:05","2022-12-20 05:07:56","2021-09-28","","","","","","","","","","","","","","","","","","","","","3 citations (Crossref) [2022-12-20] Publisher: Elsevier BV","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CPXWZKJR","journalArticle","","Ambsdorf, Jakob; Munir, Alina; Wei, Yiyao; Degkwitz, Klaas; Harms, Matthias; Stannek, Susanne; Ahrens, Kyra; Becker, Dennis; Strahl, Erik; Weber, Tom; Wermter, Stefan","Explain yourself! Effects of Explanations in Human-Robot Interaction","","","","10.1109/RO-MAN53752.2022.9900558","https://cloud.google.com/text-to-speech","Recent developments in explainable artificial intelligence promise the potential to transform human-robot interaction: Explanations of robot decisions could affect user perceptions, justify their reliability, and increase trust. However, the effects on human perceptions of robots that explain their decisions have not been studied thoroughly. To analyze the effect of explainable robots, we conduct a study in which two simulated robots play a competitive board game. While one robot explains its moves, the other robot only announces them. Providing explanations for its actions was not sufficient to change the perceived competence, intelligence, likeability or safety ratings of the robot. However, the results show that the robot that explains its moves is perceived as more lively and human-like. This study demonstrates the need for and potential of explainable human-robot interaction and the wider assessment of its effects as a novel research direction.","","2022-07-04 23:37:25","2022-12-20 05:07:55","2022-07-05","","","","","","","","","","","","","","","","","","","","","0 citations (Crossref) [2022-12-20] arXiv: 2204.04501v2 ISBN: 2204.04501v2","","C:\Users\ambreen.hanif\Zotero\storage\23BE939X\Ambsdorf et al_Explain yourself.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EXBENHPH","journalArticle","2020","Amann, Julia; Blasimme, Alessandro; Vayena, Effy; Frey, Dietmar; Madai, Vince I.","Explainability for artificial intelligence in healthcare: a multidisciplinary perspective","BMC Medical Informatics and Decision Making 2020 20:1","","1472-6947","10.1186/S12911-020-01332-6","https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-01332-6","Explainability is one of the most heavily debated topics when it comes to the application of artificial intelligence (AI) in healthcare. Even though AI-driven systems have been shown to outperform humans in certain analytical tasks, the lack of explainability continues to spark criticism. Yet, explainability is not a purely technological issue, instead it invokes a host of medical, legal, ethical, and societal questions that require thorough exploration. This paper provides a comprehensive assessment of the role of explainability in medical AI and makes an ethical evaluation of what explainability means for the adoption of AI-driven tools into clinical practice. Taking AI-based clinical decision support systems as a case in point, we adopted a multidisciplinary approach to analyze the relevance of explainability for medical AI from the technological, legal, medical, and patient perspectives. Drawing on the findings of this conceptual analysis, we then conducted an ethical assessment using the “Principles of Biomedical Ethics” by Beauchamp and Childress (autonomy, beneficence, nonmaleficence, and justice) as an analytical framework to determine the need for explainability in medical AI. Each of the domains highlights a different set of core considerations and values that are relevant for understanding the role of explainability in clinical practice. From the technological point of view, explainability has to be considered both in terms how it can be achieved and what is beneficial from a development perspective. When looking at the legal perspective we identified informed consent, certification and approval as medical devices, and liability as core touchpoints for explainability. Both the medical and patient perspectives emphasize the importance of considering the interplay between human actors and medical AI. We conclude that omitting explainability in clinical decision support systems poses a threat to core ethical values in medicine and may have detrimental consequences for individual and public health. To ensure that medical AI lives up to its promises, there is a need to sensitize developers, healthcare professionals, and legislators to the challenges and limitations of opaque algorithms in medical AI and to foster multidisciplinary collaboration moving forward.","2020-11-30","2021-09-30 00:30:35","2022-12-20 05:07:54","2021-09-30","1-9","","1","20","","","","","","","","","","","","","","","","","167 citations (Crossref) [2022-12-20] Publisher: BioMed Central QID: Q103793967","","C:\Users\ambreen.hanif\Zotero\storage\H9Q22BSB\Amann et al_2020_Explainability for artificial intelligence in healthcare.pdf","","","Health Informatics; Information Systems and Communication Service; Management of Computing and Information Systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"52VTD4SH","journalArticle","2019","Howard, Daniel; Edwards, Mark A.","Explainable A.I.: The promise of genetic programming multi-run subtree encapsulation","Proceedings - International Conference on Machine Learning and Data Engineering, iCMLDE 2018","","","10.1109/ICMLDE.2018.00037","","Deep Learning and other Artificial Neural Network based solutions are rarely transparent, and white-box solutions are often called for. This paper explains how Multirun Subtree Encapsulation can provide equivalent white box solutions to facilitate Explainable Artificial Intelligence.","2019-01-15","2021-09-30 01:46:06","2022-12-20 05:07:53","2021-09-30","158-159","","","","","","","","","","","","","","","","","","","","7 citations (Crossref) [2022-12-20] Publisher: Institute of Electrical and Electronics Engineers Inc. QID: Q102362929","","","","","Deep Learning; Genetic Programming; Explainable Artificial Intelligence; A.I.; Artificial Neural Networks; Automatically Defined Functions; black box; Evolutionary Computation; expression simplification; modularization; Multirun Subtree Encapsulation; Software Evolution; subtree database; Subtree Encapsulation; white box","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FLJJWLV7","journalArticle","2020","Hughes, Rowan; Edmond, Cameron; Wells, Lindsay; Glencross, Mashhuda; Zhu, Liming; Bednarz, Tomasz","EXplainable AI (XAI): An introduction to the XAI landscape with practical examples","SIGGRAPH Asia 2020 Courses, SA 2020","","","10.1145/3415263.3419166","","","2020-11-17","2022-05-03 03:44:29","2022-12-20 05:07:52","2022-05-03","","","","","","","","","","","","","","","","","","","","","2 citations (Crossref) [2022-12-20] Publisher: Association for Computing Machinery, Inc ISBN: 9781450381123","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7DFL2CMN","journalArticle","","David, Daniel Ben; Resheff, Yehezkel S; Tron, Talia","Explainable AI and Adoption of Financial Algorithmic Advisors: an Experimental Study; Explainable AI and Adoption of Financial Algorithmic Advisors: an Experimental Study","","","","10.1145/3461702.3462565","https://doi.org/10.1145/3461702.3462565","We study whether receiving advice from either a human or algo-rithmic advisor, accompanied by five types of Local and Global explanation labelings, has an effect on the readiness to adopt, willingness to pay, and trust in a financial AI consultant. We compare the differences over time and in various key situations using a unique experimental framework where participants play a web-based game with real monetary consequences. We observed that accuracy-based explanations of the model in initial phases leads to higher adoption rates. When the performance of the model is immaculate, there is less importance associated with the kind of explanation for adoption. Using more elaborate feature-based or accuracy-based explanations helps substantially in reducing the adoption drop upon model failure. Furthermore, using an autopilot increases adoption significantly. Participants assigned to the AI-labeled advice with explanations were willing to pay more for the advice than the AI-labeled advice with a No-explanation alternative. These results add to the literature on the importance of XAI for algorithmic adoption and trust.","","2021-09-28 06:05:52","2022-12-20 05:07:52","2021-09-28","","","","","","","","","","","","","","","","","","","","","4 citations (Crossref) [2022-12-20] arXiv: 2101.02555v3 Publisher: Virtual Event ISBN: 9781450384735","","C:\Users\ambreen.hanif\Zotero\storage\CG3XQ5IW\David et al_Explainable AI and Adoption of Financial Algorithmic Advisors.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V4PYHX2S","journalArticle","","David, Daniel Ben; Resheff, Yehezkel S; Tron, Talia","Explainable AI and Adoption of Financial Algorithmic Advisors: an Experimental Study; Explainable AI and Adoption of Financial Algorithmic Advisors: an Experimental Study","","","","10.1145/3461702.3462565","https://doi.org/10.1145/3461702.3462565","We study whether receiving advice from either a human or algo-rithmic advisor, accompanied by five types of Local and Global explanation labelings, has an effect on the readiness to adopt, willingness to pay, and trust in a financial AI consultant. We compare the differences over time and in various key situations using a unique experimental framework where participants play a web-based game with real monetary consequences. We observed that accuracy-based explanations of the model in initial phases leads to higher adoption rates. When the performance of the model is immaculate, there is less importance associated with the kind of explanation for adoption. Using more elaborate feature-based or accuracy-based explanations helps substantially in reducing the adoption drop upon model failure. Furthermore, using an autopilot increases adoption significantly. Participants assigned to the AI-labeled advice with explanations were willing to pay more for the advice than the AI-labeled advice with a No-explanation alternative. These results add to the literature on the importance of XAI for algorithmic adoption and trust.","","2021-11-25 02:33:24","2022-12-20 05:07:51","2021-11-25","","","","","","","","","","","","","","","","","","","","","4 citations (Crossref) [2022-12-20] arXiv: 2101.02555v3 Publisher: Virtual Event ISBN: 9781450384735","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A4DV2ZUT","journalArticle","2018","Chimatapu, Ravikiran; Hagras, Hani; Starkey, Andrew; Owusu, Gilbert","Explainable AI and Fuzzy Logic Systems","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","","16113349","10.1007/978-3-030-04070-3_1","https://link.springer.com/chapter/10.1007/978-3-030-04070-3_1","The recent advances in computing power coupled with the rapid increases in the quantity of available data has led to a resurgence in the theory and applications of Artificial Intelligence (AI). However, the use of complex AI algorithms like Deep Learning, Random Forests, etc., could result in a lack of transparency to users which is termed as black/opaque box models. Thus, for AI to be trusted and widely used by governments and industries, there is a need for greater transparency through the creation of explainable AI (XAI) systems. In this paper, we introduce the concepts of XAI and give an overview of hybrid systems which employ fuzzy logic systems which can hold great promise for creating trusted and explainable AI systems.","2018-12-12","2021-11-25 02:12:55","2022-12-20 05:07:50","2021-11-25","3-20","","","11324 LNCS","","","","","","","","","","","","","","","","","7 citations (Crossref) [2022-12-20] Publisher: Springer, Cham ISBN: 9783030040697 QID: Q102634572","","","","","Explainable AI; XAI; Deep fuzzy systems; Fuzzy logic systems","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SGC5BSDW","journalArticle","2020","Demajo, Lara Marie; Vella, Vince; Dingli, Alexiei","Explainable AI for Interpretable Credit Scoring","","","","10.5121/csit.2020.101516","https://arxiv.org/abs/2012.03749v1","With the ever-growing achievements in Artificial Intelligence (AI) and the recent boosted enthusiasm in Financial Technology (FinTech), applications such as credit scoring have gained substantial academic interest. Credit scoring helps financial experts make better decisions regarding whether or not to accept a loan application, such that loans with a high probability of default are not accepted. Apart from the noisy and highly imbalanced data challenges faced by such credit scoring models, recent regulations such as the `right to explanation' introduced by the General Data Protection Regulation (GDPR) and the Equal Credit Opportunity Act (ECOA) have added the need for model interpretability to ensure that algorithmic decisions are understandable and coherent. An interesting concept that has been recently introduced is eXplainable AI (XAI), which focuses on making black-box models more interpretable. In this work, we present a credit scoring model that is both accurate and interpretable. For classification, state-of-the-art performance on the Home Equity Line of Credit (HELOC) and Lending Club (LC) Datasets is achieved using the Extreme Gradient Boosting (XGBoost) model. The model is then further enhanced with a 360-degree explanation framework, which provides different explanations (i.e. global, local feature-based and local instance-based) that are required by different people in different situations. Evaluation through the use of functionallygrounded, application-grounded and human-grounded analysis show that the explanations provided are simple, consistent as well as satisfy the six predetermined hypotheses testing for correctness, effectiveness, easy understanding, detail sufficiency and trustworthiness.","2020-12-03","2021-09-30 00:52:27","2022-12-20 05:07:48","2021-09-30","185-203","","","","","","","","","","","","","","","","","","","","6 citations (Crossref) [2022-12-20] arXiv: 2012.03749 Publisher: Academy and Industry Research Collaboration Center (AIRCC)","","C:\Users\ambreen.hanif\Zotero\storage\UN56R3ZT\Demajo et al_2020_Explainable AI for Interpretable Credit Scoring.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HZPVMXLZ","journalArticle","2020","Barredo Arrieta, Alejandro; Díaz-Rodríguez, Natalia; Ser, Javier Del; Bennetot, Adrien; Tabik, Siham; Barbado, Alberto; Garcia, Salvador; Gil-Lopez, Sergio; Molina, Daniel; Benjamins, Richard; Chatila, Raja; Herrera, Francisco","Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI","Information Fusion","","","10.1016/j.inffus.2019.12.012","","In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.","2020","2021-09-27 03:55:14","2022-12-20 05:07:45","2021-09-27","82-115","","","58","","","","","","","","","","","","","","","","","1755 citations (Crossref) [2022-12-20] arXiv: 1910.10045v2 QID: Q102636901","","C:\Users\ambreen.hanif\Zotero\storage\IL6FQGK5\Barredo Arrieta et al_2020_Explainable Artificial Intelligence (XAI).pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YGMYP8W5","journalArticle","2018","Došilović, Filip Karlo; Brcic, Mario","Explainable Artificial Intelligence: A Survey Scheduling collaborative projects View project AI Safety View project","","","","10.23919/MIPRO.2018.8400040","https://www.researchgate.net/publication/325398586","","2018","2021-09-28 05:31:14","2022-12-20 05:07:44","2021-09-28","","","","","","","","","","","","","","","","","","","","","279 citations (Crossref) [2022-12-20] QID: Q60326075","","C:\Users\ambreen.hanif\Zotero\storage\QSXZS69K\Došilović_Brcic_2018_Explainable Artificial Intelligence.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5MILKE9M","journalArticle","2021","Bussmann, Niklas; Giudici, Paolo; Marinelli, · Dimitri; Papenbrock, · Jochen","Explainable Machine Learning in Credit Risk Management","Computational Economics","","","10.1007/s10614-020-10042-0","https://doi.org/10.1007/s10614-020-10042-0","The paper proposes an explainable Artificial Intelligence model that can be used in credit risk management and, in particular, in measuring the risks that arise when credit is borrowed employing peer to peer lending platforms. The model applies correlation networks to Shapley values so that Artificial Intelligence predictions are grouped according to the similarity in the underlying explanations. The empirical analysis of 15,000 small and medium companies asking for credit reveals that both risky and not risky borrowers can be grouped according to a set of similar financial characteristics, which can be employed to explain their credit score and, therefore, to predict their future behaviour.","2021","2021-09-29 01:26:00","2022-12-20 05:07:43","2021-09-29","203-216","","","57","","","","","","","","","","","","","","","","","50 citations (Crossref) [2022-12-20] ISBN: 0123456789","","C:\Users\ambreen.hanif\Zotero\storage\YWZK8TEV\Bussmann et al_2021_Explainable Machine Learning in Credit Risk Management.pdf","","","Explainable AI; Credit risk management; Financial technologies; Similarity networks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QV7IF778","journalArticle","2020","Meske, Christian; Bunde, Enrico; Schneider, Johannes; Gersch, Martin","Explainable Artificial Intelligence: Objectives, Stakeholders, and Future Research Opportunities","https://doi.org/10.1080/10580530.2020.1849465","","","10.1080/10580530.2020.1849465","https://www.tandfonline.com/doi/abs/10.1080/10580530.2020.1849465","Artificial Intelligence (AI) has diffused into many areas of our private and professional life. In this research note, we describe exemplary risks of black-box AI, the consequent need for explainab...","2020","2021-09-28 06:03:10","2022-12-20 05:07:43","2021-09-28","","","","","","","","","","","","","","","","","","","","","47 citations (Crossref) [2022-12-20] Publisher: Taylor & Francis","","C:\Users\ambreen.hanif\Zotero\storage\JP7Y5JAJ\full-text.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NI8TJQPJ","journalArticle","2020","Zhang, Yongfeng; Chen, Xu","Explainable recommendation: A survey and new perspectives","Foundations and Trends in Information Retrieval","","15540677","10.1561/1500000066","","Explainable recommendation attempts to develop models that generate not only high-quality recommendations but also intuitive explanations. The explanations may either be post-hoc or directly come from an explainable model (also called interpretable or transparent model in some contexts). Explainable recommendation tries to address the problem of why: by providing explanations to users or system designers, it helps humans to understand why certain items are recommended by the algorithm, where the human can either be users or system designers. Explainable recommendation helps to improve the transparency, persuasiveness, effectiveness, trustworthiness, and satisfaction of recommendation systems. It also facilitates system designers for better system debugging. In recent years, a large number of explainable recommendation approaches - especially model-based methods - have been proposed and applied in real-world systems. In this survey, we provide a comprehensive review for the explainable recommendation research. We first highlight the position of explainable recommendation in recommender system research by categorizing recommendation problems into the 5W, i.e., what, when, who, where, and why. We then conduct a comprehensive survey of explainable recommendation on three perspectives: 1) We provide a chronological research timeline of explainable recommendation, including user study approaches in the early years and more recent model-based approaches. 2) We provide a two-dimensional taxonomy to classify existing explainable recommendation research: one dimension is the information source (or display style) of the explanations, and the other dimension is the algorithmic mechanism to generate explainable recommendations. 3) We summarize how explainable recommendation applies to different recommendation tasks, such as product recommendation, social recommendation, and POI recommendation. We also devote a section to discuss the explanation perspectives in broader IR and AI/ML research. We end the survey by discussing potential future directions to promote the explainable recommendation research area and beyond.","2020-03-11","2022-03-27 19:59:02","2022-12-20 05:07:42","2022-03-28","1-101","","1","14","","","","","","","","","","","","","","","","","125 citations (Crossref) [2022-12-20] arXiv: 1804.11192 Publisher: Now Publishers Inc QID: Q102634409","","C:\Users\ambreen.hanif\Zotero\storage\D4XQCBRV\Zhang_Chen_2020_Explainable recommendation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GLDWAT73","journalArticle","2019","Miller, Tim","Explanation in artificial intelligence: Insights from the social sciences","Artificial Intelligence","","0004-3702","10.1016/J.ARTINT.2018.07.007","","There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.","2019-02-01","2021-09-20 01:23:17","2022-12-20 05:07:39","2021-09-20","1-38","","","267","","","","","","","","","","","","","","","","","1126 citations (Crossref) [2022-12-20] Publisher: Elsevier QID: Q102363022","","","","","Explainable AI; Explainability; Explanation; Transparency; Interpretability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"48R46B23","journalArticle","2020","Atkinson, Katie; Bench-Capon, Trevor; Bollegala, Danushka","Explanation in AI and law: Past, present and future","Artificial Intelligence","","00043702","10.1016/j.artint.2020.103387","","Explanation has been a central feature of AI systems for legal reasoning since their inception. Recently, the topic of explanation of decisions has taken on a new urgency, throughout AI in general, with the increasing deployment of AI tools and the need for lay users to be able to place trust in the decisions that the support tools are recommending. This paper provides a comprehensive review of the variety of techniques for explanation that have been developed in AI and Law. We summarise the early contributions and how these have since developed. We describe a number of notable current methods for automated explanation of legal reasoning and we also highlight gaps that must be addressed by future systems to ensure that accurate, trustworthy, unbiased decision support can be provided to legal professionals. We believe that insights from AI and Law, where explanation has long been a concern, may provide useful pointers for future development of explainable AI.","2020-12-01","2021-09-28 05:33:45","2022-12-20 05:07:39","2021-09-28","103387","","","289","","","","","","","","","","","","","","","","","33 citations (Crossref) [2022-12-20] Publisher: Elsevier","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DY95M8DE","journalArticle","2022","Mitchell, Sonia Natalie; Lahiff, Andrew; Cummings, Nathan; Hollocombe, Jonathan; Boskamp, Bram; Field, Ryan; Reddyhoff, Dennis; Zarebski, Kristian; Wilson, Antony; Viola, Bruno; Burke, Martin; Archibald, Blair; Bessell, Paul; Blackwell, Richard; Boden, Lisa A; Brett, Alys; Brett, Sam; Dundas, Ruth; Enright, Jessica; Gonzalez-Beltran, Alejandra N; Harris, Claire; Hinder, Ian; Hughes, Christopher David; Knight, Martin; Mano, Vino; Mcmonagle, Ciaran; Mellor, Dominic; Mohr, Sibylle; Marion, Glenn; Matthews, Louise; Mckendrick, Iain J; Pooley, Christopher Mark; Porphyre, Thibaud; Reeves, Aaron; Townsend, Edward; Turner, Robert; Walton, Jeremy; Reeve, Richard","FAIR Data Pipeline: provenance-driven data management for traceable scientific workflows","","","","10.1098/rsta.2021.0300","","Modern epidemiological analyses to understand and combat the spread of disease depend critically on access to, and use of, data. Rapidly evolving data, such as data streams changing during a disease outbreak, are particularly challenging. Data management is further complicated by data being imprecisely identified when used. Public trust in policy decisions resulting from such analyses is easily damaged and is often low, with cynicism arising where claims of ""following the science"" are made without accompanying evidence. Tracing the provenance of such decisions back through open software to primary data would clarify this evidence, enhancing the transparency of the decision-making process. Here, we demonstrate a Findable, Accessible, Interoperable and Reusable (FAIR) data pipeline. Although developed during the COVID-19 pandemic, it allows easy annotation of any data as they are consumed by analyses, or conversely traces the provenance of scientific outputs back through the analytical or modelling source code to primary data. Such a tool provides a mechanism for the public, and fellow scientists, to better assess scientific evidence by inspecting its provenance, while allowing scientists to support policy-makers in openly justifying their decisions. We believe that such tools should be promoted for use across all areas of policy-facing research.","2022","2022-07-04 23:22:39","2022-12-20 05:07:35","2022-07-05","","","","","","","","","","","","","","","","","","","","","3 citations (Crossref) [2022-12-20] arXiv: 2110.07117v2 QID: Q113816225","","C:\Users\ambreen.hanif\Zotero\storage\SJ6RNN3D\Mitchell et al_2022_FAIR Data Pipeline.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SXQGE2HY","journalArticle","2020","Fu, Zuohui; Xian, Yikun; Gao, Ruoyuan; Zhao, Jieyu; Huang, Qiaoying; Ge, Yingqiang; Xu, Shuyuan; Geng, Shijie; Shah, Chirag; Zhang, Yongfeng; De Melo, Gerard","Fairness-Aware Explainable Recommendation over Knowledge Graphs","SIGIR 2020 - Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval","","","10.1145/3397271.3401051","","There has been growing attention on fairness considerations recently, especially in the context of intelligent decision making systems. For example, explainable recommendation systems may suffer from both explanation bias and performance disparity. We show that inactive users may be more susceptible to receiving unsatisfactory recommendations due to their insufficient training data, and that their recommendations may be biased by the training records of active users due to the nature of collaborative filtering, which leads to unfair treatment by the system. In this paper, we analyze different groups of users according to their level of activity, and find that bias exists in recommendation performance between different groups. Empirically, we find that such performance gap is caused by the disparity of data distribution, specifically the knowledge graph path distribution in this work. We propose a fairness constrained approach via heuristic re-ranking to mitigate this unfairness problem in the context of explainable recommendation over knowledge graphs. We experiment on several real-world datasets with state-of-the-art knowledge graph-based explainable recommendation algorithms. The promising results show that our algorithm is not only able to provide high-quality explainable recommendations, but also reduces the recommendation unfairness in several aspects.","2020-07-25","2022-05-17 04:25:57","2022-12-20 05:07:34","2022-05-17","69-78","","","","","","","","","","","","","","","","","","","","47 citations (Crossref) [2022-12-20] arXiv: 2006.02046 Publisher: Association for Computing Machinery, Inc ISBN: 9781450380164","","","","","explainable recommendation; fairness; knowledge graphs","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EPWPXUUK","journalArticle","2015","Brooks, Michael; Amershi, Saleema; Lee, Bongshin; Drucker, Steven M.; Kapoor, Ashish; Simard, Patrice","FeatureInsight: Visual support for error-driven feature ideation in text classification","2015 IEEE Conference on Visual Analytics Science and Technology, VAST 2015 - Proceedings","","","10.1109/VAST.2015.7347637","","Machine learning requires an effective combination of data, features, and algorithms. While many tools exist for working with machine learning data and algorithms, support for thinking of new features, or feature ideation, remains poor. In this paper, we investigate two general approaches to support feature ideation: visual summaries and sets of errors. We present FeatureInsight, an interactive visual analytics tool for building new dictionary features (semantically related groups of words) for text classification problems. FeatureInsight supports an error-driven feature ideation process and provides interactive visual summaries of sets of misclassified documents. We conducted a controlled experiment evaluating both visual summaries and sets of errors in FeatureInsight. Our results show that visual summaries significantly improve feature ideation, especially in combination with sets of errors. Users preferred visual summaries over viewing raw data, and only preferred examining sets when visual summaries were provided. We discuss extensions of both approaches to data types other than text, and point to areas for future research.","2015-12-04","2021-11-25 02:43:14","2022-12-20 05:07:33","2021-11-25","105-112","","","","","","","","","","","","","","","","","","","","39 citations (Crossref) [2022-12-20] Publisher: Institute of Electrical and Electronics Engineers Inc. ISBN: 9781467397834","","","","","Features; machine learning; text classification; dictionaries; interactive machine learning; visualization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LXSTCHH9","journalArticle","2021","Fritz-Morgenthal, Sebastian; Hein, Bernhard; Papenbrock, Jochen","Financial Risk Management and Explainable Trustworthy Responsible AI","SSRN Electronic Journal","","","10.2139/SSRN.3873768","https://papers.ssrn.com/abstract=3873768","Executive Summary This perspective paper is based on several sessions by the members of the Round Table AI at FIRM 1 , with input from a number of external and international speakers. Its particular focus lies on the management of the model risk of productive models in banks and other financial institutions. The models in view range from simple rules-based approaches to Artificial Intelligence (AI) or Machine learning (ML) models with a high level of sophistication. The typical applications of those models are related to predictions and decision making around the value chain of credit risk (including accounting side under IFRS9 or related national GAAP approaches), insurance risk or other financial risk types. We expect more models of higher complexity in the space of anti money laundering, fraud detection and transaction monitoring as well as a rise of AI/ML models as alternatives to current methods in solving some of the more intricate stochastic differential equations needed for the pricing and/or valuation of derivatives. The same type of model is also successful in areas unrelated to risk management, such as sales optimization, customer lifetime value considerations, robo-advisory and other fields of applications. The paper makes reference to recent related publications from central banks, financial supervisors and regulators as well as by other relevant sources and working groups. It aims to give practical advice for establishing a risk-based governance and test framework for the mentioned model types and also discusses the use of recent technologies, approaches and platforms to support the establishment of responsible, trustworthy, explainable, auditable and manageable AI/ML in production. In view of the recent EU publication on AI (see European Commission 2021), also referred to as the EU Artificial Intelligence Act (AIA), we also see a certain added value for this paper as an instigator of further thinking outside of the financial services sector, in particular where ""High Risk"" models according to the mentioned EU consultation are concerned.","2021-06-25","2021-11-24 22:45:34","2022-12-20 05:07:33","2021-11-25","","","","","","","","","","","","","","","","","","","","","0 citations (Crossref) [2022-12-20] Publisher: Elsevier BV","","","","","Bernhard Hein; Financial Risk Management and Explainable Trustworthy Responsible AI; Jochen Papenbrock; Sebastian Fritz-Morgenthal; SSRN","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9D73S33Y","journalArticle","2020","Lundberg, Scott M.; Erion, Gabriel; Chen, Hugh; DeGrave, Alex; Prutkin, Jordan M.; Nair, Bala; Katz, Ronit; Himmelfarb, Jonathan; Bansal, Nisha; Lee, Su-In","From local explanations to global understanding with explainable AI for trees","Nature Machine Intelligence","","2522-5839","10.1038/s42256-019-0138-9","https://www.nature.com/articles/s42256-019-0138-9","Tree-based machine learning models such as random forests, decision trees, and gradient boosted trees are popular non-linear predictive models, yet comparatively little attention has been paid to explaining their predictions. Here, we improve the interpretability of tree-based models through three main contributions: 1) The first polynomial time algorithm to compute optimal explanations based on game theory. 2) A new type of explanation that directly measures local feature interaction effects. 3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to i) identify high magnitude but low frequency non-linear mortality risk factors in the US population, ii) highlight distinct population sub-groups with shared risk characteristics, iii) identify non-linear interaction effects among risk factors for chronic kidney disease, and iv) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model's performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains.","2020-01-17","2021-06-08 20:31:57","2022-12-20 05:07:32","2021-06-09","56-67","","1","2","","","","","","","","","","","","","","","","","1396 citations (Crossref) [2022-12-20] PMID: 32607472 Publisher: Springer Science and Business Media LLC QID: Q96823790","","","","","Computer science; Medical research; Software","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D7XQHHAF","journalArticle","2017","Chen, Liang; Bentley, Paul; Rueckert, Daniel","Fully automatic acute ischemic lesion segmentation in DWI using convolutional neural networks","NeuroImage: Clinical","","2213-1582","10.1016/J.NICL.2017.06.016","","Stroke is an acute cerebral vascular disease, which is likely to cause long-term disabilities and death. Acute ischemic lesions occur in most stroke patients. These lesions are treatable under accurate diagnosis and treatments. Although diffusion-weighted MR imaging (DWI) is sensitive to these lesions, localizing and quantifying them manually is costly and challenging for clinicians. In this paper, we propose a novel framework to automatically segment stroke lesions in DWI. Our framework consists of two convolutional neural networks (CNNs): one is an ensemble of two DeconvNets (Noh et al., 2015), which is the EDD Net; the second CNN is the multi-scale convolutional label evaluation net (MUSCLE Net), which aims to evaluate the lesions detected by the EDD Net in order to remove potential false positives. To the best of our knowledge, it is the first attempt to solve this problem and using both CNNs achieves very good results. Furthermore, we study the network architectures and key configurations in detail to ensure the best performance. It is validated on a large dataset comprising clinical acquired DW images from 741 subjects. A mean accuracy of Dice coefficient obtained is 0.67 in total. The mean Dice scores based on subjects with only small and large lesions are 0.61 and 0.83, respectively. The lesion detection rate achieved is 0.94.","2017-01-01","2022-06-06 02:13:27","2022-12-20 05:07:30","2022-06-06","633-643","","","15","","","","","","","","","","","","","","","","","167 citations (Crossref) [2022-12-20] PMID: 28664034 Publisher: Elsevier QID: Q41072409","","C:\Users\ambreen.hanif\Zotero\storage\QGHX6KIG\full-text.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FAWUQLVR","journalArticle","2020","Zhou, Jie; Cui, Ganqu; Hu, Shengding; Zhang, Zhengyan; Yang, Cheng; Liu, Zhiyuan; Wang, Lifeng; Li, Changcheng; Sun, Maosong","Graph neural networks: A review of methods and applications","AI Open","","26666510","10.1016/j.aiopen.2021.01.001","","Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.","2020","2021-11-24 23:35:23","2022-12-20 05:07:29","2021-11-25","57-81","","","1","","","","","","","","","","","","","","","","","690 citations (Crossref) [2022-12-20] arXiv: 1812.08434 Publisher: Elsevier BV","","C:\Users\ambreen.hanif\Zotero\storage\CVFL23V6\full-text.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DV2D8K5X","journalArticle","2017","Selvaraju, Ramprasaath R.; Cogswell, Michael; Das, Abhishek; Vedantam, Ramakrishna; Parikh, Devi; Batra, Dhruv","Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization","International Journal of Computer Vision","","","10.1007/s11263-019-01228-7","http://gradcam.cloudcv.org","We propose a technique for producing 'visual explana-tions' for decisions from a large class of Convolutional Neu-ral Network (CNN)-based models, making them more transparent. Our approach-Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for 'dog' or even a caption), flowing into the final convolutional layer to produce a coarse localiza-tion map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or retraining. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models , our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised local-ization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visual-izations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a 'stronger' deep network from a 'weaker' one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2] 1 and video at youtu.be/COjUB9Izk6E. * Work done at Virginia Tech. 1","2017","2021-09-28 00:14:39","2022-12-20 05:07:29","2021-09-28","336-359","","2","128","","","","","","","","","","","","","","","","","1125 citations (Crossref) [2022-12-20] Pages: 618-626 QID: Q112832282","","C:\Users\ambreen.hanif\Zotero\storage\69UGWK2P\full-text.pdf; C:\Users\ambreen.hanif\Zotero\storage\7PXSSEKP\Selvaraju et al_2017_Grad-CAM.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3NDHHGPR","journalArticle","2001","Friedman, Jerome H.","Greedy function approximation: A gradient boosting machine.","https://doi.org/10.1214/aos/1013203451","","0090-5364","10.1214/AOS/1013203451","https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full","Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent “boosting” paradigm is developed for additive expansions based on any fitting criterion.Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such “TreeBoost” models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.","2001-10-01","2022-01-14 03:20:53","2022-12-20 05:07:27","2022-01-14","1189-1232","","5","29","","","","","","","","","","","","","","","","","9503 citations (Crossref) [2022-12-20] Publisher: Institute of Mathematical Statistics QID: Q57532752","","; C:\Users\ambreen.hanif\Zotero\storage\43ECVI5D\Friedman_2001_Greedy function approximation.pdf","https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.short","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MD3V522V","journalArticle","","Wang, Danqing; Liu, Pengfei; Zheng, Yining; Qiu, Xipeng; Huang, Xuanjing","Heterogeneous Graph Neural Networks for Extractive Document Summarization","","","","10.18653/v1/2020.acl-main.553","https://github.com/brxx122/","As a crucial step in extractive document sum-marization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for ex-tractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github 1 .","","2022-05-17 04:41:14","2022-12-20 05:07:26","2022-05-17","","","","","","","","","","","","","","","","","","","","","63 citations (Crossref) [2022-12-20] arXiv: 2004.12393v1 ISBN: 2004.12393v1","","C:\Users\ambreen.hanif\Zotero\storage\ITFHU4JH\Wang et al_Heterogeneous Graph Neural Networks for Extractive Document Summarization.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GYMFM4V6","journalArticle","2016","Gillies, Marco; Fiebrink, Rebecca; Tanaka, Atau; Garcia, Jérémie; Bevilacqua, Frédéric; Heloir, Alexis; Nunnari, Fabrizio; Mackay, Wendy; Amershi, Saleema; Lee, Bongshin; D'alessandro, Nicolas; Tilmanne, Joëlle; Kulesza, Todd; Caramiaux, Baptiste","Human-centered machine learning","Conference on Human Factors in Computing Systems - Proceedings","","","10.1145/2851581.2856492","","Machine learning is one of the most important and successful techniques in contemporary computer science. It involves the statistical inference of models (such as classifiers) from data. It is often conceived in a very impersonal way, with algorithms working autonomously on passively collected data. However, this viewpoint hides considerable human work of tuning the algorithms, gathering the data, and even deciding what should be modeled in the first place. Examining machine learning from a human-centered perspective includes explicitly recognising this human work, as well as reframing machine learning workflows based on situated human working practices, and exploring the coadaptation of humans and systems. A human-centered understanding of machine learning in human context can lead not only to more usable machine learning tools, but to new ways of framing learning computationally. This workshop will bring together researchers to discuss these issues and suggest future research questions aimed at creating a human-centered approach to machine learning.","2016-05-07","2022-07-04 23:54:37","2022-12-20 05:07:24","2022-07-05","3558-3565","","","07-12-May-2016","","","","","","","","","","","","","","","","","69 citations (Crossref) [2022-12-20] Publisher: Association for Computing Machinery ISBN: 9781450340823","","","","","Machine learning; Data; User-centered design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HU23RU7B","conferencePaper","2020","Lertvittayakumjorn, Piyawat; Toni, Francesca","Human-grounded evaluations of explanation methods for text classification","EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference","978-1-950737-90-1","","10.18653/v1/d19-1523","https://www.aclweb.org/anthology/D19-1523","Due to the black-box nature of deep learning models, methods for explaining the models' results are crucial to gain trust from humans and support collaboration between AIs and humans. In this paper, we consider several model-agnostic and model-specific explanation methods for CNNs for text classification and conduct three human-grounded evaluations, focusing on different purposes of explanations: (1) revealing model behavior, (2) justifying model predictions, and (3) helping humans investigate uncertain predictions. The results highlight dissimilar qualities of the various explanation methods we consider and show the degree to which these methods could serve for each purpose.","2020","2021-05-20 00:55:44","2022-12-20 05:07:23","2021-05-20","5195-5205","","","","","","","","","","","Association for Computational Linguistics","","","","","","","","","13 citations (Crossref) [2022-12-20] arXiv: 1908.11355","","C:\Users\ambreen.hanif\Zotero\storage\IA3CTTAJ\Lertvittayakumjorn_Toni_2020_Human-grounded evaluations of explanation methods for text classification.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5483TM3N","journalArticle","2018","Khalili-Damghani, Kaveh; Abdi, Farshid; Abolmakarem, Shaghayegh","Hybrid soft computing approach based on clustering, rule mining, and decision tree analysis for customer segmentation problem: Real case of customer-centric industries","Applied Soft Computing Journal","","15684946","10.1016/j.asoc.2018.09.001","","This paper proposes a hybrid soft computing approach on the basis of clustering, rule extraction, and decision tree methodology to predict the segment of the new customers in customer-centric companies. In the first module, K-means algorithm is applied to cluster the past customers of company on the basis of their purchase behavior. In the second module, a hybrid feature selection method based on filtering and a multi-attribute decision making method is proposed. Finally, On the basis of customers’ characteristics and using decision tree analysis, IF–THEN rules are mined. The proposed approach is applied in two case studies in the field of insurance and telecommunication in order to predict potentially profitable leads and outline the most influential features available to customers in order to perform this prediction. The results validate the efficacy and applicability of proposed approach to handle real-life cases.","2018-12-01","2021-03-15 17:58:52","2022-12-20 05:07:22","2021-03-16","816-828","","","73","","","","","","","","","","","","","","","","","31 citations (Crossref) [2022-12-20] Publisher: Elsevier Ltd","","","","","Clustering algorithm; Data mining; Decision tree; Feature selection; Rule mining","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R42RIFM4","journalArticle","2020","Garcia-Mendez, Silvia; Fernandez-Gavilanes, Milagros; Juncal-Martinez, Jonathan; Gonzalez-Castano, Francisco Javier; Seara, Oscar Barba","Identifying Banking Transaction Descriptions via Support Vector Machine Short-Text Classification Based on a Specialized Labelled Corpus","IEEE Access","","21693536","10.1109/ACCESS.2020.2983584","","Short texts are omnipresent in real-time news, social network commentaries, etc. Traditional text representation methods have been successfully applied to self-contained documents of medium size. However, information in short texts is often insufficient, due, for example, to the use of mnemonics, which makes them hard to classify. Therefore, the particularities of specific domains must be exploited. In this article we describe a novel system that combines Natural Language Processing techniques with Machine Learning algorithms to classify banking transaction descriptions for personal finance management, a problem that was not previously considered in the literature. We trained and tested that system on a labelled dataset with real customer transactions that will be available to other researchers on request. Motivated by existing solutions in spam detection, we also propose a short text similarity detector to reduce training set size based on the Jaccard distance. Experimental results with a two-stage classifier combining this detector with a SVM indicate a high accuracy in comparison with alternative approaches, taking into account complexity and computing time. Finally, we present a use case with a personal finance application, CoinScrap, which is available at Google Play and App Store.","2020","2021-04-30 05:15:02","2022-12-20 05:07:22","2021-04-30","61642-61655","","","8","","","","","","","","","","","","","","","","","10 citations (Crossref) [2022-12-20] Publisher: Institute of Electrical and Electronics Engineers Inc. QID: Q114256069","","C:\Users\ambreen.hanif\Zotero\storage\ZVF6YML3\Garcia-Mendez et al_2020_Identifying Banking Transaction Descriptions via Support Vector Machine.pdf","","","Machine learning; banking; natural language processing; personal finance management","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PXVK28AC","journalArticle","2017","KrizhevskyAlex; SutskeverIlya; E., HintonGeoffrey","ImageNet classification with deep convolutional neural networks","Communications of the ACM","","","10.1145/3065386","https://dl.acm.org/doi/abs/10.1145/3065386","We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we ach...","2017-05-24","2021-08-26 01:26:37","2022-12-20 05:07:21","2021-08-26","84-90","","6","60","","","","","","","","","","","","","","","","","12646 citations (Crossref) [2022-12-20] Publisher:  		ACM 		PUB27 		New York, NY, USA QID: Q59445836","","C:\Users\ambreen.hanif\Zotero\storage\AC5HAMLP\KrizhevskyAlex et al_2017_ImageNet classification with deep convolutional neural networks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8WZQPIE8","journalArticle","2015","Russakovsky, Olga; Deng, Jia; Su, Hao; Krause, Jonathan; Satheesh, Sanjeev; Ma, Sean; Huang, Zhiheng; Karpathy, Andrej; Khosla, Aditya; Bernstein, Michael; Berg, Alexander C.; Fei-Fei, Li","ImageNet Large Scale Visual Recognition Challenge","International Journal of Computer Vision 2015 115:3","","1573-1405","10.1007/S11263-015-0816-Y","https://link.springer.com/article/10.1007/s11263-015-0816-y","The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.","2015-04-11","2021-08-25 09:23:36","2022-12-20 05:07:20","2021-08-25","211-252","","3","115","","","","","","","","","","","","","","","","","17949 citations (Crossref) [2022-12-20] Publisher: Springer QID: Q56594393","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4KDI64BA","journalArticle","2020","Deng, Changyu; Ji, Xunbi; Rainey, Colton; Zhang, Jianyu; Lu, Wei","Integrating Machine Learning with Human Knowledge","iScience","","2589-0042","10.1016/J.ISCI.2020.101656","","Machine learning has been heavily researched and widely used in many disciplines. However, achieving high accuracy requires a large amount of data that is sometimes difficult, expensive, or impractical to obtain. Integrating human knowledge into machine learning can significantly reduce data requirement, increase reliability and robustness of machine learning, and build explainable machine learning systems. This allows leveraging the vast amount of human knowledge and capability of machine learning to achieve functions and performance not available before and will facilitate the interaction between human beings and machine learning systems, making machine learning decisions understandable to humans. This paper gives an overview of the knowledge and its representations that can be integrated into machine learning and the methodology. We cover the fundamentals, current status, and recent progress of the methods, with a focus on popular and new topics. The perspectives on future directions are also discussed.","2020-11-20","2022-05-17 04:46:57","2022-12-20 05:07:19","2022-05-17","101656","","11","23","","","","","","","","","","","","","","","","","51 citations (Crossref) [2022-12-20] Publisher: Elsevier QID: Q101146914","","C:\Users\ambreen.hanif\Zotero\storage\SJG8E45W\full-text.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HJN9XNNS","journalArticle","2018","Li, Huang; Fang, Shiaofen; Mukhopadhyay, Snehasis; Saykin, Andrew J.; Shen, Li","Interactive Machine Learning by Visualization: A Small Data Solution","Proceedings : ... IEEE International Conference on Big Data. IEEE International Conference on Big Data","","","10.1109/BIGDATA.2018.8621952","/pmc/articles/PMC6499624/","Machine learning algorithms and traditional data mining process usually require a large volume of data to train the algorithm-specific models, with little or no user feedback during the model building process. Such a »big data» based automatic learning strategy is sometimes unrealistic for applications where data collection or processing is very expensive or difficult, such as in clinical trials. Furthermore, expert knowledge can be very valuable in the model building process in some fields such as biomedical sciences. In this paper, we propose a new visual analytics approach to interactive machine learning and visual data mining. In this approach, multi-dimensional data visualization techniques are employed to facilitate user interactions with the machine learning and mining process. This allows dynamic user feedback in different forms, such as data selection, data labeling, and data correction, to enhance the efficiency of model building. In particular, this approach can significantly reduce the amount of data required for training an accurate model, and therefore can be highly impactful for applications where large amount of data is hard to obtain. The proposed approach is tested on two application problems: the handwriting recognition (classification) problem and the human cognitive score prediction (regression) problem. Both experiments show that visualization supported interactive machine learning and data mining can achieve the same accuracy as an automatic process can with much smaller training data sets.","2018-01-22","2021-11-24 22:45:57","2022-12-20 05:07:18","2021-11-25","3513","","","2018","","","","","","","","","","","","","","","","","9 citations (Crossref) [2022-12-20] PMID: 31061990 Publisher: NIH Public Access ISBN: 9781538650356 QID: Q91809731","","; ","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6499624/; /pmc/articles/PMC6499624/?report=abstract","","interactive machine learning; multi-dimensional data visualization; user interaction; visual data mining","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ATMBHHQ6","journalArticle","2016","Holzinger, A","Interactive machine learning for health informatics: when do we need the human-in-the-loop?","Brain Inform","","","10.1007/s40708-016-0042-6","","Machine learning (ML) is the fastest growing field in computer science, and health informatics is among the greatest challenges. The goal of ML is to develop algorithms which can learn and improve over time and can be used for predictions. Most ML researchers concentrate on automatic machine learning (aML), where great advances have been made, for example, in speech recognition, recommender systems, or autonomous vehicles. Automatic approaches greatly benefit from big data with many training sets. However, in the health domain, sometimes we are confronted with a small number of data sets or rare events, where aML-approaches suffer of insufficient training samples. Here interactive machine learning (iML) may be of help, having its roots in reinforcement learning, preference learning, and active learning. The term iML is not yet well used, so we define it as “algorithms that can interact with agents and can optimize their learning behavior through these interactions, where the agents can also be human.” This “human-in-the-loop” can be beneficial in solving computationally hard problems, e.g., subspace clustering, protein folding, or k-anonymization of health data, where human expertise can help to reduce an exponential search space through heuristic selection of samples. Therefore, what would otherwise be an NP-hard problem, reduces greatly in complexity through the input and the assistance of a human agent involved in the learning phase.","2016-06-01","2021-09-30 00:20:20","2022-12-20 05:07:17","2021-09-30","119-131","","2","3","","","","","","","","","","","","","","","","","425 citations (Crossref) [2022-12-20] Publisher: Springer Berlin Heidelberg QID: Q28602093","","C:\Users\ambreen.hanif\Zotero\storage\ITYF8M4D\Holzinger_2016_Interactive machine learning for health informatics.pdf","","","Interactive machine learning; Health informatics","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N9JW6JYR","journalArticle","2019","A Holzinger, M Plass,; M Kickmeier-Rust; K Holzinger; GC Crian; C-M Pintea; V Palade","Interactive machine learning: experimental evidence for the human in the algorithmic loop","Appl Intell","","","10.1007/s10489-018-1361-5","","Recent advances in automatic machine learning (aML) allow solving problems without any human intervention. However, sometimes a human-in-the-loop can be beneficial in solving computationally hard problems. In this paper we provide new experimental insights on how we can improve computational intelligence by complementing it with human intelligence in an interactive machine learning approach (iML). For this purpose, we used the Ant Colony Optimization (ACO) framework, because this fosters multi-agent approaches with human agents in the loop. We propose unification between the human intelligence and interaction skills and the computational power of an artificial system. The ACO framework is used on a case study solving the Traveling Salesman Problem, because of its many practical implications, e.g. in the medical domain. We used ACO due to the fact that it is one of the best algorithms used in many applied intelligence problems. For the evaluation we used gamification, i.e. we implemented a snake-like game called Traveling Snakesman with the MAX–MIN Ant System (MMAS) in the background. We extended the MMAS–Algorithm in a way, that the human can directly interact and influence the ants. This is done by “traveling” with the snake across the graph. Each time the human travels over an ant, the current pheromone value of the edge is multiplied by 5. This manipulation has an impact on the ant’s behavior (the probability that this edge is taken by the ant increases). The results show that the humans performing one tour through the graphs have a significant impact on the shortest path found by the MMAS. Consequently, our experiment demonstrates that in our case human intelligence can positively influence machine intelligence. To the best of our knowledge this is the first study of this kind.","2019-07-15","2021-09-30 00:20:20","2022-12-20 05:07:16","2021-09-30","2401-2414","","7","49","","","","","","","","","","","","","","","","","111 citations (Crossref) [2022-12-20] Publisher: Springer New York LLC QID: Q62044188","","C:\Users\ambreen.hanif\Zotero\storage\SSCZ3HML\A Holzinger et al_2019_Interactive machine learning.pdf","","","Human-in-the-loop; Interactive machine learning; Ant Colony Optimization; Combinatorial optimization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JGGFY44G","journalArticle","2019","Mohammadian, Mahmoud; Makhani, Iman","International Academic Journal of Accounting and Financial Management RFM-Based customer segmentation as an elaborative analytical tool for enriching the creation of sales and trade marketing strategies","International Academic Journal of Accounting and Financial Management","","2454-2210","10.9756/IAJAFM/V6I1/1910009","www.iaiest.com","In the current intensified competitive market, analyzing customers meticulously and implementing customer relationship management accordingly are the main reasons behind the success of breakthrough companies. One of the main constraints and deficiencies of sales and trade marketing departments in terms of sales development in FMCG (fast moving consumer goods) industry is that they don't know which customer segments to target and how to deal with each one. In this study we discuss how these departments can gather customer data and how they can analyze these data to gain useful customer insights. We provide an overview of customer segmentation based on RFM method and customer lifetime value (CLV). The results would be useful for sales, trade marketing and marketing decision-makings in all industries specially the active companies in FMCG industry. Introduction:","2019","2021-03-15 17:59:45","2022-12-20 05:07:15","2021-03-16","102-116","","1","6","","","","","","","","","","","","","","","","","0 citations (Crossref) [2022-12-20]","","C:\Users\ambreen.hanif\Zotero\storage\4XSC3RZP\Mohammadian_Makhani_2019_International Academic Journal of Accounting and Financial Management RFM-Based.pdf","","","RFM-Based customer segmentation; trade marketing strategies","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TAP9V2MB","conferencePaper","2020","Molnar, Christoph; Casalicchio, Giuseppe; Bischl, Bernd","Interpretable Machine Learning – A Brief History, State-of-the-Art and Challenges","Communications in Computer and Information Science","978-3-030-65964-6","","10.1007/978-3-030-65965-3_28","","We present a brief history of the field of interpretable machine learning (IML), give an overview of state-of-the-art interpretation methods and discuss challenges. Research in IML has boomed in recent years. As young as the field is, it has over 200 years old roots in regression modeling and rule-based machine learning, starting in the 1960s. Recently, many new IML methods have been proposed, many of them model-agnostic, but also interpretation techniques specific to deep learning and tree-based ensembles. IML methods either directly analyze model components, study sensitivity to input perturbations, or analyze local or global surrogate approximations of the ML model. The field approaches a state of readiness and stability, with many methods not only proposed in research, but also implemented in open-source software. But many important challenges remain for IML, such as dealing with dependent features, causal interpretation, and uncertainty estimation, which need to be resolved for its successful application to scientific problems. A further challenge is a missing rigorous definition of interpretability, which is accepted by the community. To address the challenges and advance the field, we urge to recall our roots of interpretable, data-driven modeling in statistics and (rule-based) ML, but also to consider other areas such as sensitivity analysis, causal inference, and the social sciences.","2020","2021-09-20 01:16:38","2022-12-20 05:07:14","2021-09-20","417-431","","","1323","","","","","","","","","","","","","","","","","62 citations (Crossref) [2022-12-20] arXiv: 2010.09337 ISSN: 18650937","","C:\Users\ambreen.hanif\Zotero\storage\4EZINEKE\Molnar et al_2020_Interpretable Machine Learning – A Brief History, State-of-the-Art and.pdf","","","Explainable artificial intelligence; Interpretable Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IZHQL2V7","journalArticle","2020","Rodríguez-Pérez, Raquel; Bajorath, Jürgen","Interpretation of machine learning models using shapley values: application to compound potency and multi-target activity predictions","Journal of Computer-Aided Molecular Design 2020 34:10","","1573-4951","10.1007/S10822-020-00314-0","https://link.springer.com/article/10.1007/s10822-020-00314-0","Difficulties in interpreting machine learning (ML) models and their predictions limit the practical applicability of and confidence in ML in pharmaceutical research. There is a need for agnostic approaches aiding in the interpretation of ML models regardless of their complexity that is also applicable to deep neural network (DNN) architectures and model ensembles. To these ends, the SHapley Additive exPlanations (SHAP) methodology has recently been introduced. The SHAP approach enables the identification and prioritization of features that determine compound classification and activity prediction using any ML model. Herein, we further extend the evaluation of the SHAP methodology by investigating a variant for exact calculation of Shapley values for decision tree methods and systematically compare this variant in compound activity and potency value predictions with the model-independent SHAP method. Moreover, new applications of the SHAP analysis approach are presented including interpretation of DNN models for the generation of multi-target activity profiles and ensemble regression models for potency prediction.","2020-05-02","2021-08-31 02:25:46","2022-12-20 05:07:13","2021-08-31","1013-1026","","10","34","","","","","","","","","","","","","","","","","118 citations (Crossref) [2022-12-20] Publisher: Springer QID: Q94497353","","C:\Users\ambreen.hanif\Zotero\storage\KTG3SIIP\full-text.pdf","","","Animal Anatomy / Morphology / Histology; Computer Applications in Chemistry; Physical Chemistry","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZA8B5ZMN","journalArticle","2019","Ma, Weizhi; Jin, Woojeong; Zhang, Min; Wang, Chenyang; Cao, Yue; Liu, Yiqun; Ma, Shaoping; Ren, Xiang","Jointly learning explainable rules for recommendation with knowledge graph","The Web Conference 2019 - Proceedings of the World Wide Web Conference, WWW 2019","","","10.1145/3308558.3313607","","Explainability and effectiveness are two key aspects for building recommender systems. Prior efforts mostly focus on incorporating side information to achieve better recommendation performance. However, these methods have some weaknesses: (1) prediction of neural network-based embedding methods are hard to explain and debug; (2) symbolic, graph-based approaches (e.g., meta path-based models) require manual efforts and domain knowledge to define patterns and rules, and ignore the item association types (e.g. substitutable and complementary). In this paper, we propose a novel joint learning framework to integrate induction of explainable rules from knowledge graph with construction of a rule-guided neural recommendation model. The framework encourages two modules to complement each other in generating effective and explainable recommendation: 1) inductive rules, mined from item-centric knowledge graphs, summarize common multi-hop relational patterns for inferring different item associations and provide human-readable explanation for model prediction; 2) recommendation module can be augmented by induced rules and thus have better generalization ability dealing with the cold-start issue. Extensive experiments1 show that our proposed method has achieved significant improvements in item recommendation over baselines on real-world datasets. Our model demonstrates robust performance over “noisy"" item knowledge graphs, generated by linking item names to related entities.","2019-05-13","2021-09-28 02:17:21","2022-12-20 05:07:12","2021-09-28","1210-1221","","","","","","","","","","","","","","","","","","","","55 citations (Crossref) [2022-12-20] Publisher: Association for Computing Machinery, Inc","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YTYPYZUJ","journalArticle","2021","Luan, Chaoxu; Liu, Renzhi; Peng, Sicheng","Land-use suitability assessment for urban development using a GIS-based soft computing approach: A case study of Ili Valley, China","Ecological Indicators","","1470-160X","10.1016/J.ECOLIND.2020.107333","","Land-use suitability assessment is an important step in land use planning for urban development. We propose a GIS-based soft computing approach (GSC), which is a combination of two multi-criteria analysis methods, i.e., the ordered weighted averaging (OWA) method and the logic scoring of preference (LSP) method, to evaluate and map land-use suitability for urban development in Ili Valley, China. The evaluation uses 13 factors as suitability criteria for urban development. These factors are related to the topography and geology, socio-economic feasibility, ecological restrictions, and prohibitive constraints. Based on the final suitability results, Ili Valley was classified using five suitability levels: highly suitable, suitable, moderately suitable, marginally suitable, and not suitable. By comparing seven preference decision coefficient (α) scenarios, we determined that the area of the land that is highly suitable for urban development decreases, and the area of the marginally suitable land increases with increasing α. All of the scenarios show that approximately 32.6% of the land area is not suitable. Among the seven scenarios, the policy orientation of three of the scenarios was an urban expansion policy orientation (α = 0.5), a balanced policy orientation (α = 1), and an ecological protection policy orientation (α = 2). The result of the policy orientation analysis of urban development can be used as an urban development boundary for different development preferences and stages. The local land-use planning was evaluated by overlaying its planned urban development zones with the areas of the resultant suitability map. Specific recommendations are presented for the scale and timing of urban development in Ili Valley.","2021-04-01","2022-04-07 04:32:46","2022-12-20 05:07:09","2022-04-07","107333","","","123","","","","","","","","","","","","","","","","","21 citations (Crossref) [2022-12-20] Publisher: Elsevier QID: Q113876342","","C:\Users\ambreen.hanif\Zotero\storage\DD9C5854\Luan et al_2021_Land-use suitability assessment for urban development using a GIS-based soft.pdf","","","Mandatory suitability factors; Multi-criteria assessment; Ordered weighted averaging; Policy orientation analysis; Urban areas","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S2EAMECJ","journalArticle","2014","Zhang, Jia Dong; Chow, Chi Yin; Li, Yanhua","LORE: Exploiting sequential influence for location recommendations","GIS: Proceedings of the ACM International Symposium on Advances in Geographic Information Systems","","","10.1145/2666310.2666400","","Providing location recommendations becomes an important feature for location-based social networks (LBSNs), since it helps users explore new places and makes LBSNs more prevalent to users. In LBSNs, geographical influence and social influence have been intensively used in location recommendations based on the facts that geographical proximity of locations significantly affects users' check-in behaviors and social friends often have common interests. Although human movement exhibits sequential patterns, most current studies on location recommendations do not consider any sequential influence of locations on users' check-in behaviors. In this paper, we propose a new approach called LORE to exploit sequential influence on location recommendations. First, LORE incrementally mines sequential patterns from location sequences and represents the sequential patterns as a dynamic Location-Location Transition Graph (L2TG). LORE then predicts the probability of a user visiting a location by Additive Markov Chain (AMC) with L2TG. Finally, LORE fuses sequential influence with geographical influence and social influence into a unified recommendation frame- work; in particular the geographical influence is modeled as two-dimensional check-in probability distributions rather than one-dimensional distance probability distributions in existing works. We conduct a comprehensive performance evaluation for LORE using two large-scale real data sets collected from Foursquare and Gowalla. Experimental results show that LORE achieves significantly superior location recommendations compared to other state-of-the-art recommendation techniques.","2014-11-04","2021-10-26 23:24:32","2022-12-20 05:07:08","2021-10-27","103-112","","","04-07-November-2014","","","","","","","","","","","","","","","","","135 citations (Crossref) [2022-12-20] Publisher: Association for Computing Machinery","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4C689FZC","journalArticle","2020","Quade, Markus; Isele, Thomas; Abel, Markus","Machine learning control — explainable and analyzable methods","Physica D: Nonlinear Phenomena","","0167-2789","10.1016/J.PHYSD.2020.132582","","Recently, the term explainable AI came into discussion as an approach to produce models from artificial intelligence which allow interpretation. For a long time, symbolic regression has been used to produce explainable and mathematically tractable models. In this contribution, we extend previous work on symbolic regression methods to infer the optimal control of a dynamical system given one or several optimization criteria, or cost functions. In earlier publications, network control was achieved by automated machine learning control using genetic programming. Here, we focus on the subsequent path continuation analysis of the mathematical expressions which result from the machine learning model. In particular, we use AUTO to analyze the solution properties of the controlled oscillator system which served as our model. As a result, we show that there is a considerable advantage of explainable symbolic regression models over less accessible neural networks. In particular, the roadmap of future works may be to integrate such analyses into the optimization loop itself to filter out robust solutions by construction.","2020-11-01","2021-11-25 02:34:57","2022-12-20 05:07:07","2021-11-25","132582","","","412","","","","","","","","","","","","","","","","","3 citations (Crossref) [2022-12-20] Publisher: North-Holland","","","","","Explainable AI; Genetic programming; Dynamical systems; Machine learning control; Synchronization control","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9D9WCNBQ","journalArticle","2016","Silver, David; Huang, Aja; Maddison, Chris J.; Guez, Arthur; Sifre, Laurent; van den Driessche, George; Schrittwieser, Julian; Antonoglou, Ioannis; Panneershelvam, Veda; Lanctot, Marc; Dieleman, Sander; Grewe, Dominik; Nham, John; Kalchbrenner, Nal; Sutskever, Ilya; Lillicrap, Timothy; Leach, Madeleine; Kavukcuoglu, Koray; Graepel, Thore; Hassabis, Demis","Mastering the game of Go with deep neural networks and tree search","Nature 2016 529:7587","","1476-4687","10.1038/nature16961","https://www.nature.com/articles/nature16961","The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away. A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence. The victory in 1997 of the chess-playing computer Deep Blue in a six-game series against the then world champion Gary Kasparov was seen as a significant milestone in the development of artificial intelligence. An even greater challenge remained — the ancient game of Go. Despite decades of refinement, until recently the strongest computers were still playing Go at the level of human amateurs. Enter AlphaGo. Developed by Google DeepMind, this program uses deep neural networks to mimic expert players, and further improves its performance by learning from games played against itself. AlphaGo has achieved a 99% win rate against the strongest other Go programs, and defeated the reigning European champion Fan Hui 5–0 in a tournament match. This is the first time that a computer program has defeated a human professional player in even games, on a full, 19 x 19 board, in even games with no handicap.","2016-01-27","2021-08-26 01:41:43","2022-12-20 05:07:07","2021-08-26","484-489","","7587","529","","","","","","","","","","","","","","","","","6981 citations (Crossref) [2022-12-20] Publisher: Nature Publishing Group QID: Q28005460","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XTIBKC9R","journalArticle","2019","Glymour, Bruce; Herington, Jonathan","Measuring the biases that matter the ethical and casual foundations for measures of fairness in algorithms","FAT* 2019 - Proceedings of the 2019 Conference on Fairness, Accountability, and Transparency","","","10.1145/3287560.3287573","","Measures of algorithmic bias can be roughly classified into four categories, distinguished by the conditional probabilistic dependencies to which they are sensitive. First, measures of “procedural bias” diagnose bias when the score returned by an algorithm is probabilistically dependent on a sensitive class variable (e.g. race or sex). Second, measures of “outcome bias” capture probabilistic dependence between class variables and the outcome for each subject (e.g. parole granted or loan denied). Third, measures of “behavior-relative error bias” capture probabilistic dependence between class variables and the algorithmic score, conditional on target behaviors (e.g. recidivism or loan default). Fourth, measures of “score-relative error bias” capture probabilistic dependence between class variables and behavior, conditional on score. Several recent discussions have demonstrated a tradeoff between these different measures of algorithmic bias, and at least one recent paper has suggested conditions under which tradeoffs may be minimized. In this paper we use the machinery of causal graphical models to show that, under standard assumptions, the underlying causal relations among variables forces some tradeoffs. We delineate a number of normative considerations that are encoded in different measures of bias, with reference to the philosophical literature on the wrongfulness of disparate treatment and disparate impact. While both kinds of error bias are nominally motivated by concern to avoid disparate impact, we argue that consideration of causal structures shows that these measures are better understood as complicated and unreliable measures of procedural biases (i.e. disparate treatment). Moreover, while procedural bias is indicative of disparate treatment, we show that the measure of procedural bias one ought to adopt is dependent on the account of the wrongfulness of disparate treatment one endorses. Finally, given that neither score-relative nor behavior-relative measures of error bias capture the relevant normative considerations, we suggest that error bias proper is best measured by score-based measures of accuracy, such as the Brier score.","2019-01-29","2022-06-21 01:12:26","2022-12-20 05:07:06","2022-06-21","269-278","","","","","","","","","","","","","","","","","","","","23 citations (Crossref) [2022-12-20] Publisher: Association for Computing Machinery, Inc ISBN: 9781450361255","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WVUAU5N3","journalArticle","2021","Kraus, Matthias; Wagner, Nicolas; Minker, Wolfgang","Modelling and Predicting Trust for Developing Proactive Dialogue Strategies in Mixed-Initiative Interaction","","","","10.1145/3462244.3479906","","In mixed-initiative user interactions, a user and an autonomous agent collaborate for solving tasks by taking interleaving actions. However, this shift of control towards the agent requires a formation of trust for the user, otherwise the assistance possibly will be rejected and becomes obsolete. One approach for fostering a trustworthy interaction is to equip an agent with proactive dialogue capabilities. However, the development of adequate proactive dialogue strategies is complex and highly user- as well as context-dependent. Inappropriate usage of proactive conversation may even do more harm than good and corrupt the human-computer trust relationship. In order to alleviate this problem, modelling and predicting a proactive system's perceived trustworthiness during an ongoing interaction is essential. Therefore, this paper presents novel work on the development of a user model for live prediction of trust during proactive interaction, incorporating user-, system-, and context-dependent features. For predicting trust, three machine-learning algorithms - support vector machine, eXtreme Gradient Boost, gated recurrent unit network - are trained and tested on a proactive dialogue corpus. The experimental results show that among the classifiers the support vector machine showed the most well-rounded performance, while the gated recurrent unit had the best accuracy. The results prove the developed user model to be reliable for predicting trust in proactive dialogue. Based on the outcomes, the usability of the proposed method in real-life scenarios is discussed and implications for developing user-adaptive proactive dialogue strategies are described.","2021-10-18","2021-11-25 02:27:33","2022-12-20 05:07:03","2021-11-25","131-140","","","","","","","","","","","","","","","","","","","","1 citations (Crossref) [2022-12-20] Publisher: Association for Computing Machinery (ACM) ISBN: 9781450384810","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F3SHKNS5","journalArticle","2009","Hastie, Trevor; Rosset, Saharon; Zhu, Ji; Zou, Hui","Multi-class AdaBoost","Statistics and Its Interface","","19387989","10.4310/sii.2009.v2.n3.a8","https://www.intlpress.com/site/pub/pages/journals/items/sii/content/vols/0002/0003/a008/abstract.php","Boosting has been a very successful technique for solving the two-class classification problem. In going from two-class to multi-class classification, most algorithms have been restricted to reducing the multi-class classification problem to multiple two-class problems. In this paper, we develop a new algorithm that directly extends the AdaBoost algorithm to the multi-class case without reducing it tomultiple two-class problems. We show that the proposed multi-class AdaBoost algorithm is equivalent to a forward stagewise additive modeling algorithm that minimizes a novel exponential loss for multi-class classification. Furthermore, we show that the exponential loss is a member of a class of Fisher-consistent loss functions for multi-class classification. As shown in the paper, the new algorithm is extremely easy to implement and is highly competitive in terms of misclassification error rate.","2009","2021-06-08 21:01:46","2022-12-20 05:07:03","2021-06-09","349-360","","3","2","","","","","","","","","","","","","","","","","840 citations (Crossref) [2022-12-20] Publisher: International Press of Boston QID: Q105584484","","C:\Users\ambreen.hanif\Zotero\storage\CN6HU7JB\Hastie et al_2009_Multi-class AdaBoost.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ABPTR5XF","journalArticle","2010","Segel, Edward; Heer, Jeffrey","Narrative Visualization: Telling Stories with Data","","","","10.1109/TVCG.2010.179","","Data visualization is regularly promoted for its ability to reveal stories within data, yet these ""data stories"" differ in important ways from traditional forms of storytelling. Storytellers, especially online journalists, have increasingly been integrating visualizations into their narratives, in some cases allowing the visualization to function in place of a written story. In this paper, we systematically review the design space of this emerging class of visualizations. Drawing on case studies from news media to visualization research, we identify distinct genres of narrative visualization. We characterize these design differences, together with interactivity and mes-saging, in terms of the balance between the narrative flow intended by the author (imposed by graphical elements and the interface) and story discovery on the part of the reader (often through interactive exploration). Our framework suggests design strategies for narrative visualization, including promising under-explored approaches to journalistic storytelling and educational media.","2010","2022-01-23 23:16:57","2022-12-20 05:07:02","2022-01-24","","","","","","","","","","","","","","","","","","","","","544 citations (Crossref) [2022-12-20] QID: Q33727709","","C:\Users\ambreen.hanif\Zotero\storage\ATL9AL2C\Segel_Heer_2010_Narrative Visualization.pdf","","","storytelling; case study; design methods; Index Terms-Narrative visualization; journalism; social data analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DTUPLTI3","conferencePaper","2002","Loper, Edward; Bird, Steven","NLTK","Proceedings of the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics  -","","","10.3115/1118108.1118117","http://portal.acm.org/citation.cfm?doid=1118108.1118117","NLTK, the Natural Language Toolkit, is a suite of open source program modules, tutorials and problem sets, providing ready-to-use computational linguistics courseware. NLTK covers symbolic and statistical natural language processing, and is interfaced to annotated corpora. Students augment and replace existing components, learn structured programming by example, and manipulate sophisticated models from the outset.","2002","2021-03-26 14:08:43","2022-12-20 05:07:01","2021-03-27","63-70","","","1","","","","","","","","Association for Computational Linguistics (ACL)","Morristown, NJ, USA","","","","","","","","809 citations (Crossref) [2022-12-20]","","C:\Users\ambreen.hanif\Zotero\storage\KUTKESX7\Loper_Bird_2002_NLTK.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZYDAXWPK","journalArticle","2021","Vilone, Giulia; Longo, Luca","Notions of explainability and evaluation approaches for explainable artificial intelligence","Information Fusion","","1566-2535","10.1016/J.INFFUS.2021.05.009","","Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models that lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested, coupled with several studies attempting to define the concept of explainability and its evaluation. This systematic review contributes to the body of knowledge by clustering all the scientific studies via a hierarchical system that classifies theories and notions related to the concept of explainability and the evaluation approaches for XAI methods. The structure of this hierarchy builds on top of an exhaustive analysis of existing taxonomies and peer-reviewed scientific material. Findings suggest that scholars have identified numerous notions and requirements that an explanation should meet in order to be easily understandable by end-users and to provide actionable information that can inform decision making. They have also suggested various approaches to assess to what degree machine-generated explanations meet these demands. Overall, these approaches can be clustered into human-centred evaluations and evaluations with more objective metrics. However, despite the vast body of knowledge developed around the concept of explainability, there is not a general consensus among scholars on how an explanation should be defined, and how its validity and reliability assessed. Eventually, this review concludes by critically discussing these gaps and limitations, and it defines future research directions with explainability as the starting component of any artificial intelligent system.","2021-12-01","2021-11-25 02:29:58","2022-12-20 05:07:00","2021-11-25","89-106","","","76","","","","","","","","","","","","","","","","","54 citations (Crossref) [2022-12-20] Publisher: Elsevier","","C:\Users\ambreen.hanif\Zotero\storage\M64PSUTM\full-text.pdf","","","Explainable artificial intelligence; Evaluation methods; Notions of explainability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"88RGIJ6B","journalArticle","2013","Hauke, Sascha; Biedermann, Sebastian; Muhlhauser, Max; Heider, Dominik","On the application of supervised machine learning to trustworthiness assessment","Proceedings - 12th IEEE International Conference on Trust, Security and Privacy in Computing and Communications, TrustCom 2013","","","10.1109/TRUSTCOM.2013.5","","State-of-the art trust and reputation systems seek to apply machine learning methods to overcome generalizability issues of experience-based Bayesian trust assessment. These approaches are, however, often model-centric instead of focussing on data and the complex adaptive system that is driven by reputation-based service selection. This entails the risk of unrealistic model assumptions. We outline the requirements for robust probabilistic trust assessment using supervised learning and apply a selection of estimators to a real-world dataset, in order to show the effectiveness of supervised methods. Furthermore, we provide a representational mapping of estimator output to a belief logic representation for the modular integration of supervised methods with other trust assessment methodologies. © 2013 IEEE.","2013","2021-11-25 02:26:28","2022-12-20 05:06:59","2021-11-25","525-534","","","","","","","","","","","","","","","","","","","","7 citations (Crossref) [2022-12-20] ISBN: 9780769550220 QID: Q58947264","","","","","machine learning; supervised prediction; trust models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8JIKCTNJ","journalArticle","2021","Ehsan, Upol; Wintersberger, Philipp; Liao, Q. Vera; Mara, Martina; Streit, Marc; Wachter, Sandra; Riener, Andreas; Riedl, Mark O.","Operationalizing Human-Centered Perspectives in Explainable AI","Conference on Human Factors in Computing Systems - Proceedings","","","10.1145/3411763.3441342","","The realm of Artificial Intelligence (AI)'s impact on our lives is far reaching - with AI systems proliferating high-stakes domains such as healthcare, finance, mobility, law, etc., these systems must be able to explain their decision to diverse end-users comprehensibly. Yet the discourse of Explainable AI (XAI) has been predominantly focused on algorithm-centered approaches, suffering from gaps in meeting user needs and exacerbating issues of algorithmic opacity. To address these issues, researchers have called for human-centered approaches to XAI. There is a need to chart the domain and shape the discourse of XAI with reflective discussions from diverse stakeholders. The goal of this workshop is to examine how human-centered perspectives in XAI can be operationalized at the conceptual, methodological, and technical levels. Encouraging holistic (historical, sociological, and technical) approaches, we put an emphasis on ""operationalizing"", aiming to produce actionable frameworks, transferable evaluation methods, concrete design guidelines, and articulate a coordinated research agenda for XAI.","2021-05-08","2022-07-04 23:54:37","2022-12-20 05:06:57","2022-07-05","","","","","","","","","","","","","","","","","","","","","9 citations (Crossref) [2022-12-20] Publisher: Association for Computing Machinery ISBN: 9781450380959","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7WFVIIUP","journalArticle","2015","Goldstein, Alex; Kapelner, Adam; Bleich, Justin; Pitkin, Emil","Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation","http://dx.doi.org/10.1080/10618600.2014.907095","","15372715","10.1080/10618600.2014.907095","https://www.tandfonline.com/doi/abs/10.1080/10618600.2014.907095","This article presents individual conditional expectation (ICE) plots, a tool for visualizing the model estimated by any supervised learning algorithm. Classical partial dependence plots (PDPs) help...","2015-01-02","2022-01-14 03:18:14","2022-12-20 05:06:56","2022-01-14","44-65","","1","24","","","","","","","","","","","","","","","","","485 citations (Crossref) [2022-12-20] arXiv: 1309.6392 Publisher: Taylor & Francis QID: Q102362868","","","","","Exploratory data analysis; Graphical method; Model visualization","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JCSH3UPV","journalArticle","2020","Hong, Chang Woo; Lee, Changmin; Lee, Kwangsuk; Ko, Min-Seung; Kim, Dae Eun; Hur, Kyeon","Remaining Useful Life Prognosis for Turbofan Engine Using Explainable Deep Neural Networks with Dimensionality Reduction","Sensors 2020, Vol. 20, Page 6626","","","10.3390/S20226626","https://www.mdpi.com/1424-8220/20/22/6626/htm","This study prognoses the remaining useful life of a turbofan engine using a deep learning model, which is essential for the health management of an engine. The proposed deep learning model affords a significantly improved accuracy by organizing networks with a one-dimensional convolutional neural network, long short-term memory, and bidirectional long short-term memory. In particular, this paper investigates two practical and crucial issues in applying the deep learning model for system prognosis. The first is the requirement of numerous sensors for different components, i.e., the curse of dimensionality. Second, the deep neural network cannot identify the problematic component of the turbofan engine due to its &ldquo;black box&rdquo; property. This study thus employs dimensionality reduction and Shapley additive explanation (SHAP) techniques. Dimensionality reduction in the model reduces the complexity and prevents overfitting, while maintaining high accuracy. SHAP analyzes and visualizes the black box to identify the sensors. The experimental results demonstrate the high accuracy and efficiency of the proposed model with dimensionality reduction and show that SHAP enhances the explainability in a conventional deep learning model for system prognosis.","2020-11-19","2021-10-07 01:22:53","2022-12-20 05:06:54","2021-10-07","6626","","22","20","","","","","","","","","","","","","","","","","13 citations (Crossref) [2022-12-20] Publisher: Multidisciplinary Digital Publishing Institute QID: Q102332776","","; C:\Users\ambreen.hanif\Zotero\storage\ESLNWV34\Hong et al_2020_Remaining Useful Life Prognosis for Turbofan Engine Using Explainable Deep.pdf","https://www.mdpi.com/1424-8220/20/22/6626","","explainable artificial intelligence; deep neural network; dimensionality reduction; feature selection; prognostics and health monitoring; turbofan engine","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8N8RB5SA","journalArticle","2019","Kailkhura, Bhavya; Gallagher, Brian; Kim, Sookyung; Hiszpanski, Anna; Han, T. Yong-Jin","Reliable and explainable machine-learning methods for accelerated material discovery","npj Computational Materials 2019 5:1","","2057-3960","10.1038/s41524-019-0248-2","https://www.nature.com/articles/s41524-019-0248-2","Despite ML’s impressive performance in commercial applications, several unique challenges exist when applying ML in materials science applications. In such a context, the contributions of this work are twofold. First, we identify common pitfalls of existing ML techniques when learning from underrepresented/imbalanced material data. Specifically, we show that with imbalanced data, standard methods for assessing quality of ML models break down and lead to misleading conclusions. Furthermore, we find that the model’s own confidence score cannot be trusted and model introspection methods (using simpler models) do not help as they result in loss of predictive performance (reliability-explainability trade-off). Second, to overcome these challenges, we propose a general-purpose explainable and reliable machine-learning framework. Specifically, we propose a generic pipeline that employs an ensemble of simpler models to reliably predict material properties. We also propose a transfer learning technique and show that the performance loss due to models’ simplicity can be overcome by exploiting correlations among different material properties. A new evaluation metric and a trust score to better quantify the confidence in the predictions are also proposed. To improve the interpretability, we add a rationale generator component to our framework which provides both model-level and decision-level explanations. Finally, we demonstrate the versatility of our technique on two applications: (1) predicting properties of crystalline compounds and (2) identifying potentially stable solar cell materials. We also point to some outstanding issues yet to be resolved for a successful application of ML in material science.","2019-11-14","2021-09-27 04:15:45","2022-12-20 05:06:54","2021-09-27","1-9","","1","5","","","","","","","","","","","","","","","","","75 citations (Crossref) [2022-12-20] Publisher: Nature Publishing Group QID: Q84956741","","C:\Users\ambreen.hanif\Zotero\storage\5HLT7TJ2\Kailkhura et al_2019_Reliable and explainable machine-learning methods for accelerated material.pdf","","","Computational methods; Design; synthesis and processing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2X85B22N","journalArticle","2012","Domingos, Pedro","review articles Tapping into the ""folk knowledge"" needed to advance machine learning applications","","","","10.1145/2347736.2347755","","Machine learning systeMs automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. 15 Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell 16 and Witten et al. 24). However, much of the ""folk knowledge"" that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article. a few useful things to Know about machine Learning key insights machine learning algorithms can figure out how to perform important tasks by generalizing from examples. this is often feasible and cost-effective where manual programming is not. as more data becomes available, more ambitious problems can be tackled. machine learning is widely used in computer science and other fields. however, developing successful machine learning applications requires a substantial amount of ""black art"" that is difficult to find in textbooks. this article summarizes 12 key lessons that machine learning researchers and practitioners have learned. these include pitfalls to avoid, important issues to focus on, and answers to common questions.","2012","2021-03-26 14:22:38","2022-12-20 05:06:53","2021-03-27","","","10","55","","","","","","","","","","","","","","","","","1464 citations (Crossref) [2022-12-20] QID: Q112782279","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"372INHFS","journalArticle","2021","Hada, Deepesh V.; M., Vijaikumar; Shevade, Shirish K.","ReXPlug: Explainable Recommendation using Plug-and-Play Language Model","SIGIR 2021 - Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval","","","10.1145/3404835.3462939","","Explainable Recommendations provide the reasons behind why an item is recommended to a user, which often leads to increased user satisfaction and persuasiveness. An intuitive way to explain recommendations is by generating a synthetic personalized natural language review for a user-item pair. Although there exist some approaches in the literature that explain recommendations by generating reviews, the quality of the reviews is questionable. Besides, these methods usually take considerable time to train the underlying language model responsible for generating the text. In this work, we propose ReXPlug, an end-to-end framework with a plug and play way of explaining recommendations. ReXPlug predicts accurate ratings as well as exploits Plug and Play Language Model to generate high-quality reviews. We train a simple sentiment classifier for controlling a pre-trained language model for the generation, bypassing the language model's training from scratch again. Such a simple and neat model is much easier to implement and train, and hence, very efficient for generating reviews. We personalize the reviews by leveraging a special jointly-trained cross attention network. Our detailed experiments show that ReXPlug outperforms many recent models across various datasets on rating prediction by utilizing textual reviews as a regularizer. Quantitative analysis shows that the reviews generated by ReXPlug are semantically close to the ground truth reviews, while the qualitative analysis demonstrates the high quality of the generated reviews, both from empirical and analytical viewpoints. Our implementation is available online.","2021-07-11","2022-03-27 19:59:02","2022-12-20 05:06:52","2022-03-28","81-91","","","","","","","","","","","","","","","","","","","","2 citations (Crossref) [2022-12-20] Publisher: Association for Computing Machinery, Inc ISBN: 9781450380379","","","","","collaborative filtering; attention; natural language generation; neural networks; personalization; recommender systems; transfer learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z9SGHN7Q","journalArticle","2018","Christy, A. Joy; Umamakeswari, A.; Priyatharsini, L.; Neyaa, A.","RFM ranking – An effective approach to customer segmentation","Journal of King Saud University - Computer and Information Sciences","","22131248","10.1016/j.jksuci.2018.09.004","","The efficient segmentation of customers of an enterprise is categorized into groups of similar behavior based on the RFM (Recency, Frequency and Monetary) values of the customers. The transactional data of a company over is analyzed over a specific period. Segmentation gives a good understanding of the need of the customers and helps in identifying the potential customers of the company. Dividing the customers into segments also increases the revenue of the company. It is believed that retaining the customers is more important than finding new customers. For instance, the company can deploy marketing strategies that are specific to an individual segment to retain the customers. This study initially performs an RFM analysis on the transactional data and then extends to cluster the same using traditional K-means and Fuzzy C- Means algorithms. In this paper, a novel idea for choosing the initial centroids in K- Means is proposed. The results obtained from the methodologies are compared with one another by their iterations, cluster compactness and execution time.","2018-09-05","2021-03-15 18:00:45","2022-12-20 05:06:51","2021-03-16","","","","","","","","","","","","","","","","","","","","","26 citations (Crossref) [2022-12-20] Publisher: King Saud bin Abdulaziz University","","C:\Users\ambreen.hanif\Zotero\storage\QKELYQQD\Christy et al_2018_RFM ranking – An effective approach to customer segmentation.pdf","","","Customer segmentation; RFM analysis; Fuzzy C-Means; Initial centroids; K-Means","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M2YJK4PV","journalArticle","2020","Ferreira, Leonardo Augusto; Guimarães, Frederico Gadelha; Silva, Rodrigo","Applying Genetic Programming to Improve Interpretability in Machine Learning Models","","","","10.48550/arXiv.2005.09512","https://arxiv.org/abs/2005.09512v1","Explainable Artificial Intelligence (or xAI) has become an important research topic in the fields of Machine Learning and Deep Learning. In this paper, we propose a Genetic Programming (GP) based approach, named Genetic Programming Explainer (GPX), to the problem of explaining decisions computed by AI systems. The method generates a noise set located in the neighborhood of the point of interest, whose prediction should be explained, and fits a local explanation model for the analyzed sample. The tree structure generated by GPX provides a comprehensible analytical, possibly non-linear, symbolic expression which reflects the local behavior of the complex model. We considered three machine learning techniques that can be recognized as complex black-box models: Random Forest, Deep Neural Network and Support Vector Machine in twenty data sets for regression and classifications problems. Our results indicate that the GPX is able to produce more accurate understanding of complex models than the state of the art. The results validate the proposed approach as a novel way to deploy GP to improve interpretability.","2020-05-18","2022-08-05 01:05:51","2023-05-23 01:22:30","2022-08-05 01:05:51","","","","","","","","","","","","","","en","","","","","arxiv.org","","12 citations (Semantic Scholar/arXiv) [2022-12-20]","","C:\Users\ambreen.hanif\Zotero\storage\BQCVFQ5U\Ferreira et al_2020_Applying Genetic Programming to Improve Interpretability in Machine Learning.pdf; ; C:\Users\ambreen.hanif\Zotero\storage\38JPVWL3\2005.html","notion://www.notion.so/Ferreira-et-al-2020-bab8b9d04e5a4b7caf2044260220bfca","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KNYD2T7D","journalArticle","","Mei, Dr Yi","Towards Better Explainable AI Through Genetic Programming","","","","","","","","2022-08-05 00:48:41","2023-05-23 01:22:28","","113","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\XGRV3QD5\Mei - Towards Better Explainable AI Through Genetic Prog.pdf; ","notion://www.notion.so/Mei-n-d-a02eb526f39a40a99aace24f4de8b634","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U8FV3PTQ","book","2009","Farrugia, Michael; Quigley, Aaron","TGD: Visual data exploration of temporal graph data","","","","","","This paper describes the social networking questions, analysis, design and approach taken in the realisation of an interactive solution for the VAST 2008 challenge. The solution presented is a case study in this approach and won the phone call mini challenge award. The problem scenario of the competition is used as a case study to explain the approach and experience with the developed tool. Design considerations and observations on the process used are drawn and suggestions on further research in the area of temporal graph data are made.","2009-01-18","2022-08-05 00:12:31","2023-05-23 01:22:24","","","724309","","7243","","","TGD","","","","","","","","","","","","ResearchGate","","DOI: 10.1117/12.814921","","C:\Users\ambreen.hanif\Zotero\storage\XY93XN5N\Farrugia_Quigley_2009_TGD.pdf; ; ","notion://www.notion.so/Farrugia-Quigley-2009-8cb083da3c96472fb0b547e352db7948; https://www.researchgate.net/publication/220836078_TGD_Visual_data_exploration_of_temporal_graph_data","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DSL84BIC","journalArticle","2021","Wells, Lindsay; Bednarz, Tomasz","Explainable AI and Reinforcement Learning—A Systematic Review of Current Approaches and Trends","Frontiers in Artificial Intelligence","","2624-8212","10.3389/frai.2021.550030","https://www.frontiersin.org/articles/10.3389/frai.2021.550030","Research into Explainable Artificial Intelligence (XAI) has been increasing in recent years as a response to the need for increased transparency and trust in AI. This is particularly important as AI is used in sensitive domains with societal, ethical, and safety implications. Work in XAI has primarily focused on Machine Learning (ML) for classification, decision, or action, with detailed systematic reviews already undertaken. This review looks to explore current approaches and limitations for XAI in the area of Reinforcement Learning (RL). From 520 search results, 25 studies (including 5 snowball sampled) are reviewed, highlighting visualization, query-based explanations, policy summarization, human-in-the-loop collaboration, and verification as trends in this area. Limitations in the studies are presented, particularly a lack of user studies, and the prevalence of toy-examples and difficulties providing understandable explanations. Areas for future study are identified, including immersive visualization, and symbolic representation.","2021","2022-08-05 00:08:48","2023-05-23 01:22:21","2022-08-05 00:08:48","550030","","","4","","","","","","","","","","","","","","","Frontiers","","30 citations (Semantic Scholar/DOI) [2022-12-20] 18 citations (Crossref) [2022-12-20]","","; C:\Users\ambreen.hanif\Zotero\storage\R2MYYZF6\Wells_Bednarz_2021_Explainable AI and Reinforcement Learning—A Systematic Review of Current.pdf","notion://www.notion.so/Wells-Bednarz-2021-dfb671e1508a4f67a5a7c9fbf8a9ef8d","notion; read; xai; reinforcement learning; survey","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3IVIRDM8","preprint","2020","Vilone, Giulia; Longo, Luca","Explainable Artificial Intelligence: a Systematic Review","","","","","http://arxiv.org/abs/2006.00093","Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions.","2020-10-12","2022-08-04 23:57:17","2023-05-23 01:22:19","2022-08-04 23:57:17","","","","","","","Explainable Artificial Intelligence","","","","","arXiv","","","","","","","arXiv.org","","110 citations (Semantic Scholar/arXiv) [2022-12-20] arXiv:2006.00093 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\4T8HZVE8\2006.html; ; C:\Users\ambreen.hanif\Zotero\storage\WFR99IXC\Vilone_Longo_2020_Explainable Artificial Intelligence.pdf","notion://www.notion.so/Vilone-Longo-2020-72216d586577411dacf265e9b528cc75","notion","","","","","","","","","","","","","","","","","","","","arXiv:2006.00093","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QNNKYIL8","conferencePaper","2017","Klein, Guillaume; Kim, Yoon; Deng, Yuntian; Senellart, Jean; Rush, Alexander","OpenNMT: Open-Source Toolkit for Neural Machine Translation","Proceedings of ACL 2017, System Demonstrations","","","10.18653/v1/P17-4012","http://aclweb.org/anthology/P17-4012","","2017","2022-08-03 05:27:28","2023-05-23 01:22:18","2022-08-03 05:27:27","67-72","","","","","","OpenNMT","","","","","Association for Computational Linguistics","Vancouver, Canada","en","","","","","DOI.org (Crossref)","","1491 citations (Semantic Scholar/DOI) [2022-12-20] 343 citations (Crossref) [2022-12-20]","","C:\Users\ambreen.hanif\Zotero\storage\TIWRAZBC\Klein et al_2017_OpenNMT.pdf; ","notion://www.notion.so/Klein-et-al-2017-42d8796707b6434185cf52455928f014","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of ACL 2017, System Demonstrations","","","","","","","","","","","","","","",""
"2X5BFAQA","journalArticle","","Yuan, Hao; Yu, Haiyang; Gui, Shurui; Ji, Shuiwang","Explainability in Graph Neural Networks: A Taxonomic Survey","","","","10.1109/TPAMI.2022.3204236","","Deep learning methods are achieving ever-increasing performance on many artiﬁcial intelligence tasks. A major limitation of deep models is that they are not amenable to interpretability. This limitation can be circumvented by developing post hoc techniques to explain the predictions, giving rise to the area of explainability. Recently, explainability of deep models on images and texts has achieved signiﬁcant progress. In the area of graph data, graph neural networks (GNNs) and their explainability are experiencing rapid developments. However, there is neither a uniﬁed treatment of GNN explainability methods, nor a standard benchmark and testbed for evaluations. In this survey, we provide a uniﬁed and taxonomic view of current GNN explainability methods. Our uniﬁed and taxonomic treatments of this subject shed lights on the commonalities and differences of existing methods and set the stage for further methodological developments. To facilitate evaluations, we generate a set of benchmark graph datasets speciﬁcally for GNN explainability. We summarize current datasets and metrics for evaluating GNN explainability. Altogether, this work provides a uniﬁed methodological treatment of GNN explainability and a standardized testbed for evaluations.","","2022-08-02 00:42:36","2023-05-23 01:22:16","","14","","","","","","","","","","","","","en","","","","","Zotero","","136 citations (Semantic Scholar/DOI) [2022-12-20] 6 citations (Crossref) [2022-12-20]","","; C:\Users\ambreen.hanif\Zotero\storage\P6FEFS9J\Yuan et al. - Explainability in Graph Neural Networks A Taxonom.pdf","notion://www.notion.so/Yuan-et-al-n-d-36fc7103cc684e209ee8cb5621755aa9","notion; XAI; survey; GNN","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ERM4Y5PX","webpage","2020","","Explainability in Graph Neural Networks: A Taxonomic Survey","DeepAI","","","","https://deepai.org/publication/explainability-in-graph-neural-networks-a-taxonomic-survey","12/31/20 - Deep learning methods are achieving ever-increasing performance on many artificial intelligence tasks. A major limitation of deep ...","2020-12-31","2022-08-02 00:42:34","2023-05-23 01:22:15","2022-08-02 00:42:34","","","","","","","Explainability in Graph Neural Networks","","","","","","","","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\GHE2H385\explainability-in-graph-neural-networks-a-taxonomic-survey.html","notion://www.notion.so/Explainability-in-Graph-Neural-Networks-2020-219b1f2ae4b24fc695b5df6a27cd1b57","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D75X54RN","webpage","","","Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) – Jay Alammar – Visualizing machine learning one concept at a time.","","","","","https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/","","","2022-08-01 06:46:08","2023-05-23 01:22:13","2022-08-01 06:46:08","","","","","","","","","","","","","","","","","","","","","","","","notion://www.notion.so/Visualizing-A-Neural-Machine-Translation-Model-Mechanics-of-Seq2seq-Models-With-Attention-Jay-Al-8d1a1bbe7fa745af8e3a8682c83f0c3e","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TR9SBQFZ","conferencePaper","2010","Gutmann, Michael; Hyvärinen, Aapo","Noise-contrastive estimation: A new estimation principle for unnormalized statistical models","Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics","","","","https://proceedings.mlr.press/v9/gutmann10a.html","We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity.  We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance.  In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.","2010-03-31","2022-07-25 07:03:09","2023-05-23 01:22:10","2022-07-25 07:03:09","297-304","","","","","","Noise-contrastive estimation","","","","","JMLR Workshop and Conference Proceedings","","en","","","","","proceedings.mlr.press","","ISSN: 1938-7228","","C:\Users\ambreen.hanif\Zotero\storage\YJABTZHJ\Gutmann_Hyvärinen_2010_Noise-contrastive estimation.pdf; ","notion://www.notion.so/Gutmann-Hyv-rinen-2010-cfd224b560a548868ffb7686b2f6e8c2","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics","","","","","","","","","","","","","","",""
"VWNRI8TW","bookSection","2022","Badami, Maisie; Benatallah, Boualem; Baez, Marcos","Systematic Literature Review Search Query Refinement Pipeline: Incremental Enrichment and Adaptation","Advanced Information Systems Engineering","978-3-031-07471-4 978-3-031-07472-1","","","https://link.springer.com/10.1007/978-3-031-07472-1_8","Systematic literature reviews (SLRs) are at the heart of evidence-based research, collecting and integrating empirical evidence regarding speciﬁc research questions. A leading step in the search for relevant evidence is composing Boolean search queries, which are still at the core of how information retrieval systems work to perform an advanced literature search. Building these queries thus requires going from the general aims of the research questions into actionable search terms that are combined into potentially complex Boolean expressions. Researchers are thus tasked with the daunting and challenging task of building and reﬁning search queries in their quest for suﬃcient coverage and proper representation of the literature. In this paper, we propose an adaptive Boolean query generation and reﬁnement pipeline for SLR search. Our approach utilizes a reinforcement learning technique to learn the optimal modiﬁcations for a query based on the feedback collecting from the researchers about the query retrieval performance. Empirical evaluations with 10 SLR datasets showed our approach to achieve comparable performance to that of queries manually composed by SLR authors.","2022","2022-07-12 05:38:49","2023-05-23 01:22:07","2022-07-12 05:38:48","129-146","","","13295","","","Systematic Literature Review Search Query Refinement Pipeline","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-031-07472-1_8","","C:\Users\ambreen.hanif\Zotero\storage\BWGP6IAS\Badami et al. - 2022 - Systematic Literature Review Search Query Refineme.pdf; ","notion://www.notion.so/Badami-et-al-2022-7387ca42981e4582bb73d8e7faae5dd2","notion","","Franch, Xavier; Poels, Geert; Gailly, Frederik; Snoeck, Monique","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9E7LW4F9","journalArticle","2017","Beheshti, Seyed-Mehdi-Reza; Benatallah, Boualem; Venugopal, Srikumar; Ryu, Seung Hwan; Motahari-Nezhad, Hamid Reza; Wang, Wei","A systematic review and comparative analysis of cross-document coreference resolution methods and tools","Computing","","0010-485X, 1436-5057","10.1007/s00607-016-0490-0","http://link.springer.com/10.1007/s00607-016-0490-0","","2017-04","2022-07-12 05:35:07","2023-05-23 01:22:04","2022-07-12 05:35:07","313-349","","4","99","","Computing","","","","","","","","en","","","","","DOI.org (Crossref)","","46 citations (Semantic Scholar/DOI) [2022-12-20] 26 citations (Crossref) [2022-12-20]","","; C:\Users\ambreen.hanif\Zotero\storage\XKMZEPLY\s00607-016-0490-0.pdf","notion://www.notion.so/Beheshti-et-al-2017-66753bfdb4984214be09f3d5af70abb7","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KBTSFLHW","journalArticle","2022","Zytek, Alexandra; Arnaldo, Ignacio; Liu, Dongyu; Berti-Equille, Laure; Veeramachaneni, Kalyan","The Need for Interpretable Features: Motivation and Taxonomy","arXiv preprint arXiv:2202.11748","","","","","Through extensive experience developing and explaining machine learning (ML) applications for real-world domains, we have learned that ML models are only as interpretable as their features. Even simple, highly interpretable model types such as regression models can be diﬃcult or impossible to understand if they use uninterpretable features. Diﬀerent users, especially those using ML models for decision-making in their domains, may require diﬀerent levels and types of feature interpretability. Furthermore, based on our experiences, we claim that the term “interpretable feature” is not speciﬁc nor detailed enough to capture the full extent to which features impact the usefulness of ML explanations. In this paper, we motivate and discuss three key lessons: 1) more attention should be given to what we refer to as the interpretable feature space, or the state of features that are useful to domain experts taking real-world actions, 2) a formal taxonomy is needed of the feature properties that may be required by these domain experts (we propose a partial taxonomy in this paper), and 3) transforms that take data from the model-ready state to an interpretable form are just as essential as traditional ML transforms that prepare features for the model.","2022","2022-07-11 03:05:25","2023-05-23 01:22:03","","","","","","","","The Need for Interpretable Features","","","","","","","","","","","","Google Scholar","","","","; C:\Users\ambreen.hanif\Zotero\storage\E5GXC9MC\2202.html; C:\Users\ambreen.hanif\Zotero\storage\IJXGUW4N\Zytek et al_2022_The Need for Interpretable Features.pdf","notion://www.notion.so/Zytek-et-al-2022-410a4dbdcab94ef8a5ccde66eb560e3c","notion; Interpretable Features","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6DZQV7KC","journalArticle","2019","Du, Mengnan; Liu, Ninghao; Hu, Xia","Techniques for Interpretable Machine Learning","Communications of the ACM","","","10.48550/arXiv.1808.00033","http://arxiv.org/abs/1808.00033","Interpretable machine learning tackles the important problem that humans cannot understand the behaviors of complex machine learning models and how these models arrive at a particular decision. Although many approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. We provide a survey covering existing techniques to increase the interpretability of machine learning models. We also discuss crucial issues that the community should consider in future work such as designing user-friendly explanations and developing comprehensive evaluation metrics to further push forward the area of interpretable machine learning.","2019-05-19","2022-07-07 05:55:34","2023-05-23 01:22:00","2022-07-07 05:55:34","68-77","","1","63","","","","","","","","","","","","","","","arXiv.org","","arXiv:1808.00033 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\NXND6HF7\1808.html; C:\Users\ambreen.hanif\Zotero\storage\D4XDPQQQ\Du et al_2019_Techniques for Interpretable Machine Learning.pdf; ; C:\Users\ambreen.hanif\Zotero\storage\LYL67YWH\3359786.html","notion://www.notion.so/Du-et-al-2019-6745ceb23dc143cfab2f207bb0555e61","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q7IYWA25","journalArticle","2021","Antoniadi, Anna Markella; Du, Yuhan; Guendouz, Yasmine; Wei, Lan; Mazo, Claudia; Becker, Brett A.; Mooney, Catherine","Current Challenges and Future Opportunities for XAI in Machine Learning-Based Clinical Decision Support Systems: A Systematic Review","Applied Sciences 2021, Vol. 11, Page 5088","","2076-3417","10.3390/APP11115088","https://www.mdpi.com/2076-3417/11/11/5088/htm","Machine Learning and Artificial Intelligence (AI) more broadly have great immediate and future potential for transforming almost all aspects of medicine. However, in many applications, even outside medicine, a lack of transparency in AI applications has become increasingly problematic. This is particularly pronounced where users need to interpret the output of AI systems. Explainable AI (XAI) provides a rationale that allows users to understand why a system has produced a given output. The output can then be interpreted within a given context. One area that is in great need of XAI is that of Clinical Decision Support Systems (CDSSs). These systems support medical practitioners in their clinic decision-making and in the absence of explainability may lead to issues of under or over-reliance. Providing explanations for how recommendations are arrived at will allow practitioners to make more nuanced, and in some cases, life-saving decisions. The need for XAI in CDSS, and the medical field in general, is amplified by the need for ethical and fair decision-making and the fact that AI trained with historical data can be a reinforcement agent of historical actions and biases that should be uncovered. We performed a systematic literature review of work to-date in the application of XAI in CDSS. Tabular data processing XAI-enabled systems are the most common, while XAI-enabled CDSS for text analysis are the least common in literature. There is more interest in developers for the provision of local explanations, while there was almost a balance between post-hoc and ante-hoc explanations, as well as between model-specific and model-agnostic techniques. Studies reported benefits of the use of XAI such as the fact that it could enhance decision confidence for clinicians, or generate the hypothesis about causality, which ultimately leads to increased trustworthiness and acceptability of the system and potential for its incorporation in the clinical workflow. However, we found an overall distinct lack of application of XAI in the context of CDSS and, in particular, a lack of user studies exploring the needs of clinicians. We propose some guidelines for the implementation of XAI in CDSS and explore some opportunities, challenges, and future research needs.","2021-05-31","2022-07-05 00:07:16","2023-05-23 01:21:58","2022-07-05","5088","","11","11","","","","","","","","","","","","","","","","","52 citations (Crossref) [2022-12-20] Publisher: Multidisciplinary Digital Publishing Institute","","; C:\Users\ambreen.hanif\Zotero\storage\3BWHZAZ4\Antoniadi et al_2021_Current Challenges and Future Opportunities for XAI in Machine Learning-Based.pdf; ; C:\Users\ambreen.hanif\Zotero\storage\4ZM7MHAR\5088.html","https://www.mdpi.com/2076-3417/11/11/5088; notion://www.notion.so/Antoniadi-et-al-2021-03866d74332f495e865edc869e98fccd","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QSMU59MI","journalArticle","2018","Pérez, Beatriz; Rubio, Julio; Sáenz-Adán, Carlos","A systematic review of provenance systems","Knowledge and Information Systems 2018 57:3","","0219-3116","10.1007/S10115-018-1164-3","https://link.springer.com/article/10.1007/s10115-018-1164-3","Provenance refers to the entire amount of information, comprising all the elements and their relationships, that contribute to the existence of a piece of data. The knowledge of provenance data allows a great number of benefits such as verifying a product, result reproductivity, sharing and reuse of knowledge, or assessing data quality and validity. With such tangible benefits, it is no wonder that in recent years, research on provenance has grown exponentially, and has been applied to a wide range of different scientific disciplines. Some years ago, managing and recording provenance information were performed manually. Given the huge volume of information available nowadays, the manual performance of such tasks is no longer an option. The problem of systematically performing tasks such as the understanding, capture and management of provenance has gained significant attention by the research community and industry over the past decades. As a consequence, there has been a huge amount of contributions and proposed provenance systems as solutions for performing such kinds of tasks. The overall objective of this paper is to plot the landscape of published systems in the field of provenance, with two main purposes. First, we seek to evaluate the desired characteristics that provenance systems are expected to have. Second, we aim at identifying a set of representative systems (both early and recent use) to be exhaustively analyzed according to such characteristics. In particular, we have performed a systematic literature review of studies, identifying a comprehensive set of 105 relevant resources in all. The results show that there are common aspects or characteristics of provenance systems thoroughly renowned throughout the literature on the topic. Based on these results, we have defined a six-dimensional taxonomy of provenance characteristics attending to: general aspects, data capture, data access, subject, storage, and non-functional aspects. Additionally, the study has found that there are 25 most referenced provenance systems within the provenance context. This study exhaustively analyzes and compares such systems attending to our taxonomy and pinpoints future directions.","2018-02-17","2022-05-04 00:03:16","2023-05-23 01:21:53","2022-05-04","495-543","","3","57","","","","","","","","","","","","","","","","","23 citations (Crossref) [2022-12-20] Publisher: Springer","","; C:\Users\ambreen.hanif\Zotero\storage\CPXCUBXL\Pérez et al_2018_A systematic review of provenance systems.pdf; ","notion://www.notion.so/P-rez-et-al-2018-27cc919910b64b3d994200c7c285eb28; https://www.researchgate.net/publication/323242431_A_systematic_review_of_provenance_systems","notion","Information Systems and Communication Service; Data Mining and Knowledge Discovery; Database Management; Information Storage and Retrieval; Information Systems Applications (incl.Internet); IT in Business","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TGRSSNQA","journalArticle","2018","Greenwell, Brandon M.; Boehmke, Bradley C.; McCarthy, Andrew J.","A Simple and Effective Model-Based Variable Importance Measure","","","","","https://arxiv.org/abs/1805.04755v1","In the era of ""big data"", it is becoming more of a challenge to not only build state-of-the-art predictive models, but also gain an understanding of what's really going on in the data. For example, it is often of interest to know which, if any, of the predictors in a fitted model are relatively influential on the predicted outcome. Some modern algorithms---like random forests and gradient boosted decision trees---have a natural way of quantifying the importance or relative influence of each feature. Other algorithms---like naive Bayes classifiers and support vector machines---are not capable of doing so and model-free approaches are generally used to measure each predictor's importance. In this paper, we propose a standardized, model-based approach to measuring predictor importance across the growing spectrum of supervised learning algorithms. Our proposed method is illustrated through both simulated and real data examples. The R code to reproduce all of the figures in this paper is available in the supplementary materials.","2018-05-12","2022-01-06 05:29:41","2023-05-23 01:21:52","2022-01-06","","","","","","","","","","","","","","","","","","","","","126 citations (Semantic Scholar/arXiv) [2022-12-20] arXiv: 1805.04755","","C:\Users\ambreen.hanif\Zotero\storage\RT7UAWMA\Greenwell et al_2018_A Simple and Effective Model-Based Variable Importance Measure.pdf; ","notion://www.notion.so/Greenwell-et-al-2018-6a08f61101cf43eb980da9feafbb1134","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LKYKKQK7","webpage","","","[2206.00474] Towards Responsible AI: A Design Space Exploration of Human-Centered Artificial Intelligence User Interfaces to Investigate Fairness","","","","","https://arxiv.org/abs/2206.00474","","","2023-05-23 01:21:11","2023-05-23 01:21:25","2023-05-23 01:21:11","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\YRED6B4F\2206.html; ","notion://www.notion.so/2206-00474-Towards-Responsible-AI-A-Design-Space-Exploration-of-Human-Centered-Artificial-Intellig-2309adb5c2ca4df0afca42b98413a5e0","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IT5LIKZV","journalArticle","2023","Nabavi, Ehsan; Browne, Chris","Leverage zones in Responsible AI: towards a systems thinking conceptualization","Humanities and Social Sciences Communications","","2662-9992","10.1057/s41599-023-01579-0","https://www.nature.com/articles/s41599-023-01579-0","There is a growing debate amongst academics and practitioners on whether interventions made, thus far, towards Responsible AI have been enough to engage with the root causes of AI problems. Failure to effect meaningful changes in this system could see these initiatives not reach their potential and lead to the concept becoming another buzzword for companies to use in their marketing campaigns. Systems thinking is often touted as a methodology to manage and effect change; however, there is little practical advice available for decision-makers to include systems thinking insights to work towards Responsible AI. Using the notion of ‘leverage zones’ adapted from the systems thinking literature, we suggest a novel approach to plan for and experiment with potential initiatives and interventions. This paper presents a conceptual framework called the Five Ps to help practitioners construct and identify holistic interventions that may work towards Responsible AI, from lower-order interventions such as short-term fixes, tweaking algorithms and updating parameters, through to higher-order interventions such as redefining the system’s foundational structures that govern those parameters, or challenging the underlying purpose upon which those structures are built and developed in the first place. Finally, we reflect on the framework as a scaffold for transdisciplinary question-asking to improve outcomes towards Responsible AI.","2023-03-04","2023-05-22 23:52:40","2023-05-22 23:52:42","2023-05-22 23:52:40","1-9","","1","10","","Humanit Soc Sci Commun","Leverage zones in Responsible AI","","","","","","","en","2023 The Author(s)","","","","www-nature-com.simsrad.net.ocs.mq.edu.au","","0 citations (Crossref) [2023-05-23] Number: 1 Publisher: Palgrave","","C:\Users\ambreen.hanif\Zotero\storage\DRBBXMEK\Nabavi_Browne_2023_Leverage zones in Responsible AI.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P3PHFPV3","webpage","","","Towards Responsible AI for Financial Transactions","","","","","https://ieeexplore.ieee.org/document/9308456","The application of AI in finance is increasingly dependent on the principles of responsible AI. These principles-explainability, fairness, privacy, accountability, transparency and soundness form the basis for trust in future AI systems. In this empirical study, we address the first principle by providing an explanation for a deep neural nenvork that is trained on a mixture of numerical, categorical and textual inputs for financial transaction classification. The explanation is achieved through (1) a feature importance analysis using Shapley additive explanations (SHAP) and (2) a hybrid approach of text clustering and decision tree classifiers. We then test the robustness of the model by exposing it to a targeted evasion attack, leveraging the knowledge we gained about the model through the extracted explanation.","","2023-05-22 23:52:23","2023-05-22 23:52:23","2023-05-22 23:52:23","","","","","","","","","","","","","","en-US","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\UL5XZA2J\9308456.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"95TG77MD","webpage","","","Towards Responsible AI for Financial Transactions | IEEE Conference Publication | IEEE Xplore","","","","","https://ieeexplore-ieee-org.simsrad.net.ocs.mq.edu.au/document/9308456","","","2023-05-22 23:17:14","2023-05-22 23:17:14","2023-05-22 23:17:14","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\9MZESEX3\9308456.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A2JCH3PG","journalArticle","2019","Jobin, Anna; Ienca, Marcello; Vayena, Effy","The global landscape of AI ethics guidelines","Nature Machine Intelligence","","2522-5839","10.1038/s42256-019-0088-2","https://www.nature.com/articles/s42256-019-0088-2","In the past five years, private companies, research institutions and public sector organizations have issued principles and guidelines for ethical artificial intelligence (AI). However, despite an apparent agreement that AI should be ‘ethical’, there is debate about both what constitutes ‘ethical AI’ and which ethical requirements, technical standards and best practices are needed for its realization. To investigate whether a global agreement on these questions is emerging, we mapped and analysed the current corpus of principles and guidelines on ethical AI. Our results reveal a global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy), with substantive divergence in relation to how these principles are interpreted, why they are deemed important, what issue, domain or actors they pertain to, and how they should be implemented. Our findings highlight the importance of integrating guideline-development efforts with substantive ethical analysis and adequate implementation strategies.","2019-09","2023-05-22 23:16:27","2023-05-22 23:16:30","2023-05-22 23:16:27","389-399","","9","1","","Nat Mach Intell","","","","","","","","en","2019 Springer Nature Limited","","","","www-nature-com.simsrad.net.ocs.mq.edu.au","","1030 citations (Crossref) [2023-05-23] Number: 9 Publisher: Nature Publishing Group","","C:\Users\ambreen.hanif\Zotero\storage\IJVKI2ID\Jobin et al_2019_The global landscape of AI ethics guidelines.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AD695FHA","webpage","","","The global landscape of AI ethics guidelines | Nature Machine Intelligence","","","","","https://www-nature-com.simsrad.net.ocs.mq.edu.au/articles/s42256-019-0088-2","","","2023-05-22 23:16:11","2023-05-22 23:16:11","2023-05-22 23:16:11","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\3A889GFR\s42256-019-0088-2.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YTV3AQH6","conferencePaper","2022","Lu, Qinghua; Zhu, Liming; Xu, Xiwei; Whittle, Jon; Douglas, David; Sanderson, Conrad","Software Engineering for Responsible AI: An Empirical Study and Operationalised Patterns","2022 IEEE/ACM 44th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)","","","10.1109/ICSE-SEIP55303.2022.9793864","http://arxiv.org/abs/2111.09478","Although artificial intelligence (AI) is solving real-world challenges and transforming industries, there are serious concerns about its ability to behave and make decisions in a responsible way. Many AI ethics principles and guidelines for responsible AI have been recently issued by governments, organisations, and enterprises. However, these AI ethics principles and guidelines are typically high-level and do not provide concrete guidance on how to design and develop responsible AI systems. To address this shortcoming, we first present an empirical study where we interviewed 21 scientists and engineers to understand the practitioners' perceptions on AI ethics principles and their implementation. We then propose a template that enables AI ethics principles to be operationalised in the form of concrete patterns and suggest a list of patterns using the newly created template. These patterns provide concrete, operationalised guidance that facilitate the development of responsible AI systems.","2022-05","2023-05-22 23:14:48","2023-05-22 23:14:56","2023-05-22 23:14:48","241-242","","","","","","Software Engineering for Responsible AI","","","","","","","","","","","","arXiv.org","","1 citations (Crossref) [2023-05-23] arXiv:2111.09478 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\36GL6D4M\2111.html; C:\Users\ambreen.hanif\Zotero\storage\N3IUD63Z\Lu et al_2022_Software Engineering for Responsible AI.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I5774NWF","preprint","2022","Lu, Qinghua; Zhu, Liming; Xu, Xiwei; Whittle, Jon","Responsible-AI-by-Design: a Pattern Collection for Designing Responsible AI Systems","","","","10.48550/arXiv.2203.00905","http://arxiv.org/abs/2203.00905","Although AI has significant potential to transform society, there are serious concerns about its ability to behave and make decisions responsibly. Many ethical regulations, principles, and guidelines for responsible AI have been issued recently. However, these principles are high-level and difficult to put into practice. In the meantime much effort has been put into responsible AI from the algorithm perspective, but they are limited to a small subset of ethical principles amenable to mathematical analysis. Responsible AI issues go beyond data and algorithms and are often at the system-level crosscutting many system components and the entire software engineering lifecycle. Based on the result of a systematic literature review, this paper identifies one missing element as the system-level guidance - how to design the architecture of responsible AI systems. We present a summary of design patterns that can be embedded into the AI systems as product features to contribute to responsible-AI-by-design.","2022-09-20","2023-05-22 23:14:46","2023-05-22 23:14:46","2023-05-22 23:14:44","","","","","","","Responsible-AI-by-Design","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2203.00905 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\Z3K4FRNP\2203.html; C:\Users\ambreen.hanif\Zotero\storage\JFS9GMFL\Lu et al_2022_Responsible-AI-by-Design.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2203.00905","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SQQ7QFY9","preprint","2022","Lu, Qinghua; Zhu, Liming; Xu, Xiwei; Whittle, Jon; Xing, Zhenchang","Towards a Roadmap on Software Engineering for Responsible AI","","","","10.48550/arXiv.2203.08594","http://arxiv.org/abs/2203.08594","Although AI is transforming the world, there are serious concerns about its ability to behave and make decisions responsibly. Many ethical regulations, principles, and frameworks for responsible AI have been issued recently. However, they are high level and difficult to put into practice. On the other hand, most AI researchers focus on algorithmic solutions, while the responsible AI challenges actually crosscut the entire engineering lifecycle and components of AI systems. To close the gap in operationalizing responsible AI, this paper aims to develop a roadmap on software engineering for responsible AI. The roadmap focuses on (i) establishing multi-level governance for responsible AI systems, (ii) setting up the development processes incorporating process-oriented practices for responsible AI systems, and (iii) building responsible-AI-by-design into AI systems through system-level architectural style, patterns and techniques.","2022-03-09","2023-05-22 23:14:43","2023-05-22 23:14:43","2023-05-22 23:14:43","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2203.08594 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\VFKYWGEM\2203.html; C:\Users\ambreen.hanif\Zotero\storage\GFJDNUV2\Lu et al_2022_Towards a Roadmap on Software Engineering for Responsible AI.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2203.08594","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P6Y322KS","preprint","2022","Lu, Qinghua; Zhu, Liming; Xu, Xiwei; Whittle, Jon; Zowghi, Didar; Jacquet, Aurelie","Responsible AI Pattern Catalogue: A Multivocal Literature Review","","","","","http://arxiv.org/abs/2209.04963","Responsible AI has been widely considered as one of the greatest scientific challenges of our time and the key to increase the adoption of AI. A number of AI ethics principles frameworks have been published recently. However, without further best practice guidance, practitioners are left with nothing much beyond truisms. Also, significant efforts have been placed at algorithm-level rather than system-level, mainly focusing on a subset of mathematics-amenable ethical principles (such as fairness). Nevertheless, ethical issues can occur at any step of the development lifecycle crosscutting many AI and non-AI components of systems beyond AI algorithms and models. To operationalize responsible AI from a system perspective, in this paper, we present a Responsible AI Pattern Catalogue based on the results of a Multivocal Literature Review (MLR). Rather than staying at the principle or algorithm level, we focus on patterns that AI system stakeholders can undertake in practice to ensure that the developed AI systems are responsible throughout the entire governance and engineering lifecycle. The Responsible AI Pattern Catalogue classifies the patterns into three groups: multi-level governance patterns, trustworthy process patterns, and responsible-AI-by-design product patterns. These patterns provide a systematic and actionable guidance for stakeholders to implement responsible AI.","2022-09-14","2023-05-22 23:14:39","2023-05-22 23:14:39","2023-05-22 23:14:39","","","","","","","Responsible AI Pattern Catalogue","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2209.04963 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\65AIFQXZ\2209.html; C:\Users\ambreen.hanif\Zotero\storage\IPUIUJY8\Lu et al_2022_Responsible AI Pattern Catalogue.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2209.04963","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QRKHL9UP","preprint","2021","Kokhlikyan, Narine; Miglani, Vivek; Alsallakh, Bilal; Martin, Miguel; Reblitz-Richardson, Orion","Investigating sanity checks for saliency maps with image and text classification","","","","10.48550/arXiv.2106.07475","http://arxiv.org/abs/2106.07475","Saliency maps have shown to be both useful and misleading for explaining model predictions especially in the context of images. In this paper, we perform sanity checks for text modality and show that the conclusions made for image do not directly transfer to text. We also analyze the effects of the input multiplier in certain saliency maps using similarity scores, max-sensitivity and infidelity evaluation metrics. Our observations reveal that the input multiplier carries input's structural patterns in explanation maps, thus leading to similar results regardless of the choice of model parameters. We also show that the smoothness of a Neural Network (NN) function can affect the quality of saliency-based explanations. Our investigations reveal that replacing ReLUs with Softplus and MaxPool with smoother variants such as LogSumExp (LSE) can lead to explanations that are more reliable based on the infidelity evaluation metric.","2021-06-08","2023-05-22 23:10:01","2023-05-22 23:10:01","2023-05-22 23:10:01","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2106.07475 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\KX39Z9BW\2106.html; C:\Users\ambreen.hanif\Zotero\storage\S85RVSW3\Kokhlikyan et al_2021_Investigating sanity checks for saliency maps with image and text classification.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2106.07475","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RP7ZH73I","webpage","","","[2206.04394] Xplique: A Deep Learning Explainability Toolbox","","","","","https://arxiv.org/abs/2206.04394","","","2023-05-22 23:08:14","2023-05-22 23:08:14","2023-05-22 23:08:14","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\55MXYZ68\2206.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J9EDAKBI","webpage","","","Explainable AI","Google Cloud","","","","https://cloud.google.com/explainable-ai","Deploy interpretable and inclusive machine learning models with Explainable AI, tools and frameworks designed to instill user trust.","","2023-05-22 13:03:06","2023-05-22 13:03:06","2023-05-22 13:03:06","","","","","","","","","","","","","","en","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\VFBXL9KP\explainable-ai.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PW857I6S","webpage","","","[2206.04394] Xplique: A Deep Learning Explainability Toolbox","","","","","https://arxiv.org/abs/2206.04394","","","2023-05-22 03:57:28","2023-05-22 03:57:28","2023-05-22 03:57:28","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\R693C6H5\2206.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"77KTPYPM","computerProgram","2023","","deel-ai/xplique","","","","","https://github.com/deel-ai/xplique","👋 Xplique is a Neural Networks Explainability Toolbox","2023-05-15","2023-05-22 03:57:21","2023-05-22 03:57:21","2023-05-22 03:57:21","","","","","","","","","","","","DEEL","","","","","","","GitHub","","original-date: 2020-04-05T15:25:54Z","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Python","","","","","","","","",""
"IVLYNMBA","preprint","2020","Kokhlikyan, Narine; Miglani, Vivek; Martin, Miguel; Wang, Edward; Alsallakh, Bilal; Reynolds, Jonathan; Melnikov, Alexander; Kliushkina, Natalia; Araya, Carlos; Yan, Siqi; Reblitz-Richardson, Orion","Captum: A unified and generic model interpretability library for PyTorch","","","","10.48550/arXiv.2009.07896","http://arxiv.org/abs/2009.07896","In this paper we introduce a novel, unified, open-source model interpretability library for PyTorch [12]. The library contains generic implementations of a number of gradient and perturbation-based attribution algorithms, also known as feature, neuron and layer importance algorithms, as well as a set of evaluation metrics for these algorithms. It can be used for both classification and non-classification models including graph-structured models built on Neural Networks (NN). In this paper we give a high-level overview of supported attribution algorithms and show how to perform memory-efficient and scalable computations. We emphasize that the three main characteristics of the library are multimodality, extensibility and ease of use. Multimodality supports different modality of inputs such as image, text, audio or video. Extensibility allows adding new algorithms and features. The library is also designed for easy understanding and use. Besides, we also introduce an interactive visualization tool called Captum Insights that is built on top of Captum library and allows sample-based model debugging and visualization using feature importance metrics.","2020-09-16","2023-05-22 03:56:06","2023-05-22 03:56:06","2023-05-22 03:56:06","","","","","","","Captum","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2009.07896 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\MBBRTXZL\2009.html; C:\Users\ambreen.hanif\Zotero\storage\DA67ED99\Kokhlikyan et al_2020_Captum.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2009.07896","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZKUDCKAD","preprint","2017","Kindermans, Pieter-Jan; Schütt, Kristof T.; Alber, Maximilian; Müller, Klaus-Robert; Erhan, Dumitru; Kim, Been; Dähne, Sven","Learning how to explain neural networks: PatternNet and PatternAttribution","","","","10.48550/arXiv.1705.05598","http://arxiv.org/abs/1705.05598","DeConvNet, Guided BackProp, LRP, were invented to better understand deep neural networks. We show that these methods do not produce the theoretically correct explanation for a linear model. Yet they are used on multi-layer networks with millions of parameters. This is a cause for concern since linear models are simple neural networks. We argue that explanation methods for neural nets should work reliably in the limit of simplicity, the linear models. Based on our analysis of linear models we propose a generalization that yields two explanation techniques (PatternNet and PatternAttribution) that are theoretically sound for linear models and produce improved explanations for deep networks.","2017-10-24","2023-05-22 03:56:00","2023-05-22 03:56:00","2023-05-22 03:56:00","","","","","","","Learning how to explain neural networks","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1705.05598 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\7GLMAVG4\1705.html; C:\Users\ambreen.hanif\Zotero\storage\U863Y5MU\Kindermans et al_2017_Learning how to explain neural networks.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1705.05598","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4E9UMAT9","journalArticle","2018","Molnar, Christoph; Casalicchio, Giuseppe; Bischl, Bernd","iml: An R package for Interpretable Machine Learning","Journal of Open Source Software","","2475-9066","10.21105/joss.00786","https://joss.theoj.org/papers/10.21105/joss.00786","Molnar et al., (2018). iml: An R package for Interpretable Machine Learning . Journal of Open Source Software, 3(26), 786, https://doi.org/10.21105/joss.00786","2018-06-27","2023-05-22 03:48:34","2023-05-22 03:48:36","2023-05-22 03:48:34","786","","26","3","","","iml","","","","","","","en","","","","","joss.theoj.org","","305 citations (Crossref) [2023-05-22]","","C:\Users\ambreen.hanif\Zotero\storage\Z3J3T2BJ\Molnar et al_2018_iml.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MHCZK5DX","preprint","2019","Nori, Harsha; Jenkins, Samuel; Koch, Paul; Caruana, Rich","InterpretML: A Unified Framework for Machine Learning Interpretability","","","","10.48550/arXiv.1909.09223","http://arxiv.org/abs/1909.09223","InterpretML is an open-source Python package which exposes machine learning interpretability algorithms to practitioners and researchers. InterpretML exposes two types of interpretability - glassbox models, which are machine learning models designed for interpretability (ex: linear models, rule lists, generalized additive models), and blackbox explainability techniques for explaining existing systems (ex: Partial Dependence, LIME). The package enables practitioners to easily compare interpretability algorithms by exposing multiple methods under a unified API, and by having a built-in, extensible visualization platform. InterpretML also includes the first implementation of the Explainable Boosting Machine, a powerful, interpretable, glassbox model that can be as accurate as many blackbox models. The MIT licensed source code can be downloaded from github.com/microsoft/interpret.","2019-09-19","2023-05-22 03:44:42","2023-05-22 03:44:42","2023-05-22 03:44:42","","","","","","","InterpretML","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1909.09223 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\4YTUBMAI\1909.html; C:\Users\ambreen.hanif\Zotero\storage\KIKHETAG\Nori et al_2019_InterpretML.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1909.09223","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HHBKBFYH","webpage","","","[1909.09223] InterpretML: A Unified Framework for Machine Learning Interpretability","","","","","https://arxiv.org/abs/1909.09223","","","2023-05-22 03:44:18","2023-05-22 03:44:18","2023-05-22 03:44:18","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\PH3EQJTR\1909.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TJJ58G3N","conferencePaper","","Nori,, Harsha; Jenkins, Samuel; Konch, paul","InterpretML: A Unified Framework for Machine Learning Interpretability   author={Nori, Harsha and Jenkins, Samuel and Koch, Paul and Caruana, Rich},   journal={arXiv preprint arXiv:1909.09223},   year={2019} }","","","","","","","","2023-05-22 03:42:24","2023-05-22 03:44:18","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QQ54RVBL","conferencePaper","2018","Chander, Ajay; Srinivasan, Ramya","Evaluating Explanations by Cognitive Value","Machine Learning and Knowledge Extraction","978-3-319-99740-7","","10.1007/978-3-319-99740-7_23","","The transparent AI initiative has ignited several academic and industrial endeavors and produced some impressive technologies and results thus far. Many state-of-the-art methods provide explanations that mostly target the needs of AI engineers. However, there is very little work on providing explanations that support the needs of business owners, software developers, and consumers who all play significant roles in the service development and use cycle. By considering the overall context in which an explanation is presented, including the role played by the human-in-the-loop, we can hope to craft effective explanations. In this paper, we introduce the notion of the “cognitive value” of an explanation and describe its role in providing effective explanations within a given context. Specifically, we consider the scenario of a business owner seeking to improve sales of their product, and compare explanations provided by some existing interpretable machine learning algorithms (random forests, scalable Bayesian Rules, causal models) in terms of the cognitive value they offer to the business owner. We hope that our work will foster future research in the field of transparent AI to incorporate the cognitive value of explanations in crafting and evaluating explanations.","2018","2023-05-22 00:56:30","2023-05-22 00:56:34","","314-328","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","6 citations (Crossref) [2023-05-22]","","C:\Users\ambreen.hanif\Zotero\storage\JFBFRPLV\Chander_Srinivasan_2018_Evaluating Explanations by Cognitive Value.pdf","","","","Holzinger, Andreas; Kieseberg, Peter; Tjoa, A Min; Weippl, Edgar","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HEA8Z47I","journalArticle","","Utkin, Lev V; Kovalev, Maxim S; Kasimov, Ernest M","SurvLIME-Inf: A simpliﬁed modiﬁcation of SurvLIME for explanation of machine learning survival models","","","","","","A new modiﬁcation of the explanation method SurvLIME called SurvLIME-Inf for explaining machine learning survival models is proposed. The basic idea behind SurvLIME as well as SurvLIME-Inf is to apply the Cox proportional hazards model to approximate the black-box survival model at the local area around a test example. The Cox model is used due to the linear relationship of covariates. In contrast to SurvLIME, the proposed modiﬁcation uses L∞-norm for deﬁning distances between approximating and approximated cumulative hazard functions. This leads to a simple linear programming problem for determining important features and for explaining the black-box model prediction. Moreover, SurvLIME-Inf outperforms SurvLIME when the training set is very small. Numerical experiments with synthetic and real datasets demonstrate the SurvLIME-Inf eﬃciency.","","2023-05-12 00:33:27","2023-05-18 12:47:09","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\NZVHT25C\Utkin et al. - SurvLIME-Inf A simpliﬁed modiﬁcation of SurvLIME .pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6XZ4DR4H","journalArticle","","Silva, Marta Contreiras; Faria, Daniel; Pesquita, Catia","Integrating Knowledge Graphs for Explainable Artiﬁcial Intelligence in Biomedicine","","","","","","","","2023-05-11 01:53:05","2023-05-18 12:47:09","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\UZ4YMAFD\Silva et al. - Integrating Knowledge Graphs for Explainable Artiﬁ.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MW5FTRZS","conferencePaper","2020","Ormenisan, Alexandru A.; Meister, Moritz; Buso, Fabio; Andersson, Robin; Haridi, Seif; Dowling, Jim","Time Travel and Provenance for Machine Learning Pipelines","","","","","https://www.usenix.org/conference/opml20/presentation/ormenisan","","2020","2023-05-15 04:18:17","2023-05-18 12:47:05","2023-05-15 04:18:17","","","","","","","","","","","","","","en","","","","","www.usenix.org","","","","C:\Users\ambreen.hanif\Zotero\storage\YMMNQZKZ\Ormenisan et al_2020_Time Travel and Provenance for Machine Learning Pipelines.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020 USENIX Conference on Operational Machine Learning (OpML 20)","","","","","","","","","","","","","","",""
"PX6I2KHJ","journalArticle","","Ormenisan, Alexandru A; Ismail, Mahmoud; Haridi, Seif; Dowling, Jim","Implicit Provenance for Machine Learning Artifacts","","","","","","Machine learning (ML) presents new challenges for reproducible software engineering, as the artifacts required for repeatably training models are not just versioned code, but also hyperparameters, code dependencies, and the exact version of the training data. Existing systems for tracking the lineage of ML artifacts, such as TensorFlow Extended or MLFlow, are invasive, requiring developers to refactor their code that now is controlled by the external system. In this paper, we present an alternative approach, we call implicit provenance, where we instrument a distributed file system and APIs to capture changes to ML artifacts, that, along with file naming conventions, mean that full lineage can be tracked for TensorFlow/Keras/Pytorch programs without requiring code changes. We address challenges related to adding strongly consistent metadata extensions to the distributed file system, while minimizing provenance overhead, and ensuring transparent eventual consistent replication of extended metadata to an efficient search engine, Elasticsearch. Our provenance framework is integrated into the open-source Hopsworks framework, and used in production to enable full provenance for end-to-end machine learning pipelines.","","2023-05-15 04:25:58","2023-05-18 12:47:04","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\8TJ5YWUE\Ormenisan et al. - Implicit Provenance for Machine Learning Artifacts.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3EWFARCG","conferencePaper","2020","Mohankumar, Akash Kumar; Nema, Preksha; Narasimhan, Sharan; Khapra, Mitesh M.; Srinivasan, Balaji Vasan; Ravindran, Balaraman","Towards Transparent and Explainable Attention Models","Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics","","","10.18653/v1/2020.acl-main.387","https://aclanthology.org/2020.acl-main.387","Recent studies on interpretability of attention distributions have led to notions of faithful and plausible explanations for a model's predictions. Attention distributions can be considered a faithful explanation if a higher attention weight implies a greater impact on the model's prediction. They can be considered a plausible explanation if they provide a human-understandable justification for the model's predictions. In this work, we first explain why current attention mechanisms in LSTM based encoders can neither provide a faithful nor a plausible explanation of the model's predictions. We observe that in LSTM based encoders the hidden representations at different time-steps are very similar to each other (high conicity) and attention weights in these situations do not carry much meaning because even a random permutation of the attention weights does not affect the model's predictions. Based on experiments on a wide variety of tasks and datasets, we observe attention distributions often attribute the model's predictions to unimportant words such as punctuation and fail to offer a plausible explanation for the predictions. To make attention mechanisms more faithful and plausible, we propose a modified LSTM cell with a diversity-driven training objective that ensures that the hidden representations learned at different time steps are diverse. We show that the resulting attention distributions offer more transparency as they (i) provide a more precise importance ranking of the hidden states (ii) are better indicative of words important for the model's predictions (iii) correlate better with gradient-based attribution methods. Human evaluations indicate that the attention distributions learned by our model offer a plausible explanation of the model's predictions. Our code has been made publicly available at https://github.com/akashkm99/Interpretable-Attention","2020-07","2023-05-15 06:36:16","2023-05-15 06:36:22","2023-05-15 06:36:16","4206–4216","","","","","","","","","","","Association for Computational Linguistics","Online","","","","","","ACLWeb","","19 citations (Crossref) [2023-05-15]","","C:\Users\ambreen.hanif\Zotero\storage\T97RYDAQ\Mohankumar et al_2020_Towards Transparent and Explainable Attention Models.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ACL 2020","","","","","","","","","","","","","","",""
"K3LFXLXL","conferencePaper","2019","Ming, Yao; Xu, Panpan; Qu, Huamin; Ren, Liu","Interpretable and Steerable Sequence Learning via Prototypes","Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining","","","10.1145/3292500.3330908","http://arxiv.org/abs/1907.09728","One of the major challenges in machine learning nowadays is to provide predictions with not only high accuracy but also user-friendly explanations. Although in recent years we have witnessed increasingly popular use of deep neural networks for sequence modeling, it is still challenging to explain the rationales behind the model outputs, which is essential for building trust and supporting the domain experts to validate, critique and refine the model. We propose ProSeNet, an interpretable and steerable deep sequence model with natural explanations derived from case-based reasoning. The prediction is obtained by comparing the inputs to a few prototypes, which are exemplar cases in the problem domain. For better interpretability, we define several criteria for constructing the prototypes, including simplicity, diversity, and sparsity and propose the learning objective and the optimization procedure. ProSeNet also provides a user-friendly approach to model steering: domain experts without any knowledge on the underlying model or parameters can easily incorporate their intuition and experience by manually refining the prototypes. We conduct experiments on a wide range of real-world applications, including predictive diagnostics for automobiles, ECG, and protein sequence classification and sentiment analysis on texts. The result shows that ProSeNet can achieve accuracy on par with state-of-the-art deep learning models. We also evaluate the interpretability of the results with concrete case studies. Finally, through user study on Amazon Mechanical Turk (MTurk), we demonstrate that the model selects high-quality prototypes which align well with human knowledge and can be interactively refined for better interpretability without loss of performance.","2019-07-25","2023-05-15 06:27:36","2023-05-15 06:27:41","2023-05-15 06:27:36","903-913","","","","","","","","","","","","","","","","","","arXiv.org","","40 citations (Crossref) [2023-05-15] arXiv:1907.09728 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\ZH2RTXJP\1907.html; C:\Users\ambreen.hanif\Zotero\storage\3L8RQEHK\Ming et al_2019_Interpretable and Steerable Sequence Learning via Prototypes.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"24BHEN82","journalArticle","2021","Singla, Sumedha; Wallace, Stephen; Triantafillou, Sofia; Batmanghelich, Kayhan","Using Causal Analysis for Conceptual Deep Learning Explanation","Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention","","","10.1007/978-3-030-87199-4_49","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8579822/","Model explainability is essential for the creation of trustworthy Machine Learning models in healthcare. An ideal explanation resembles the decision-making process of a domain expert and is expressed using concepts or terminology that is meaningful to the clinicians. To provide such explanation, we first associate the hidden units of the classifier to clinically relevant concepts. We take advantage of radiology reports accompanying the chest X-ray images to define concepts. We discover sparse associations between concepts and hidden units using a linear sparse logistic regression. To ensure that the identified units truly influence the classifier’s outcome, we adopt tools from Causal Inference literature and, more specifically, mediation analysis through counterfactual interventions. Finally, we construct a low-depth decision tree to translate all the discovered concepts into a straightforward decision rule, expressed to the radiologist. We evaluated our approach on a large chest x-ray dataset, where our model produces a global explanation consistent with clinical knowledge.","2021","2023-05-15 04:40:23","2023-05-15 04:40:28","2023-05-15 04:40:23","519-528","","","12903","","Med Image Comput Comput Assist Interv","","","","","","","","","","","","","PubMed Central","","4 citations (Crossref) [2023-05-15] PMID: 34766174 PMCID: PMC8579822","","; C:\Users\ambreen.hanif\Zotero\storage\G7Z996EI\Singla et al_2021_Using Causal Analysis for Conceptual Deep Learning Explanation.pdf","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8579822/","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DH9G84KE","journalArticle","2019","Belinkov, Yonatan; Glass, James","Analysis Methods in Neural Language Processing: A Survey","Transactions of the Association for Computational Linguistics","","2307-387X","10.1162/tacl_a_00254","https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00254/43503/Analysis-Methods-in-Neural-Language-Processing-A","Abstract             The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.","2019-04-01","2023-05-15 04:36:57","2023-05-15 04:37:00","2023-05-15 04:36:56","49-72","","","7","","","Analysis Methods in Neural Language Processing","","","","","","","en","","","","","DOI.org (Crossref)","","88 citations (Crossref) [2023-05-15]","","C:\Users\ambreen.hanif\Zotero\storage\3TIHF9CM\Belinkov_Glass_2019_Analysis Methods in Neural Language Processing.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LLEH9IZU","webpage","","Sayedi, Husna","Explainable AI (XAI): NLP Edition","","","","","https://www.taus.net/resources/blog/explainable-ai-xai-nlp-edition","Explaining what Explainable AI (XAI) entails and diving into five major XAI techniques for Natural Language Processing (NLP).","","2023-05-15 04:29:24","2023-05-15 04:29:24","2023-05-15 04:29:24","","","","","","","Explainable AI (XAI)","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\V4KBXCJ9\explainable-ai-xai-nlp-edition.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8G72UMRX","bookSection","2020","Sarker, Md Kamruzzaman; Schwartz, Joshua; Hitzler, Pascal; Zhou, Lu; Nadella, Srikanth; Minnery, Brandon; Juvina, Ion; Raymer, Michael L.; Aue, William R.","Wikipedia Knowledge Graph for Explainable AI","Knowledge Graphs and Semantic Web","978-3-030-65383-5 978-3-030-65384-2","","","https://link.springer.com/10.1007/978-3-030-65384-2_6","Explainable artiﬁcial intelligence (XAI) requires domain information to explain a system’s decisions, for which structured forms of domain information like Knowledge Graphs (KGs) or ontologies are best suited. As such, readily available KGs are important to accelerate progress in XAI. To facilitate the advancement of XAI, we present the cycle-free Wikipedia Knowledge Graph (WKG) based on information from English Wikipedia. Each Wikipedia article title, its corresponding category, and the category hierarchy are transformed into diﬀerent entities in the knowledge graph. Along with cycle-free version we also provide the original knowledge graph as it is. We evaluate whether the WKG is helpful to improve XAI compared with existing KGs, ﬁnding that WKG is better suited than the current state of the art. We also compare the cycle-free WKG with the Suggested Upper Merged Ontology (SUMO) and DBpedia schema KGs, ﬁnding minimal to no information loss.","2020","2023-05-15 04:15:32","2023-05-15 04:15:35","2023-05-15 04:15:32","72-87","","","1232","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Communications in Computer and Information Science DOI: 10.1007/978-3-030-65384-2_6","","C:\Users\ambreen.hanif\Zotero\storage\6WDBN5CQ\Sarker et al. - 2020 - Wikipedia Knowledge Graph for Explainable AI.pdf","","","","Villazón-Terrazas, Boris; Ortiz-Rodríguez, Fernando; Tiwari, Sanju M.; Shandilya, Shishir K.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GD49UT4L","journalArticle","2020","Kovalev, Maxim S.; Utkin, Lev V.; Kasimov, Ernest M.","SurvLIME: A method for explaining machine learning survival models","Knowledge-Based Systems","","0950-7051","10.1016/j.knosys.2020.106164","https://www.sciencedirect.com/science/article/pii/S0950705120304044","A new method called SurvLIME for explaining machine learning survival models is proposed. It can be viewed as an extension or modification of the well-known method LIME. The main idea behind the proposed method is to apply the Cox proportional hazards model to approximate the survival model at the local area around a test example. The Cox model is used because it considers a linear combination of the example covariates such that coefficients of the covariates can be regarded as quantitative impacts on the prediction. Another idea is to approximate cumulative hazard functions of the explained model and the Cox model by using a set of perturbed points in a local area around the point of interest. The method is reduced to solving an unconstrained convex optimization problem. A lot of numerical experiments demonstrate the SurvLIME efficiency.","2020-09-05","2023-05-12 00:34:07","2023-05-12 00:34:12","2023-05-12 00:34:07","106164","","","203","","Knowledge-Based Systems","SurvLIME","","","","","","","en","","","","","ScienceDirect","","30 citations (Crossref) [2023-05-12]","","C:\Users\ambreen.hanif\Zotero\storage\DCD33JT3\Kovalev et al_2020_SurvLIME.pdf; C:\Users\ambreen.hanif\Zotero\storage\MUAXSU3V\S0950705120304044.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2VN9IDYQ","journalArticle","2019","Holzinger, Andreas; Langs, Georg; Denk, Helmut; Zatloukal, Kurt; Müller, Heimo","Causability and explainability of artificial intelligence in medicine","WIREs Data Mining and Knowledge Discovery","","1942-4795","10.1002/widm.1312","https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1312","Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black-box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use-case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system This article is categorized under: Fundamental Concepts of Data and Knowledge > Human Centricity and User Interaction","2019","2023-05-08 23:22:44","2023-05-08 23:22:46","2023-05-08 23:22:44","e1312","","4","9","","","","","","","","","","en","","","","","Wiley Online Library","","428 citations (Crossref) [2023-05-09] _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1312","","C:\Users\ambreen.hanif\Zotero\storage\TSMK7G6I\Holzinger et al_2019_Causability and explainability of artificial intelligence in medicine.pdf; C:\Users\ambreen.hanif\Zotero\storage\TH5QDK7A\widm.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3H2UZBNB","bookSection","2016","Longo, Luca","Argumentation for Knowledge Representation, Conflict Resolution, Defeasible Inference and Its Integration with Machine Learning","Machine Learning for Health Informatics: State-of-the-Art and Future Challenges","978-3-319-50478-0","","","https://doi.org/10.1007/978-3-319-50478-0_9","Modern machine Learning is devoted to the construction of algorithms and computational procedures that can automatically improve with experience and learn from data. Defeasible argumentation has emerged as sub-topic of artificial intelligence aimed at formalising common-sense qualitative reasoning. The former is an inductive approach for inference while the latter is deductive, each one having advantages and limitations. A great challenge for theoretical and applied research in AI is their integration. The first aim of this chapter is to provide readers informally with the basic notions of defeasible and non-monotonic reasoning. It then describes argumentation theory, a paradigm for implementing defeasible reasoning in practice as well as the common multi-layer schema upon which argument-based systems are usually built. The second aim is to describe a selection of argument-based applications in the medical and health-care sectors, informed by the multi-layer schema. A summary of the features that emerge from the applications under review is aimed at showing why defeasible argumentation is attractive for knowledge-representation, conflict resolution and inference under uncertainty. Open problems and challenges in the field of argumentation are subsequently described followed by a future outlook in which three points of integration with machine learning are proposed.","2016","2023-05-08 23:14:08","2023-05-08 23:14:08","2023-05-08 23:14:08","183-208","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","DOI: 10.1007/978-3-319-50478-0_9","","C:\Users\ambreen.hanif\Zotero\storage\Z3GSZWV8\Longo_2016_Argumentation for Knowledge Representation, Conflict Resolution, Defeasible.pdf","","","","Holzinger, Andreas","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TISCS2JW","bookSection","2023","Yang, Zhongliang; Jia, Xingli; Zhang, Xinyu; Tang, Jialu","Interpretable Neural Symbol Learning Methods to Fuse Deep Learning Representation and Knowledge Graph: Zhejiang Cuisine Recipe Intangible Cultural Heritage Use Case","Design Studies and Intelligence Engineering","","","","https://ebooks.iospress.nl/doi/10.3233/FAIA220700","","2023","2023-05-08 19:35:34","2023-05-08 19:35:34","2023-05-08 19:35:34","53-61","","","","","","Interpretable Neural Symbol Learning Methods to Fuse Deep Learning Representation and Knowledge Graph","","","","","IOS Press","","","","","","","ebooks-iospress-nl.simsrad.net.ocs.mq.edu.au","","DOI: 10.3233/FAIA220700","","C:\Users\ambreen.hanif\Zotero\storage\WR96464Q\Yang et al_2023_Interpretable Neural Symbol Learning Methods to Fuse Deep Learning.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UNRSJ2JI","journalArticle","2021","Conati, Cristina; Barral, Oswald; Putnam, Vanessa; Rieger, Lea","Toward personalized XAI: A case study in intelligent tutoring systems","Artificial Intelligence","","0004-3702","10.1016/j.artint.2021.103503","https://www.sciencedirect.com/science/article/pii/S0004370221000540","Our research is a step toward ascertaining the need for personalization in XAI, and we do so in the context of investigating the value of explanations of AI-driven hints and feedback in Intelligent Tutoring Systems (ITS). We added an explanation functionality to the Adaptive CSP (ACSP) applet, an interactive simulation that helps students learn an algorithm for constraint satisfaction problems by providing AI-driven hints adapted to their predicted level of learning. We present the design of the explanation functionality and the results of a controlled study to evaluate its impact on students' learning and perception of the ACPS hints. The study includes an analysis of how these outcomes are modulated by several user characteristics such as personality traits and cognitive abilities, to asses if explanations should be personalized to these characteristics. Our results indicate that providing explanations increase students' trust in the ACPS hints, perceived usefulness of the hints, and intention to use them again. In addition, we show that students' access of the ACSP explanation and learning gains are modulated by three user characteristics, Need for Cognition, Contentiousness and Reading Proficiency, providing insights on how to personalize the ACSP explanations to these traits, as well as initial evidence on the potential value of personalized Explainable AI (XAI) for ITS.","2021-09-01","2023-05-08 19:31:40","2023-05-08 19:31:44","2023-05-08 19:31:40","103503","","","298","","Artificial Intelligence","Toward personalized XAI","","","","","","","en","","","","","ScienceDirect","","18 citations (Crossref) [2023-05-09]","","C:\Users\ambreen.hanif\Zotero\storage\95Q6E4C9\Conati et al_2021_Toward personalized XAI.pdf; C:\Users\ambreen.hanif\Zotero\storage\NDIMJVNW\S0004370221000540.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D5BU7RES","preprint","2021","Rožanec, Jože M.; Zajec, Patrik; Kenda, Klemen; Novalija, Inna; Fortuna, Blaž; Mladenić, Dunja","XAI-KG: knowledge graph to support XAI and decision-making in manufacturing","","","","","http://arxiv.org/abs/2105.01929","The increasing adoption of artificial intelligence requires accurate forecasts and means to understand the reasoning of artificial intelligence models behind such a forecast. Explainable Artificial Intelligence (XAI) aims to provide cues for why a model issued a certain prediction. Such cues are of utmost importance to decision-making since they provide insights on the features that influenced most certain forecasts and let the user decide if the forecast can be trusted. Though many techniques were developed to explain black-box models, little research was done on assessing the quality of those explanations and their influence on decision-making. We propose an ontology and knowledge graph to support collecting feedback regarding forecasts, forecast explanations, recommended decision-making options, and user actions. This way, we provide means to improve forecasting models, explanations, and recommendations of decision-making options. We tailor the knowledge graph for the domain of demand forecasting and validate it on real-world data.","2021-05-05","2023-05-08 19:31:22","2023-05-08 19:31:22","2023-05-08 19:28:08","","","","","","","XAI-KG","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2105.01929 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\X43I5RUE\Rožanec et al. - 2021 - XAI-KG knowledge graph to support XAI and decisio.html; C:\Users\ambreen.hanif\Zotero\storage\5EGSD7ZS\Rožanec et al. - 2021 - XAI-KG knowledge graph to support XAI and decisio.pdf","","","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","arXiv:2105.01929","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X5CPHCRB","journalArticle","","","Personalized Detection of Cognitive Biases in Actions of Users from Their Logs: Anchoring and Recency Biases. (arXiv:2206.15129v1 [cs.AI])","arXiv Computer Science","","","","https://arxiv.org/abs/2206.15129?utm_source=researcher_app&utm_medium=referral&utm_campaign=RESR_MRKT_Researcher_inbound","Cognitive biases are mental shortcuts humans use in dealing with information and the environment, and which result in biased actions and behaviors (or, actions), unbeknownst to themselves. Biases take many forms, with cognitive biases occupying a central role that inflicts fairness, accountability, transparency, ethics, law, medicine, and discrimination. Detection of biases is considered a necessary step toward their mitigation. Herein, we focus on two cognitive biases - anchoring and recency. The recognition of cognitive bias in computer science is largely in the domain of information retrieval, and bias is identified at an aggregate level with the help of annotated data. Proposing a different direction for bias detection, we offer a principled approach along with Machine Learning to detect these two cognitive biases from Web logs of users' actions. Our individual user level detection makes it truly personalized, and does not rely on annotated data. Instead, we start with two basic principles established in cognitive psychology, use modified training of an attention network, and interpret attention weights in a novel way according to those principles, to infer and distinguish between these two biases. The personalized approach allows detection for specific users who are susceptible to these biases when performing their tasks, and can help build awareness among them so as to undertake bias mitigation.","","2023-01-03 05:51:07","2023-05-08 19:29:53","","","","","","","","","","","","","","","","","","","","","","","","","","Researcher App","⚠️ Invalid DOI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y3PZUZ3G","journalArticle","2022","Marques-Silva, Joao; Ignatiev, Alexey","Delivering Trustworthy AI through Formal XAI","Proceedings of the AAAI Conference on Artificial Intelligence","","2374-3468","10.1609/aaai.v36i11.21499","https://ojs.aaai.org/index.php/AAAI/article/view/21499","The deployment of systems of artificial intelligence (AI) in high-risk settings warrants the need for trustworthy AI. This crucial requirement is highlighted by recent EU guidelines and regulations, but also by recommendations from OECD and UNESCO, among several other examples. One critical premise of trustworthy AI involves the necessity of finding explanations that offer reliable guarantees of soundness. This paper argues that the best known eXplainable AI (XAI) approaches fail to provide sound explanations, or that alternatively find explanations which can exhibit significant redundancy. The solution to these drawbacks are explanation approaches that offer formal guarantees of rigor. These formal explanations are not only sound but guarantee irredundancy. This paper summarizes the recent developments in the emerging discipline of formal XAI. The paper also outlines existing challenges for formal XAI.","2022-06-28","2023-05-04 19:24:54","2023-05-04 19:24:57","2023-05-04 19:24:54","12342-12350","","11","36","","","","","","","","","","en","Copyright (c) 2022 Association for the Advancement of Artificial Intelligence","","","","ojs.aaai.org","","6 citations (Crossref) [2023-05-05] Number: 11","","C:\Users\ambreen.hanif\Zotero\storage\Z68G7Z27\Marques-Silva_Ignatiev_2022_Delivering Trustworthy AI through Formal XAI.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T354EA2P","journalArticle","","Reddy, Chandan K; Li, Yan","Machine Learning for Survival Analysis","Statistical Methods","","","","","","","2023-05-04 19:01:28","2023-05-04 19:05:21","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\NQLCEXSU\Reddy and Li - Machine Learning for Survival Analysis.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NSPR7IR2","preprint","2014","Xiao, Tianjun; Xu, Yichong; Yang, Kuiyuan; Zhang, Jiaxing; Peng, Yuxin; Zhang, Zheng","The Application of Two-level Attention Models in Deep Convolutional Neural Network for Fine-grained Image Classification","","","","10.48550/arXiv.1411.6447","http://arxiv.org/abs/1411.6447","Fine-grained classification is challenging because categories can only be discriminated by subtle and local differences. Variances in the pose, scale or rotation usually make the problem more difficult. Most fine-grained classification systems follow the pipeline of finding foreground object or object parts (where) to extract discriminative features (what). In this paper, we propose to apply visual attention to fine-grained classification task using deep neural network. Our pipeline integrates three types of attention: the bottom-up attention that propose candidate patches, the object-level top-down attention that selects relevant patches to a certain object, and the part-level top-down attention that localizes discriminative parts. We combine these attentions to train domain-specific deep nets, then use it to improve both the what and where aspects. Importantly, we avoid using expensive annotations like bounding box or part information from end-to-end. The weak supervision constraint makes our work easier to generalize. We have verified the effectiveness of the method on the subsets of ILSVRC2012 dataset and CUB200_2011 dataset. Our pipeline delivered significant improvements and achieved the best accuracy under the weakest supervision condition. The performance is competitive against other methods that rely on additional annotations.","2014-11-24","2023-04-17 02:45:55","2023-04-17 02:45:55","2023-04-17 02:45:55","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1411.6447 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\7HXJJZYH\1411.html; C:\Users\ambreen.hanif\Zotero\storage\WHP7WQIP\Xiao et al_2014_The Application of Two-level Attention Models in Deep Convolutional Neural.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1411.6447","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2HSPAXFK","journalArticle","2017","Herman, Bernease","The Promise and Peril of Human Evaluation for Model Interpretability","ArXiv","","","","https://www.semanticscholar.org/paper/The-Promise-and-Peril-of-Human-Evaluation-for-Model-Herman/956a17cb8f5db9cfa3c6256f1de76616268a4c5e","Transparency, user trust, and human comprehension are popular ethical motivations for interpretable machine learning. In support of these goals, researchers evaluate model explanation performance using humans and real world applications. This alone presents a challenge in many areas of artificial intelligence. In this position paper, we propose a distinction between descriptive and persuasive explanations. We discuss reasoning suggesting that functional interpretability may be correlated with cognitive function and user preferences. If this is indeed the case, evaluation and optimization using functional metrics could perpetuate implicit cognitive bias in explanations that threaten transparency. Finally, we propose two potential research directions to disambiguate cognitive function and explanation models, retaining control over the tradeoff between accuracy and interpretability.","2017-11-20","2023-04-06 20:14:45","2023-04-14 01:43:01","2023-04-06 20:14:45","","","","","","","","","","","","","","","","","","","Semantic Scholar","","","","","https://www.semanticscholar.org/paper/The-Promise-and-Peril-of-Human-Evaluation-for-Model-Herman/956a17cb8f5db9cfa3c6256f1de76616268a4c5e","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZW526V7L","journalArticle","2018","Guidotti, Riccardo; Monreale, Anna; Ruggieri, Salvatore; Turini, Franco; Pedreschi, Dino; Giannotti, Fosca","A Survey Of Methods For Explaining Black Box Models","ACM computing surveys (CSUR)","","","10.1145/3236009","","In the last years many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness sometimes at the cost of scar-ifying accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, delineating explicitly or implicitly its own definition of in-terpretability and explanation. The aim of this paper is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.","2018","2021-09-30 00:50:35","2023-04-13 04:43:37","2021-09-30","1-42","","5","51","","","","","","","","","","","","","","","","","1153 citations (Crossref) [2022-12-20] arXiv: 1802.01933v3 QID: Q102362901","","C:\Users\ambreen.hanif\Zotero\storage\2GYJR6TH\full-text.pdf","","survey","Interpretability; Explanations; Open The Black Box; Transparent Models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T9XQWVI3","preprint","2021","Kurshan, E.; Shen, H.; Yu, H.","Financial Crime & Fraud Detection Using Graph Computing: Application Considerations & Outlook","","","","","http://arxiv.org/abs/2103.01854","In recent years, the unprecedented growth in digital payments fueled consequential changes in fraud and financial crimes. In this new landscape, traditional fraud detection approaches such as rule-based engines have largely become ineffective. AI and machine learning solutions using graph computing principles have gained significant interest. Graph neural networks and emerging adaptive solutions provide compelling opportunities for the future of fraud and financial crime detection. However, implementing the graph-based solutions in financial transaction processing systems has brought numerous obstacles and application considerations to light. In this paper, we overview the latest trends in the financial crimes landscape and discuss the implementation difficulties current and emerging graph solutions face. We argue that the application demands and implementation challenges provide key insights in developing effective solutions.","2021-03-02","2023-04-11 06:12:38","2023-04-11 06:12:38","2023-04-11 06:12:38","","","","","","","Financial Crime & Fraud Detection Using Graph Computing","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2103.01854 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\UHK33SGG\2103.html; C:\Users\ambreen.hanif\Zotero\storage\8BDWNMQY\Kurshan et al_2021_Financial Crime & Fraud Detection Using Graph Computing.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2103.01854","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NBUL794T","conferencePaper","2006","Budgen, David; Brereton, Pearl","Performing systematic literature reviews in software engineering","Proceedings of the 28th international conference on Software engineering","978-1-59593-375-1","","10.1145/1134285.1134500","https://dl.acm.org/doi/10.1145/1134285.1134500","Context: Making best use of the growing number of empirical studies in Software Engineering, for making decisions and formulating research questions, requires the ability to construct an objective summary of available research evidence. Adopting a systematic approach to assessing and aggregating the outcomes from a set of empirical studies is also particularly important in Software Engineering, given that such studies may employ very different experimental forms and be undertaken in very different experimental contexts.Objectives: To provide an introduction to the role, form and processes involved in performing Systematic Literature Reviews. After the tutorial, participants should be able to read and use such reviews, and have gained the knowledge needed to conduct systematic reviews of their own.Method: We will use a blend of information presentation (including some experiences of the problems that can arise in the Software Engineering domain), and also of interactive working, using review material prepared in advance.","2006-05-28","2023-04-11 05:29:11","2023-04-11 05:29:15","2023-04-10","1051–1052","","","","","","","ICSE '06","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","230 citations (Crossref) [2023-04-11]","","C:\Users\ambreen.hanif\Zotero\storage\C7C96JCA\Budgen_Brereton_2006_Performing systematic literature reviews in software engineering.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CMPLVPC2","webpage","","","Performing systematic literature reviews in software engineering | Proceedings of the 28th international conference on Software engineering","","","","","https://dl-acm-org.simsrad.net.ocs.mq.edu.au/doi/10.1145/1134285.1134500","","","2023-04-11 05:26:29","2023-04-11 05:26:29","2023-04-11 05:26:29","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\MJ697MMJ\1134285.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MBKYBQUY","preprint","2022","Leventi-Peetz, A.-M.; Östreich, T.","Deep Learning Reproducibility and Explainable AI (XAI)","","","","","http://arxiv.org/abs/2202.11452","The nondeterminism of Deep Learning (DL) training algorithms and its influence on the explainability of neural network (NN) models are investigated in this work with the help of image classification examples. To discuss the issue, two convolutional neural networks (CNN) have been trained and their results compared. The comparison serves the exploration of the feasibility of creating deterministic, robust DL models and deterministic explainable artificial intelligence (XAI) in practice. Successes and limitation of all here carried out efforts are described in detail. The source code of the attained deterministic models has been listed in this work. Reproducibility is indexed as a development-phase-component of the Model Governance Framework, proposed by the EU within their excellence in AI approach. Furthermore, reproducibility is a requirement for establishing causality for the interpretation of model results and building of trust towards the overwhelming expansion of AI systems applications. Problems that have to be solved on the way to reproducibility and ways to deal with some of them, are examined in this work.","2022-03-02","2023-04-10 23:23:41","2023-04-10 23:23:41","2023-04-10 23:23:41","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2202.11452 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\H7L4QSVV\2202.html; C:\Users\ambreen.hanif\Zotero\storage\2DKP8YDC\Leventi-Peetz_Östreich_2022_Deep Learning Reproducibility and Explainable AI (XAI).pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2202.11452","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3C69DP7R","journalArticle","2017","Ancona, Marco; Ceolini, Enea; Öztireli, Cengiz; Gross, Markus","A unified view of gradient-based attribution methods for Deep Neural Networks","","","","10.3929/ETHZ-B-000237705","http://hdl.handle.net/20.500.11850/237705","Understanding the flow of information in Deep Neural Networks is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, only few attempts to analyze them from a theoretical perspective have been made in the past. In this work we analyze various state-of-the-art attribution methods and prove unexplored connections between them. We also show how some methods can be reformulated and more conveniently implemented. Finally, we perform an empirical evaluation with six attribution methods on a variety of tasks and architectures and discuss their strengths and limitations.","2017","2023-04-10 23:23:17","2023-04-10 23:23:17","2023-04-10 23:23:17","11 p.","","","","","","","","","","","","","en","http://rightsstatements.org/page/InC-NC/1.0/, info:eu-repo/semantics/openAccess","","","","Semantic Scholar","","Artwork Size: 11 p. Medium: application/pdf Publisher: ETH Zurich","","C:\Users\ambreen.hanif\Zotero\storage\VIXWCBBS\Ancona et al_2017_A unified view of gradient-based attribution methods for Deep Neural Networks.pdf; ","https://www.semanticscholar.org/paper/A-unified-view-of-gradient-based-attribution-for-Ancona-Ceolini/de2447a25012c71ad316487b4b9e7378a4fcccc0","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ME3KN3F2","webpage","","","Extracting rules from artificial neural networks with distributed representations | Proceedings of the 7th International Conference on Neural Information Processing Systems","","","","","https://dl-acm-org.simsrad.net.ocs.mq.edu.au/doi/10.5555/2998687.2998750","","","2023-04-07 23:56:05","2023-04-07 23:56:05","2023-04-07 23:56:05","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\4ZR7YRCR\2998687.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"63YVN4PP","journalArticle","1999","Schmitz, G.P.J.; Aldrich, C.; Gouws, F.S.","ANN-DT: an algorithm for extraction of decision trees from artificial neural networks","IEEE Transactions on Neural Networks","","1941-0093","10.1109/72.809084","","Although artificial neural networks can represent a variety of complex systems with a high degree of accuracy, these connectionist models are difficult to interpret. This significantly limits the applicability of neural networks in practice, especially where a premium is placed on the comprehensibility or reliability of systems. A novel artificial neural-network decision tree algorithm (ANN-DT) is therefore proposed, which extracts binary decision trees from a trained neural network. The ANN-DT algorithm uses the neural network to generate outputs for samples interpolated from the training data set. In contrast to existing techniques, ANN-DT can extract rules from feedforward neural networks with continuous outputs. These rules are extracted from the neural network without making assumptions about the internal structure of the neural network or the features of the data. A novel attribute selection criterion based on a significance analysis of the variables on the neural-network output is examined. It is shown to have significant benefits in certain cases when compared with the standard criteria of minimum weighted variance over the branches. In three case studies the ANN-DT algorithm compared favorably with CART, a standard decision tree algorithm.","1999-11","2023-04-07 23:47:47","2023-04-07 23:47:50","","1392-1401","","6","10","","","ANN-DT","","","","","","","","","","","","IEEE Xplore","","92 citations (Crossref) [2023-04-08] Conference Name: IEEE Transactions on Neural Networks","","C:\Users\ambreen.hanif\Zotero\storage\U9KV4QK5\809084.html; C:\Users\ambreen.hanif\Zotero\storage\SFQKKNYJ\Schmitz et al_1999_ANN-DT.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GL6TGDPD","journalArticle","2000","Tsukimoto, H.","Extracting rules from trained neural networks","IEEE Transactions on Neural Networks","","1941-0093","10.1109/72.839008","","Presents an algorithm for extracting rules from trained neural networks. The algorithm is a decompositional approach which can be applied to any neural network whose output function is monotone such as a sigmoid function. Therefore, the algorithm can be applied to multilayer neural networks, recurrent neural networks and so on. It does not depend on training algorithms, and its computational complexity is polynomial. The basic idea is that the units of neural networks are approximated by Boolean functions. But the computational complexity of the approximation is exponential, and so a polynomial algorithm is presented. The author has applied the algorithm to several problems to extract understandable and accurate rules. The paper shows the results for the votes data, mushroom data, and others. The algorithm is extended to the continuous domain, where extracted rules are continuous Boolean functions. Roughly speaking, the representation by continuous Boolean functions means the representation using conjunction, disjunction, direct proportion, and reverse proportion. This paper shows the results for iris data.","2000-03","2023-04-07 22:38:19","2023-04-07 22:38:21","","377-389","","2","11","","","","","","","","","","","","","","","IEEE Xplore","","111 citations (Crossref) [2023-04-08] Conference Name: IEEE Transactions on Neural Networks","","C:\Users\ambreen.hanif\Zotero\storage\UYEVP75V\839008.html; C:\Users\ambreen.hanif\Zotero\storage\XPLRBRYT\Tsukimoto_2000_Extracting rules from trained neural networks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PW5GCABY","webpage","","","Extracting rules from trained neural networks | IEEE Journals & Magazine | IEEE Xplore","","","","","https://ieeexplore-ieee-org.simsrad.net.ocs.mq.edu.au/document/839008","","","2023-04-07 22:33:08","2023-04-07 22:33:08","2023-04-07 22:33:08","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\QWVBSL7M\839008.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M275KYEZ","journalArticle","1994","Salzberg, Steven L.","C4.5: Programs for Machine Learning by J. Ross Quinlan. Morgan Kaufmann Publishers, Inc., 1993","Machine Learning","","1573-0565","10.1007/BF00993309","https://doi.org/10.1007/BF00993309","","1994-09-01","2023-04-06 22:03:30","2023-04-06 22:03:33","2023-04-06 22:03:30","235-240","","3","16","","Mach Learn","C4.5","","","","","","","en","","","","","Springer Link","","386 citations (Crossref) [2023-04-07]","","C:\Users\ambreen.hanif\Zotero\storage\PKMZX4DL\Salzberg_1994_C4.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J796E2LW","journalArticle","2012","Augasta, M. Gethsiyal; Kathirvalavakumar, T.","Reverse Engineering the Neural Networks for Rule Extraction in Classification Problems","Neural Processing Letters","","1573-773X","10.1007/s11063-011-9207-8","https://doi.org/10.1007/s11063-011-9207-8","Artificial neural networks often achieve high classification accuracy rates, but they are considered as black boxes due to their lack of explanation capability. This paper proposes the new rule extraction algorithm RxREN to overcome this drawback. In pedagogical approach the proposed algorithm extracts the rules from trained neural networks for datasets with mixed mode attributes. The algorithm relies on reverse engineering technique to prune the insignificant input neurons and to discover the technological principles of each significant input neuron of neural network in classification. The novelty of this algorithm lies in the simplicity of the extracted rules and conditions in rule are involving both discrete and continuous mode of attributes. Experimentation using six different real datasets namely iris, wbc, hepatitis, pid, ionosphere and creditg show that the proposed algorithm is quite efficient in extracting smallest set of rules with high classification accuracy than those generated by other neural network rule extraction methods.","2012-04-01","2023-04-06 22:02:51","2023-04-06 22:02:55","2023-04-06 22:02:51","131-150","","2","35","","Neural Process Lett","","","","","","","","en","","","","","Springer Link","","86 citations (Crossref) [2023-04-07]","","C:\Users\ambreen.hanif\Zotero\storage\3GGX5UIA\Augasta_Kathirvalavakumar_2012_Reverse Engineering the Neural Networks for Rule Extraction in Classification.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DU2YNTX9","conferencePaper","2001","Sato, M.; Tsukimoto, H.","Rule extraction from neural networks via decision tree induction","IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222)","","","10.1109/IJCNN.2001.938448","","Rule extraction from neural networks is the task for obtaining comprehensible descriptions that approximate the predictive behavior of neural networks. Rule-extraction algorithms are used for both interpreting neural networks and mining the relationship between input and output variables in data. This paper describes a new rule extraction algorithm that extracts rules that contain both continuous (real-valued) and discrete literals. This algorithm decomposes a neural network using decision trees and obtains production rules by merging the rules extracted from each tree. Results tested on the databases in UCI repository are presented.","2001-07","2023-04-06 22:02:06","2023-04-06 22:02:11","","1870-1875 vol.3","","","3","","","","","","","","","","","","","","","IEEE Xplore","","34 citations (Crossref) [2023-04-07] ISSN: 1098-7576","","C:\Users\ambreen.hanif\Zotero\storage\45RCJCQM\938448.html; C:\Users\ambreen.hanif\Zotero\storage\BNKMR768\Sato_Tsukimoto_2001_Rule extraction from neural networks via decision tree induction.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","IJCNN'01. International Joint Conference on Neural Networks. Proceedings (Cat. No.01CH37222)","","","","","","","","","","","","","","",""
"PY5ILQ3K","computerProgram","2023","","SelfExplainML/PiML-Toolbox","","","","","https://github.com/SelfExplainML/PiML-Toolbox","PiML (Python Interpretable Machine Learning) toolbox for model development and model validation","2023-04-06","2023-04-06 22:01:12","2023-04-06 22:01:12","2023-04-06 22:01:12","","","","","","","","","","","","Self-Explanatory Machine Learning","","","Apache-2.0","","","","GitHub","","original-date: 2022-04-29T08:40:20Z","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Jupyter Notebook","","","","","","","","",""
"ZDBKZ5HN","journalArticle","2017","Samek, Wojciech; Wiegand, Thomas; Müller, Klaus-Robert","Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models","arXiv preprint arXiv:1708.08296","","","","http://arxiv.org/abs/1708.08296","With the availability of large databases and recent improvements in deep learning methodology, the performance of AI systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classification, sentiment analysis, speech understanding or strategic game playing. However, because of their nested non-linear structure, these highly successful machine learning and artificial intelligence models are usually applied in a black box manner, i.e., no information is provided about what exactly makes them arrive at their predictions. Since this lack of transparency can be a major drawback, e.g., in medical applications, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This paper summarizes recent developments in this field and makes a plea for more interpretability in artificial intelligence. Furthermore, it presents two approaches to explaining predictions of deep learning models, one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. These methods are evaluated on three classification tasks.","2017-08-28","2022-06-21 01:12:26","2023-04-06 21:31:23","2022-06-21","","","","","","","","","","","","","","","","","","","","","arXiv: 1708.08296 QID: Q38135445","","","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4SM79869","journalArticle","2022","Herm, Lukas-Valentin; Heinrich, Kai; Wanner, Jonas; Janiesch, Christian","Stop ordering machine learning algorithms by their explainability! A user-centered investigation of performance and explainability","International Journal of Information Management","","","10.1016/j.ijinfomgt.2022.102538","","Machine learning algorithms enable advanced decision making in contemporary intelligent systems. Research indicates that there is a tradeoff between their model performance and explainability. Machine learning models with higher performance are often based on more complex algorithms and therefore lack explainability and vice versa. However, there is little to no empirical evidence of this tradeoff from an end user perspective. We aim to provide empirical evidence by conducting two user experiments. Using two distinct datasets, we first measure the tradeoff for five common classes of machine learning algorithms. Second, we address the problem of end user perceptions of explainable artificial intelligence augmentations aimed at increasing the understanding of the decision logic of high-performing complex models. Our results diverge from the widespread assumption of a tradeoff curve and indicate that the tradeoff between model performance and explainability is much less gradual in the end user’s perception. This is a stark contrast to assumed inherent model interpretability. Further, we found the tradeoff to be situational for example due to data complexity. Results of our second experiment show that while explainable artificial intelligence augmentations can be used to increase explainability, the type of explanation plays an essential role in end user perception.","2022-06-01","2023-04-06 21:21:25","2023-04-06 21:21:28","","102538","","","69","","International Journal of Information Management","","","","","","","","","","","","","ResearchGate","","7 citations (Crossref) [2023-04-07]","","C:\Users\ambreen.hanif\Zotero\storage\4GV372WB\Herm et al_2022_Stop ordering machine learning algorithms by their explainability.pdf; ","https://www.researchgate.net/publication/361398589_Stop_ordering_machine_learning_algorithms_by_their_explainability_A_user-centered_investigation_of_performance_and_explainability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZIAK3EBS","conferencePaper","2020","Wanner, Jonas; Herm, Lukas-Valentin; Heinrich, Kai","White, Grey, Black: Effects of XAI Augmentation on the Confidence in AI-based Decision Support Systems","ICIS","","","","https://www.researchgate.net/publication/344357897","","2020","2021-09-27 04:18:43","2023-04-06 21:21:27","2021-09-27","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\3N4EZNIE\Wanner et al_2020_White, Grey, Black.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BQ9IMDVU","journalArticle","2022","Ras, Gabriëlle; Xie, Ning; Van Gerven, Marcel; Doran, Derek","Explainable Deep Learning:A Field Guide for the Uninitiated","Journal of Artificial Intelligence Research","","","","","Deep neural networks (DNNs) have become a proven and indispensable machine learning tool. As a black-box model, it remains difficult to diagnose what aspects of the model's input drive the decisions of a DNN. In countless real-world domains, from legislation and law enforcement to healthcare, such diagnosis is essential to ensure that DNN decisions are driven by aspects appropriate in the context of its use. The development of methods and studies enabling the explanation of a DNN's decisions has thus blossomed into an active, broad area of research. A practitioner wanting to study explainable deep learning may be intimidated by the plethora of orthogonal directions the field has taken. This complexity is further exacerbated by competing definitions of what it means ""to explain"" the actions of a DNN and to evaluate an approach's ""ability to explain"". This article offers a field guide to explore the space of explainable deep learning aimed at those uninitiated in the field. The field guide: i) Introduces three simple dimensions defining the space of founda-tional methods that contribute to explainable deep learning, ii) discusses the evaluations for model explanations, iii) places explainability in the context of other related deep learning research areas, and iv) finally elaborates on user-oriented explanation designing and potential future directions on explainable deep learning. We hope the guide is used as an easy-to-digest starting point for those just embarking on research in this field.","2022","2021-09-28 05:44:13","2023-04-06 21:06:20","2021-09-28","329-397","","","73","","","","","","","","","","","","","","","","","arXiv: 2004.14545v2","","C:\Users\ambreen.hanif\Zotero\storage\RFK9LMR8\Ras et al_Explainable Deep Learning.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4A2IWXGZ","webpage","","","[1711.07414] The Promise and Peril of Human Evaluation for Model Interpretability","","","","","https://arxiv.org/abs/1711.07414","","","2023-04-06 20:15:09","2023-04-06 20:15:09","2023-04-06 20:15:09","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\VZC29497\1711.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PYIMHVVZ","webpage","","","No PDF for 1711.07414","","","","","https://arxiv.org/pdf/1711.07414.pdf","","","2023-04-06 20:14:56","2023-04-06 20:14:56","2023-04-06 20:14:56","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\P654QP6P\1711.07414.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"79ZF9Q9X","preprint","2019","Slack, Dylan; Friedler, Sorelle A.; Scheidegger, Carlos; Roy, Chitradeep Dutta","Assessing the Local Interpretability of Machine Learning Models","","","","10.48550/arXiv.1902.03501","http://arxiv.org/abs/1902.03501","The increasing adoption of machine learning tools has led to calls for accountability via model interpretability. But what does it mean for a machine learning model to be interpretable by humans, and how can this be assessed? We focus on two definitions of interpretability that have been introduced in the machine learning literature: simulatability (a user's ability to run a model on a given input) and ""what if"" local explainability (a user's ability to correctly determine a model's prediction under local changes to the input, given knowledge of the model's original prediction). Through a user study with 1,000 participants, we test whether humans perform well on tasks that mimic the definitions of simulatability and ""what if"" local explainability on models that are typically considered locally interpretable. To track the relative interpretability of models, we employ a simple metric, the runtime operation count on the simulatability task. We find evidence that as the number of operations increases, participant accuracy on the local interpretability tasks decreases. In addition, this evidence is consistent with the common intuition that decision trees and logistic regression models are interpretable and are more interpretable than neural networks.","2019-08-02","2023-04-06 19:51:46","2023-04-06 19:51:46","2023-04-06 19:51:46","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1902.03501 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\43CMG3T4\1902.html; C:\Users\ambreen.hanif\Zotero\storage\RN9C6GVB\Slack et al_2019_Assessing the Local Interpretability of Machine Learning Models.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1902.03501","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SA38PNL3","preprint","2019","Lage, Isaac; Chen, Emily; He, Jeffrey; Narayanan, Menaka; Kim, Been; Gershman, Sam; Doshi-Velez, Finale","An Evaluation of the Human-Interpretability of Explanation","","","","","http://arxiv.org/abs/1902.00006","Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable under three specific tasks that users may perform with machine learning systems: simulation of the response, verification of a suggested response, and determining whether the correctness of a suggested response changes under a change to the inputs. Through carefully controlled human-subject experiments, we identify regularizers that can be used to optimize for the interpretability of machine learning systems. Our results show that the type of complexity matters: cognitive chunks (newly defined concepts) affect performance more than variable repetitions, and these trends are consistent across tasks and domains. This suggests that there may exist some common design principles for explanation systems.","2019-08-28","2023-04-06 07:09:59","2023-04-06 07:09:59","2023-04-06 07:09:59","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1902.00006 [cs, stat] version: 2","","C:\Users\ambreen.hanif\Zotero\storage\3DWR2HYQ\1902.html; C:\Users\ambreen.hanif\Zotero\storage\GEP8AD6E\Lage et al_2019_An Evaluation of the Human-Interpretability of Explanation.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1902.00006","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S8RTASYD","preprint","2021","Poursabzi-Sangdeh, Forough; Goldstein, Daniel G.; Hofman, Jake M.; Vaughan, Jennifer Wortman; Wallach, Hanna","Manipulating and Measuring Model Interpretability","","","","10.48550/arXiv.1802.07810","http://arxiv.org/abs/1802.07810","With machine learning models being increasingly used to aid decision making even in high-stakes domains, there has been a growing interest in developing interpretable models. Although many supposedly interpretable models have been proposed, there have been relatively few experimental studies investigating whether these models achieve their intended effects, such as making people more closely follow a model's predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake. We present a sequence of pre-registered experiments (N=3,800) in which we showed participants functionally identical models that varied only in two factors commonly thought to make machine learning models more or less interpretable: the number of features and the transparency of the model (i.e., whether the model internals are clear or black box). Predictably, participants who saw a clear model with few features could better simulate the model's predictions. However, we did not find that participants more closely followed its predictions. Furthermore, showing participants a clear model meant that they were less able to detect and correct for the model's sizable mistakes, seemingly due to information overload. These counterintuitive findings emphasize the importance of testing over intuition when developing interpretable models.","2021-08-15","2023-04-06 07:08:46","2023-04-06 07:08:46","2023-04-06 07:08:46","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1802.07810 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\PVHJZXWX\1802.html; C:\Users\ambreen.hanif\Zotero\storage\76WDV9FF\Poursabzi-Sangdeh et al_2021_Manipulating and Measuring Model Interpretability.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1802.07810","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DNUBP3MR","journalArticle","2011","Huysmans, Johan; Dejaeger, Karel; Mues, Christophe; Vanthienen, Jan; Baesens, Bart","An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models","Decision Support Systems","","0167-9236","10.1016/j.dss.2010.12.003","https://www.sciencedirect.com/science/article/pii/S0167923610002368","An important objective of data mining is the development of predictive models. Based on a number of observations, a model is constructed that allows the analysts to provide classifications or predictions for new observations. Currently, most research focuses on improving the accuracy or precision of these models and comparatively little research has been undertaken to increase their comprehensibility to the analyst or end-user. This is mainly due to the subjective nature of ‘comprehensibility’, which depends on many factors outside the model, such as the user's experience and his/her prior knowledge. Despite this influence of the observer, some representation formats are generally considered to be more easily interpretable than others. In this paper, an empirical study is presented which investigates the suitability of a number of alternative representation formats for classification when interpretability is a key requirement. The formats under consideration are decision tables, (binary) decision trees, propositional rules, and oblique rules. An end-user experiment was designed to test the accuracy, response time, and answer confidence for a set of problem-solving tasks involving the former representations. Analysis of the results reveals that decision tables perform significantly better on all three criteria, while post-test voting also reveals a clear preference of users for decision tables in terms of ease of use.","2011-04-01","2023-04-06 07:08:12","2023-04-06 07:08:15","2023-04-06 07:08:11","141-154","","1","51","","Decision Support Systems","","","","","","","","en","","","","","ScienceDirect","","203 citations (Crossref) [2023-04-06]","","C:\Users\ambreen.hanif\Zotero\storage\EJUH4ZPX\Huysmans et al_2011_An empirical evaluation of the comprehensibility of decision table, tree and.pdf; C:\Users\ambreen.hanif\Zotero\storage\66RBD5JC\S0167923610002368.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4GVVPE6L","preprint","2021","Poursabzi-Sangdeh, Forough; Goldstein, Daniel G.; Hofman, Jake M.; Vaughan, Jennifer Wortman; Wallach, Hanna","Manipulating and Measuring Model Interpretability","","","","","http://arxiv.org/abs/1802.07810","With machine learning models being increasingly used to aid decision making even in high-stakes domains, there has been a growing interest in developing interpretable models. Although many supposedly interpretable models have been proposed, there have been relatively few experimental studies investigating whether these models achieve their intended effects, such as making people more closely follow a model's predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake. We present a sequence of pre-registered experiments (N=3,800) in which we showed participants functionally identical models that varied only in two factors commonly thought to make machine learning models more or less interpretable: the number of features and the transparency of the model (i.e., whether the model internals are clear or black box). Predictably, participants who saw a clear model with few features could better simulate the model's predictions. However, we did not find that participants more closely followed its predictions. Furthermore, showing participants a clear model meant that they were less able to detect and correct for the model's sizable mistakes, seemingly due to information overload. These counterintuitive findings emphasize the importance of testing over intuition when developing interpretable models.","2021-08-15","2023-04-06 06:27:22","2023-04-06 06:27:22","2023-04-06 06:27:22","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1802.07810 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\B3V6JMX8\1802.html; C:\Users\ambreen.hanif\Zotero\storage\ZC2Z5CYP\Poursabzi-Sangdeh et al_2021_Manipulating and Measuring Model Interpretability.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1802.07810","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W38DALKA","conferencePaper","2013","Kulesza, Todd; Stumpf, Simone; Burnett, Margaret; Yang, Sherry; Kwan, Irwin; Wong, Weng-Keen","Too much, too little, or just right? Ways explanations impact end users' mental models","2013 IEEE Symposium on Visual Languages and Human Centric Computing","","","10.1109/VLHCC.2013.6645235","","Research is emerging on how end users can correct mistakes their intelligent agents make, but before users can correctly “debug” an intelligent agent, they need some degree of understanding of how it works. In this paper we consider ways intelligent agents should explain themselves to end users, especially focusing on how the soundness and completeness of the explanations impacts the fidelity of end users' mental models. Our findings suggest that completeness is more important than soundness: increasing completeness via certain information types helped participants' mental models and, surprisingly, their perception of the cost/benefit tradeoff of attending to the explanations. We also found that oversimplification, as per many commercial agents, can be a problem: when soundness was very low, participants experienced more mental demand and lost trust in the explanations, thereby reducing the likelihood that users will pay attention to such explanations at all.","2013-09","2023-04-06 06:13:43","2023-04-06 06:13:46","","3-10","","","","","","Too much, too little, or just right?","","","","","","","","","","","","IEEE Xplore","","130 citations (Crossref) [2023-04-06] ISSN: 1943-6106","","C:\Users\ambreen.hanif\Zotero\storage\QQ2W8XNZ\6645235.html; C:\Users\ambreen.hanif\Zotero\storage\8YA86QSF\Kulesza et al_2013_Too much, too little, or just right.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2013 IEEE Symposium on Visual Languages and Human Centric Computing","","","","","","","","","","","","","","",""
"HGUDMMAG","journalArticle","2023","Kaur, Davinder; Uslu, Suleyman; Rittichier, Kaley J.; Durresi, Arjan","Trustworthy Artificial Intelligence: A Review","ACM Computing Surveys","","0360-0300, 1557-7341","10.1145/3491209","https://dl.acm.org/doi/10.1145/3491209","Artificial intelligence (AI) and algorithmic decision making are having a profound impact on our daily lives. These systems are vastly used in different high-stakes applications like healthcare, business, government, education, and justice, moving us toward a more algorithmic society. However, despite so many advantages of these systems, they sometimes directly or indirectly cause harm to the users and society. Therefore, it has become essential to make these systems safe, reliable, and trustworthy. Several requirements, such as fairness, explainability, accountability, reliability, and acceptance, have been proposed in this direction to make these systems trustworthy. This survey analyzes all of these different requirements through the lens of the literature. It provides an overview of different approaches that can help mitigate AI risks and increase trust and acceptance of the systems by utilizing the users and society. It also discusses existing strategies for validating and verifying these systems and the current standardization efforts for trustworthy AI. Finally, we present a holistic view of the recent advancements in trustworthy AI to help the interested researchers grasp the crucial facets of the topic efficiently and offer possible future research directions.","2023-02-28","2023-04-06 06:06:27","2023-04-06 06:06:29","2023-04-06 06:06:27","1-38","","2","55","","ACM Comput. Surv.","Trustworthy Artificial Intelligence","","","","","","","en","","","","","DOI.org (Crossref)","","27 citations (Crossref) [2023-04-06]","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XXUD3XBR","conferencePaper","2019","Cabitza, Federico; Campagner, Andrea; Ciucci, Davide","New Frontiers in Explainable AI: Understanding the GI to Interpret the GO","Machine Learning and Knowledge Extraction","978-3-030-29726-8","","10.1007/978-3-030-29726-8_3","","In this paper we focus on the importance of interpreting the quality of the input of predictive models (potentially a GI, i.e., a Garbage In) to make sense of the reliability of their output (potentially a GO, a Garbage Out) in support of human decision making, especially in critical domains, like medicine. To this aim, we propose a framework where we distinguish between the Gold Standard (or Ground Truth) and the set of annotations from which this is derived, and a set of quality dimensions that help to assess and interpret the AI advice: fineness, trueness, representativeness, conformity, dryness. We then discuss implications for obtaining more informative training sets and for the design of more usable Decision Support Systems.","2019","2023-04-06 04:16:09","2023-04-06 04:16:12","","27-47","","","","","","New Frontiers in Explainable AI","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","6 citations (Crossref) [2023-04-06]","","C:\Users\ambreen.hanif\Zotero\storage\54AN5NGY\Cabitza et al_2019_New Frontiers in Explainable AI.pdf","","","","Holzinger, Andreas; Kieseberg, Peter; Tjoa, A Min; Weippl, Edgar","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4ELJFVWI","webpage","","","The Best Explanation: Criteria for Theory Choice - Paul R. Thagard - The Journal of Philosophy (Philosophy Documentation Center)","","","","","https://www.pdcnet.org/jphil/content/jphil_1978_0075_0002_0076_0092","","","2023-04-06 03:59:47","2023-04-06 03:59:47","2023-04-06 03:59:47","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\6I95XUV8\jphil_1978_0075_0002_0076_0092.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TJ7B6HDY","book","1993","Bromberger, Sylvain","On What We Know We Don’t Know: Explanation, Theory, Linguistics, and How Questions Shape Them","","978-0-226-07539-6","","","https://press.uchicago.edu/ucp/books/book/chicago/O/bo3643124.html","In this collection of essays, Bromberger explores the centrality of questions and predicaments they create in scientific research. He discusses the nature of explanation, theory, and the foundations of linguistics.","1993-01","2023-04-06 03:52:50","2023-04-06 03:52:50","2023-04-06 03:52:50","","240","","","","","On What We Know We Don’t Know","","","","","University of Chicago Press","Chicago, IL","en","","","","","University of Chicago Press","","","","C:\Users\ambreen.hanif\Zotero\storage\T5T5T4LY\bo3643124.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"E9F43VYY","conferencePaper","2015","Caruana, Rich; Lou, Yin; Gehrke, Johannes; Koch, Paul; Sturm, Marc; Elhadad, Noemie","Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission","Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","978-1-4503-3664-2","","10.1145/2783258.2788613","https://doi.org/10.1145/2783258.2788613","In machine learning often a tradeoff must be made between accuracy and intelligibility. More accurate models such as boosted trees, random forests, and neural nets usually are not intelligible, but more intelligible models such as logistic regression, naive-Bayes, and single decision trees often have significantly worse accuracy. This tradeoff sometimes limits the accuracy of models that can be applied in mission-critical applications such as healthcare where being able to understand, validate, edit, and trust a learned model is important. We present two case studies where high-performance generalized additive models with pairwise interactions (GA2Ms) are applied to real healthcare problems yielding intelligible models with state-of-the-art accuracy. In the pneumonia risk prediction case study, the intelligible model uncovers surprising patterns in the data that previously had prevented complex learned models from being fielded in this domain, but because it is intelligible and modular allows these patterns to be recognized and removed. In the 30-day hospital readmission case study, we show that the same methods scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods.","2015-08-10","2023-01-29 21:32:24","2023-05-23 01:27:26","2023-01-29","1721–1730","","","","","","Intelligible Models for HealthCare","KDD '15","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","C:\Users\ambreen.hanif\Zotero\storage\PDYQWY3C\Caruana et al_2015_Intelligible Models for HealthCare.pdf; ","notion://www.notion.so/Caruana-et-al-2015-a8ddc6b86e96432fbf04d2017f54c00f","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VCHYMYCK","report","2021","Phillips, P. Jonathon; Hahn, Carina A.; Fontana, Peter C.; Yates, Amy N.; Greene, Kristen; Broniatowski, David A.; Przybocki, Mark A.","Four Principles of Explainable Artificial Intelligence","","","","","https://nvlpubs.nist.gov/nistpubs/ir/2021/NIST.IR.8312.pdf","We introduce four principles for explainable artiﬁcial intelligence (AI) that comprise fundamental properties for explainable AI systems. We propose that explainable AI systems deliver accompanying evidence or reasons for outcomes and processes; provide explanations that are understandable to individual users; provide explanations that correctly reﬂect the system’s process for generating the output; and that a system only operates under conditions for which it was designed and when it reaches sufﬁcient conﬁdence in its output. We have termed these four principles as explanation, meaningful, explanation accuracy, and knowledge limits, respectively. Through signiﬁcant stakeholder engagement, these four principles were developed to encompass the multidisciplinary nature of explainable AI, including the ﬁelds of computer science, engineering, and psychology. Because one-sizeﬁts-all explanations do not exist, different users will require different types of explanations. We present ﬁve categories of explanation and summarize theories of explainable AI. We give an overview of the algorithms in the ﬁeld that cover the major classes of explainable algorithms. As a baseline comparison, we assess how well explanations provided by people follow our four principles. This assessment provides insights to the challenges of designing explainable AI systems.","2021-09-29","2023-01-29 21:29:09","2023-05-23 01:27:22","2023-01-29 21:29:09","","","","","","","","","","","","National Institute of Standards and Technology","","en","","","","","DOI.org (Crossref)","","DOI: 10.6028/NIST.IR.8312","","C:\Users\ambreen.hanif\Zotero\storage\7QQ6LBTS\NIST.IR.8312.pdf; ","notion://www.notion.so/Phillips-et-al-2021-3208c3bd372a4e789520b293371ee0de","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AQBS6PFJ","conferencePaper","2020","Ehsan, Upol; Riedl, Mark O.","Human-Centered Explainable AI: Towards a Reflective Sociotechnical Approach","HCI International 2020 - Late Breaking Papers: Multimodality and Intelligence","978-3-030-60117-1","","10.1007/978-3-030-60117-1_33","","Explanations—a form of post-hoc interpretability—play an instrumental role in making systems accessible as AI continues to proliferate complex and sensitive sociotechnical systems. In this paper, we introduce Human-centered Explainable AI (HCXAI) as an approach that puts the human at the center of technology design. It develops a holistic understanding of “who” the human is by considering the interplay of values, interpersonal dynamics, and the socially situated nature of AI systems. In particular, we advocate for a reflective sociotechnical approach. We illustrate HCXAI through a case study of an explanation system for non-technical end-users that shows how technical advancements and the understanding of human factors co-evolve. Building on the case study, we lay out open research questions pertaining to further refining our understanding of “who” the human is and extending beyond 1-to-1 human-computer interactions. Finally, we propose that a reflective HCXAI paradigm—mediated through the perspective of Critical Technical Practice and supplemented with strategies from HCI, such as value-sensitive design and participatory design—not only helps us understand our intellectual blind spots, but it can also open up new design and research spaces.","2020","2023-01-27 00:14:46","2023-05-23 01:27:20","","449-466","","","","","","Human-Centered Explainable AI","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","C:\Users\ambreen.hanif\Zotero\storage\J3QT7LXA\Ehsan_Riedl_2020_Human-Centered Explainable AI.pdf; ","notion://www.notion.so/Ehsan-Riedl-2020-255f03baed2c41599090a0da42357ca1","notion","","Stephanidis, Constantine; Kurosu, Masaaki; Degen, Helmut; Reinerman-Jones, Lauren","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EEH7U42A","journalArticle","2022","Fu, Tingchen; Gao, Shen; Zhao, Xueliang; Wen, Ji-rong; Yan, Rui","Learning towards conversational AI: A survey","AI Open","","2666-6510","10.1016/j.aiopen.2022.02.001","https://www.sciencedirect.com/science/article/pii/S2666651022000079","Recent years have witnessed a surge of interest in the field of open-domain dialogue. Thanks to the rapid development of social media, large dialogue corpus from the Internet builds up a fundamental premise for data-driven dialogue model. The breakthrough in neural network also brings new ideas to researchers in AI and NLP. A great number of new techniques and methods therefore came into being. In this paper, we review some of the most representative works in recent years and divide existing prevailing frameworks for a dialogue model into three categories. We further analyze the trend of development for open-domain dialogue and summarize the goal of an open-domain dialogue system in two aspects, informative and controllable. The methods we review in this paper are selected according to our unique perspectives and by no means complete. Rather, we hope this servery could benefit NLP community for future research in open-domain dialogue.","2022-01-01","2023-01-25 06:21:03","2023-05-23 01:27:16","2023-01-25 06:21:03","14-28","","","3","","AI Open","Learning towards conversational AI","","","","","","","en","","","","","ScienceDirect","","","","C:\Users\ambreen.hanif\Zotero\storage\5QY5333K\Fu et al_2022_Learning towards conversational AI.pdf; ; C:\Users\ambreen.hanif\Zotero\storage\SJ7MSK2M\S2666651022000079.html","notion://www.notion.so/Fu-et-al-2022-f5bcd7689251441ca0d8eec4899ac123","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4BMGUPPB","journalArticle","2021","Rheu, Minjin; Shin, Ji Youn; Peng, Wei; Huh-Yoo, Jina","Systematic Review: Trust-Building Factors and Implications for Conversational Agent Design","International Journal of Human–Computer Interaction","","1044-7318","10.1080/10447318.2020.1807710","https://doi.org/10.1080/10447318.2020.1807710","Off-the-shelf conversational agents are permeating people’s everyday lives. In these artificial intelligence devices, trust plays a key role in users’ initial adoption and successful utilization. Factors enhancing trust toward conversational agents include appearances, voice features, and communication styles. Synthesizing such work will be useful in designing evidence-based, trustworthy conversational agents appropriate for various contexts. We conducted a systematic review of the experimental studies that investigated the effect of conversational agents’ and users’ characteristics on trust. From a full-text review of 29 articles, we identified five agent design-themes affecting trust toward conversational agents: social intelligence of the agent, voice characteristics and communication style, look of the agent, non-verbal communication, and performance quality. We also found that participants’ demographic, personality, or use context moderate the effect of these themes. We discuss implications for designing trustworthy conversational agents and responsibilities around on stereotypes and social norm building through agent design.","2021-01-02","2023-01-25 06:20:36","2023-05-23 01:27:12","2023-01-25 06:20:36","81-96","","1","37","","","Systematic Review","","","","","","","","","","","","Taylor and Francis+NEJM","","Publisher: Taylor & Francis _eprint: https://doi.org/10.1080/10447318.2020.1807710","","; C:\Users\ambreen.hanif\Zotero\storage\PUK7RV33\Rheu et al_2021_Systematic Review.pdf","notion://www.notion.so/Rheu-et-al-2021-b373c7daaa224016abcf2ea6ff7a2e6d","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"24CA8PEQ","journalArticle","2015","Mayer-Schönberger, Viktor","Connecting the dots <b>The Black Box Society: The Secret Algorithms That Control Money and Information</b> <i>Frank Pasquale</i> Harvard University Press, 2015. 319 pp.","Science","","0036-8075, 1095-9203","10.1126/science.aaa5358","https://www.science.org/doi/10.1126/science.aaa5358","When it comes to the data that can make or break us, who holds the power?           ,                             Troves of our personal data are being collected and analyzed every day by players who have the power to influence what we see online and how we are seen in real life. The methods by which this information is collected and analyzed are shockingly opaque, and attempts to protect our privacy are no longer effective (if they ever were). Viktor Mayer-Schönberger advances the ongoing debate over Internet privacy in a review of               The Black Box Society: The Secret Algorithms that Control Money and Information               .","2015-01-30","2023-01-20 03:00:28","2023-05-23 01:27:07","2023-01-20 03:00:28","481-481","","6221","347","","Science","Connecting the dots <b>The Black Box Society","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\ambreen.hanif\Zotero\storage\W6LVMUCW\Mayer-Schönberger - 2015 - Connecting the dots The Black Box Society The .pdf; ","notion://www.notion.so/Mayer-Sch-nberger-2015-e36d0020607845dcbc0b696c5c264608","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PHYHIIE8","preprint","2022","Aytekin, Caglar","Neural Networks are Decision Trees","","","","","http://arxiv.org/abs/2210.05189","In this manuscript, we show that any neural network with any activation function can be represented as a decision tree. The representation is equivalence and not an approximation, thus keeping the accuracy of the neural network exactly as is. We believe that this work provides better understanding of neural networks and paves the way to tackle their black-box nature. We share equivalent trees of some neural networks and show that besides providing interpretability, tree representation can also achieve some computational advantages for small networks. The analysis holds both for fully connected and convolutional networks, which may or may not also include skip connections and/or normalizations.","2022-10-25","2023-01-17 03:28:12","2023-05-23 01:27:01","2023-01-17 03:28:12","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2210.05189 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\F6CJMVZ7\2210.html; C:\Users\ambreen.hanif\Zotero\storage\8IMAQKJ5\Aytekin_2022_Neural Networks are Decision Trees.pdf; ","notion://www.notion.so/Aytekin-2022-2bd7fc9053d443d09f529d6941f835f8","notion","","","","","","","","","","","","","","","","","","","","arXiv:2210.05189","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9J4BFEWK","preprint","2019","Ghorbani, Amirata; Zou, James","Data Shapley: Equitable Valuation of Data for Machine Learning","","","","","http://arxiv.org/abs/1904.02868","As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this work, we develop a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on $n$ data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley value uniquely satisfies several natural properties of equitable data valuation. We develop Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data Shapley has several other benefits: 1) it is more powerful than the popular leave-one-out or leverage score in providing insight on what data is more valuable for a given learning task; 2) low Shapley value data effectively capture outliers and corruptions; 3) high Shapley value data inform what type of new data to acquire to improve the predictor.","2019-06-10","2023-01-12 23:23:50","2023-05-23 01:26:58","2023-01-12 23:23:50","","","","","","","Data Shapley","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1904.02868 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\NA6Y8BS2\1904.html; C:\Users\ambreen.hanif\Zotero\storage\JZXR9W3V\Ghorbani_Zou_2019_Data Shapley.pdf; ","notion://www.notion.so/Ghorbani-Zou-2019-9080a536ebbf4affa0f6ef74bdec727a","notion","","","","","","","","","","","","","","","","","","","","arXiv:1904.02868","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UAIJLMLD","webpage","","","ml-basics/02 - Real Estate Regression Challenge.ipynb at master · MicrosoftDocs/ml-basics","GitHub","","","","https://github.com/MicrosoftDocs/ml-basics","Exercise notebooks for Machine Learning modules on Microsoft Learn - ml-basics/02 - Real Estate Regression Challenge.ipynb at master · MicrosoftDocs/ml-basics","","2023-01-10 04:00:49","2023-05-23 01:26:54","2023-01-10 04:00:49","","","","","","","","","","","","","","en","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\DJYF3ZPC\challenges.html","notion://www.notion.so/Ml-Basics-02-Real-Estate-Regression-Challenge-Ipynb-at-Master-MicrosoftDocs-Ml-Basics-n-d-15faacfc84ff477cbd6b569ebbf8f0b6","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5Y56MZBJ","journalArticle","2021","Varitimiadis, Savvas; Kotis, Konstantinos; Pittou, Dimitra; Konstantakis, Georgios","Graph-Based Conversational AI: Towards a Distributed and Collaborative Multi-Chatbot Approach for Museums","Applied Sciences","","2076-3417","10.3390/app11199160","https://www.mdpi.com/2076-3417/11/19/9160","Nowadays, museums are developing chatbots to assist their visitors and to provide an enhanced visiting experience. Most of these chatbots do not provide a human-like conversation and fail to deliver the complete requested knowledge by the visitors. There are plenty of stand-alone museum chatbots, developed using a chatbot platform, that provide predefined dialog routes. However, as chatbot platforms are evolving and AI technologies mature, new architectural approaches arise. Museums are already designing chatbots that are trained using machine learning techniques or chatbots connected to knowledge graphs, delivering more intelligent chatbots. This paper is surveying a representative set of developed museum chatbots and platforms for implementing them. More importantly, this paper presents the result of a systematic evaluation approach for evaluating both chatbots and platforms. Furthermore, the paper is introducing a novel approach in developing intelligent chatbots for museums. This approach emphasizes graph-based, distributed, and collaborative multi-chatbot conversational AI systems for museums. The paper accentuates the use of knowledge graphs as the key technology for potentially providing unlimited knowledge to chatbot users, satisfying conversational AI’s need for rich machine-understandable content. In addition, the proposed architecture is designed to deliver an efficient deployment solution where knowledge can be distributed (distributed knowledge graphs) and shared among different chatbots that collaborate when is needed.","2021-01","2023-01-10 03:59:30","2023-05-23 01:26:51","2023-01-10 03:59:30","9160","","19","11","","","Graph-Based Conversational AI","","","","","","","en","http://creativecommons.org/licenses/by/3.0/","","","","www.mdpi.com","","Number: 19 Publisher: Multidisciplinary Digital Publishing Institute","","; C:\Users\ambreen.hanif\Zotero\storage\CEZMLCJQ\Varitimiadis et al_2021_Graph-Based Conversational AI.pdf","notion://www.notion.so/Varitimiadis-et-al-2021-6c833d5fc4894969add029d97db175a6","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YI9Q9YF2","journalArticle","2022","","A survey on near-human conversational agents","Journal of King Saud University - Computer and Information Sciences","","1319-1578","10.1016/j.jksuci.2021.10.013","https://www.sciencedirect.com/science/article/pii/S1319157821003001","Conversational AI intends for machine-human interactions to appear and feel more natural and inclined to communicate in a near-human context. Chatbots…","2022-11-01","2023-01-10 03:58:42","2023-05-23 01:26:49","2023-01-10 03:58:42","8852-8866","","10","34","","","","","","","","","","en","","","","","www-sciencedirect-com.simsrad.net.ocs.mq.edu.au","","Publisher: Elsevier","","; C:\Users\ambreen.hanif\Zotero\storage\59ZCT95Q\S1319157821003001.html","notion://www.notion.so/A-Survey-on-Near-Human-Conversational-Agents-2022-081f13934e9f44dd9c527ed3f20ce45c","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D4MSW4EJ","journalArticle","2019","Montenegro, Joao Luis Zeni; da Costa, Cristiano André; da Rosa Righi, Rodrigo","Survey of conversational agents in health","Expert Systems with Applications","","0957-4174","10.1016/j.eswa.2019.03.054","https://www.sciencedirect.com/science/article/pii/S0957417419302283","Artificial intelligence (AI) has transformed the world and the relationships among humans as the learning capabilities of machines have allowed for a new means of communication between humans and machines. In the field of health, there is much interest in new technologies that help to improve and automate services in hospitals. This article aims to explore the literature related to conversational agents applied to health care, searching for definitions, patterns, methods, architectures, and data types. Furthermore, this work identifies an agent application taxonomy, current challenges, and research gaps. In this work, we use a systematic literature review approach. We guide and refine this study and the research questions by applying Population, Intervention, Comparison, Outcome, and Context (PICOC) criteria. The present study investigated approximately 4145 articles involving conversational agents in health published over the last ten years. In this context, we finally selected 40 articles based on their approaches and objectives as related to our main subject. As a result, we developed a taxonomy, identified the main challenges in the field, and defined the main types of dialog and contexts related to conversational agents in health. These results contributed to discussions regarding conversational health agents, and highlighted some research gaps for future study.","2019-09-01","2023-01-10 03:57:55","2023-05-23 01:26:45","2023-01-10 03:57:54","56-67","","","129","","Expert Systems with Applications","","","","","","","","en","","","","","ScienceDirect","","","","C:\Users\ambreen.hanif\Zotero\storage\P873NZTF\Montenegro et al_2019_Survey of conversational agents in health.pdf; ; C:\Users\ambreen.hanif\Zotero\storage\CCSJTVXW\S0957417419302283.html","notion://www.notion.so/Montenegro-et-al-2019-38e6715ec07c4d2aa7d8df49221cb203","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XBI4DR23","conferencePaper","2018","Lee, Kimin; Lee, Kibok; Lee, Honglak; Shin, Jinwoo","A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks","Advances in Neural Information Processing Systems","","","","https://proceedings.neurips.cc/paper/2018/hash/abdeb6f575ac5c6676b747bca8d09cc2-Abstract.html","Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (low- and upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-the-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in harsh cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that the proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models.","2018","2023-01-06 00:18:48","2023-05-23 01:26:43","2023-01-06 00:18:48","","","","31","","","","","","","","Curran Associates, Inc.","","","","","","","Neural Information Processing Systems","","","","C:\Users\ambreen.hanif\Zotero\storage\QSUIM5ER\Lee et al_2018_A Simple Unified Framework for Detecting Out-of-Distribution Samples and.pdf; ","notion://www.notion.so/Lee-et-al-2018-a623cf06253646608aa1e223cb017fdc","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EC3KI2UY","webpage","","","Amirata Ghorbani - Publications","","","","","https://www.amiratag.com/publications","2022:","","2023-01-05 23:37:45","2023-05-23 01:26:38","2023-01-05 23:37:45","","","","","","","","","","","","","","en-GB","","","","","","","","","","notion://www.notion.so/Amirata-Ghorbani-Publications-n-d-bd701f11fc19493abcb94fbacf33922e","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SZU8TFA8","journalArticle","2022","Liang, Weixin; Tadesse, Girmaw Abebe; Ho, Daniel; Fei-Fei, L.; Zaharia, Matei; Zhang, Ce; Zou, James","Advances, challenges and opportunities in creating data for trustworthy AI","Nature Machine Intelligence","","2522-5839","10.1038/s42256-022-00516-1","https://www.nature.com/articles/s42256-022-00516-1","As artificial intelligence (AI) transitions from research to deployment, creating the appropriate datasets and data pipelines to develop and evaluate AI models is increasingly the biggest challenge. Automated AI model builders that are publicly available can now achieve top performance in many applications. In contrast, the design and sculpting of the data used to develop AI often rely on bespoke manual work, and they critically affect the trustworthiness of the model. This Perspective discusses key considerations for each stage of the data-for-AI pipeline—starting from data design to data sculpting (for example, cleaning, valuation and annotation) and data evaluation—to make AI more reliable. We highlight technical advances that help to make the data-for-AI pipeline more scalable and rigorous. Furthermore, we discuss how recent data regulations and policies can impact AI.","2022-08","2023-01-05 23:24:26","2023-05-23 01:26:35","2023-01-05 23:24:26","669-677","","8","4","","Nat Mach Intell","","","","","","","","en","2022 Springer Nature Limited","","","","www-nature-com.simsrad.net.ocs.mq.edu.au","","Number: 8 Publisher: Nature Publishing Group","","C:\Users\ambreen.hanif\Zotero\storage\W6C2K2CH\Liang et al_2022_Advances, challenges and opportunities in creating data for trustworthy AI.pdf; ","notion://www.notion.so/Liang-et-al-2022-f05d53cd90a8430ca0d78b22d905d8b5","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KL9LA5TZ","conferencePaper","2023","Heng, Yi Sheng; Subramanian, Preethi","A Systematic Review of Machine Learning and Explainable Artificial Intelligence (XAI) in Credit Risk Modelling","Proceedings of the Future Technologies Conference (FTC) 2022, Volume 1","978-3-031-18461-1","","10.1007/978-3-031-18461-1_39","","The emergence of machine learning and artificial intelligence has created new opportunities for data-intensive science within the financial industry. The implementation of machine learning algorithms still faces doubt and distrust, mainly in the credit risk domain due to the lack of transparency in terms of decision making. This paper presents a comprehensive review of research dedicated to the application of machine learning in credit risk modelling and how Explainable Artificial Intelligence (XAI) could increase the robustness of a predictive model. In addition to that, some fully developed credit risk software available in the market is also reviewed. It is evident that adopting complex machine learning models produced high performance but had limited interpretability. Thus, the review also studies some XAI techniques that helps to overcome this problem whilst breaking out from the nature of the ‘black-box’ concept. XAI models mitigate the bias and establish trust and compliance with the regulators to ensure fairness in loan lending in the financial industry.","2023","2023-01-05 23:22:39","2023-05-23 01:26:32","","596-614","","","","","","","Lecture Notes in Networks and Systems","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","","","C:\Users\ambreen.hanif\Zotero\storage\9REMI94H\Heng_Subramanian_2023_A Systematic Review of Machine Learning and Explainable Artificial Intelligence.pdf; ","notion://www.notion.so/Heng-Subramanian-2023-dcec030346034a189e65e49041744105","notion","","Arai, Kohei","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FDUWXZE5","bookSection","2023","Nizam, Tasleem; Zafar, Sherin","Explainable Artificial Intelligence (XAI): Conception, Visualization and Assessment Approaches Towards Amenable XAI","Explainable Edge AI: A Futuristic Computing Perspective","978-3-031-18292-1","","","https://doi.org/10.1007/978-3-031-18292-1_3","For last decade, due to the accessibility of huge databases and recent advancements in deep learning methodology, machine learning systems have arrived at or transcended tremendous performance in a spacious variety of tasks. One can see this speedy development in speech analysis, image recognition, sentiment analysis, strategic game planning and many more, for e.g. in medical field, it’s used for diagnosing different diseases, like breast cancer etc., based on their symptoms. But many state-of- the-art models is facing lack of transparency and interpretability which is a major hindrance in many applications, e.g. finance and healthcare where visualization, interpretation and explanation for model's decision is an obligation for trust. This is an implicit problem of the current techniques carried by sub-symbolism (e.g. Deep Neural Networks) that were not shown in the last hype of AI (specifically, rule based models and expert systems). Models underlying this problem come within the so-called Explainable AI (XAI) field, which is extensively acknowledged as a racial feature for the practical deployment of AI models. As a result, explainable artificial intelligence (XAI) has turned into scientific interest in last recent years. So, this chapter epitomizes contemporary developments in Explainable AI that describes explainability in Machine Learning, constituting a fiction definition of explainable Machine Learning that envelopes such prior conceptual propositions with a considerable focus on the audience for which the explainability is needed. Except of this definition, this chapter starts a confabulation on its various techniques that are essentials for analysing interpretability and explainability of Artificial Intelligence, and also gives a comparison between two medical experiments, that are based on predicting heart disease using disparate Explainable Artificial Intelligence techniques, which can give a lead for researchers as well as practitioners or newcomers in the field of Artificial Intelligence for selecting suitable methods with Explainable AI to grasp the advances of AI in their action sectors, without any previous bias for its dearth of interpretability.","2023","2023-01-03 06:19:44","2023-05-23 01:26:30","2023-01-03 06:19:44","35-51","","","","","","Explainable Artificial Intelligence (XAI)","Studies in Computational Intelligence","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","DOI: 10.1007/978-3-031-18292-1_3","","C:\Users\ambreen.hanif\Zotero\storage\2WEWNEDN\Nizam_Zafar_2023_Explainable Artificial Intelligence (XAI).pdf; ","notion://www.notion.so/Nizam-Zafar-2023-9ddccdd19e094a0a932de68c31dd4ec5","notion; low-quality","","Hassanien, Aboul Ella; Gupta, Deepak; Singh, Anuj Kumar; Garg, Ankit","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JX8EMRKL","journalArticle","2023","Haque, AKM Bahalul; Islam, A. K. M. Najmul; Mikalef, Patrick","Explainable Artificial Intelligence (XAI) from a user perspective: A synthesis of prior literature and problematizing avenues for future research","Technological Forecasting and Social Change","","0040-1625","10.1016/j.techfore.2022.122120","https://www.sciencedirect.com/science/article/pii/S0040162522006412","The rapid growth and use of artificial intelligence (AI)-based systems have raised concerns regarding explainability. Recent studies have discussed the emerging demand for explainable AI (XAI); however, a systematic review of explainable artificial intelligence from an end user's perspective can provide a comprehensive understanding of the current situation and help close the research gap. The purpose of this study was to perform a systematic literature review of explainable AI from the end user's perspective and to synthesize the findings. To be precise, the objectives were to 1) identify the dimensions of end users' explanation needs; 2) investigate the effect of explanation on end user's perceptions, and 3) identify the research gaps and propose future research agendas for XAI, particularly from end users' perspectives based on current knowledge. The final search query for the Systematic Literature Review (SLR) was conducted on July 2022. Initially, we extracted 1707 journal and conference articles from the Scopus and Web of Science databases. Inclusion and exclusion criteria were then applied, and 58 articles were selected for the SLR. The findings show four dimensions that shape the AI explanation, which are format (explanation representation format), completeness (explanation should contain all required information, including the supplementary information), accuracy (information regarding the accuracy of the explanation), and currency (explanation should contain recent information). Moreover, along with the automatic representation of the explanation, the users can request additional information if needed. We have also described five dimensions of XAI effects: trust, transparency, understandability, usability, and fairness. We investigated current knowledge from selected articles to problematize future research agendas as research questions along with possible research paths. Consequently, a comprehensive framework of XAI and its possible effects on user behavior has been developed.","2023-01-01","2023-01-03 06:19:32","2023-05-23 01:26:26","2023-01-03 06:19:31","122120","","","186","","Technological Forecasting and Social Change","Explainable Artificial Intelligence (XAI) from a user perspective","","","","","","","en","","","","","ScienceDirect","","","","C:\Users\ambreen.hanif\Zotero\storage\Z2AFKSWR\Haque et al_2023_Explainable Artificial Intelligence (XAI) from a user perspective.pdf; ; C:\Users\ambreen.hanif\Zotero\storage\PVXVXK42\S0040162522006412.html","notion://www.notion.so/Haque-et-al-2023-52b2516c956e49bc8175848e6f9e92f9","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7Y8VVPF7","journalArticle","2023","Naiseh, Mohammad; Al-Thani, Dena; Jiang, Nan; Ali, Raian","How the different explanation classes impact trust calibration: The case of clinical decision support systems","International Journal of Human-Computer Studies","","1071-5819","10.1016/j.ijhcs.2022.102941","https://www.sciencedirect.com/science/article/pii/S1071581922001616","Machine learning has made rapid advances in safety-critical applications, such as traffic control, finance, and healthcare. With the criticality of decisions they support and the potential consequences of following their recommendations, it also became critical to provide users with explanations to interpret machine learning models in general, and black-box models in particular. However, despite the agreement on explainability as a necessity, there is little evidence on how recent advances in eXplainable Artificial Intelligence literature (XAI) can be applied in collaborative decision-making tasks, i.e., human decision-maker and an AI system working together, to contribute to the process of trust calibration effectively. This research conducts an empirical study to evaluate four XAI classes for their impact on trust calibration. We take clinical decision support systems as a case study and adopt a within-subject design followed by semi-structured interviews. We gave participants clinical scenarios and XAI interfaces as a basis for decision-making and rating tasks. Our study involved 41 medical practitioners who use clinical decision support systems frequently. We found that users perceive the contribution of explanations to trust calibration differently according to the XAI class and to whether XAI interface design fits their job constraints and scope. We revealed additional requirements on how explanations shall be instantiated and designed to help a better trust calibration. Finally, we build on our findings and present guidelines for designing XAI interfaces.","2023-01-01","2023-01-03 06:19:22","2023-05-23 01:26:25","2023-01-03 06:19:22","102941","","","169","","International Journal of Human-Computer Studies","How the different explanation classes impact trust calibration","","","","","","","en","","","","","ScienceDirect","","","","C:\Users\ambreen.hanif\Zotero\storage\Q4XEFB4Z\Naiseh et al_2023_How the different explanation classes impact trust calibration.pdf; ; C:\Users\ambreen.hanif\Zotero\storage\YTEU5937\S1071581922001616.html","notion://www.notion.so/Naiseh-et-al-2023-1ea16b02f86c4fac805ff16cb5b0d822","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NYQAZ8YG","conferencePaper","2021","De, Soumi; P, Prabu; Paulose, Joy","Effective ML Techniques to Predict Customer Churn","2021 Third International Conference on Inventive Research in Computing Applications (ICIRCA)","","","10.1109/ICIRCA51532.2021.9544785","","Customer churn is one of the most challenging problems that affects revenue and growth strategy of a company. According to a recent Gartner Tech Marketing survey, 91% of C-level respondents rate customer churn as one of their top concerns. However, only 43% have invested in additional resources to support customer expansion. Hence, retaining existing customers is of paramount importance to a company's growth. Many authors in the past have presented different versions of models to predict customer churn using machine learning techniques. The aim of this paper is to study some of the most important machine learning techniques used by researchers in the recent years. The paper also summarizes the prediction techniques, datasets used and performance achieved in these studies for a deeper understanding of the domain. The analysis shows that although hybrid and ensemble methods have been widely successful in improving model performance, there is a need for well-defined guidelines on appropriate model evaluation measures. While most approaches used are quantitative in nature, there is lack of research that focuses on information-rich content in customer company interaction instances, like emails, phone calls or customer support chat records. The information presented in the paper will not only help to increase awareness in industry about emerging trends in machine learning algorithms used in churn prediction, but also help new or existing researchers position their research activity appropriately.","2021-09","2022-12-27 06:17:15","2023-05-23 01:26:21","","895-902","","","","","","","","","","","","","","","","","","IEEE Xplore","","","","C:\Users\ambreen.hanif\Zotero\storage\9VZPTE8Q\De et al_2021_Effective ML Techniques to Predict Customer Churn.pdf; C:\Users\ambreen.hanif\Zotero\storage\VHL7VJWR\9544785.html; ","notion://www.notion.so/De-et-al-2021-9d354423b2384c5eaf94168fb6ce4cad","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2021 Third International Conference on Inventive Research in Computing Applications (ICIRCA)","","","","","","","","","","","","","","",""
"AYZ8FIJE","journalArticle","2022","Lalwani, Praveen; Mishra, Manas Kumar; Chadha, Jasroop Singh; Sethi, Pratyush","Customer churn prediction system: a machine learning approach","Computing","","1436-5057","10.1007/s00607-021-00908-y","https://doi.org/10.1007/s00607-021-00908-y","The customer churn prediction (CCP) is one of the challenging problems in the telecom industry. With the advancement in the field of machine learning and artificial intelligence, the possibilities to predict customer churn has increased significantly. Our proposed methodology, consists of six phases. In the first two phases, data pre-processing and feature analysis is performed. In the third phase, feature selection is taken into consideration using gravitational search algorithm. Next, the data has been split into two parts train and test set in the ratio of 80% and 20% respectively. In the prediction process, most popular predictive models have been applied, namely, logistic regression, naive bayes, support vector machine, random forest, decision trees, etc. on train set as well as boosting and ensemble techniques are applied to see the effect on accuracy of models. In addition, K-fold cross validation has been used over train set for hyperparameter tuning and to prevent overfitting of models. Finally, the obtained results on test set have been evaluated using confusion matrix and AUC curve. It was found that Adaboost and XGboost Classifier gives the highest accuracy of 81.71% and 80.8% respectively. The highest AUC score of 84%, is achieved by both Adaboost and XGBoost Classifiers which outperforms over others.","2022-02-01","2022-12-27 06:16:36","2023-05-23 01:26:20","2022-12-27 06:16:36","271-294","","2","104","","Computing","Customer churn prediction system","","","","","","","en","","","","","Springer Link","","","","C:\Users\ambreen.hanif\Zotero\storage\XK9995T3\Lalwani et al_2022_Customer churn prediction system.pdf; ","notion://www.notion.so/Lalwani-et-al-2022-3c1428316bed4a509959b0a30f399e23","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T7U6VHZ4","conferencePaper","2013","Mikolov, Tomas; Sutskever, Ilya; Chen, Kai; Corrado, Greg S; Dean, Jeff","Distributed Representations of Words and Phrases and their Compositionality","Advances in Neural Information Processing Systems","","","","https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html","The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships.  In this paper we present several improvements that make the Skip-gram model more expressive and enable it to learn higher quality vectors more rapidly.  We show that by subsampling frequent words we obtain significant speedup,  and also learn higher quality representations as measured by our tasks. We also introduce Negative Sampling, a simplified variant of Noise Contrastive Estimation (NCE) that learns more accurate vectors for frequent words compared to the hierarchical softmax.   An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases.  For example, the meanings of Canada'' and ""Air'' cannot be easily combined to obtain ""Air Canada''.  Motivated by this example, we present a simple and efficient method for finding phrases, and show that their vector representations can be accurately learned by the Skip-gram model. ""","2013","2022-12-27 05:09:56","2023-05-23 01:26:18","2022-12-27 05:09:56","","","","26","","","","","","","","Curran Associates, Inc.","","","","","","","Neural Information Processing Systems","","","","C:\Users\ambreen.hanif\Zotero\storage\Z68DDDDF\Mikolov et al_2013_Distributed Representations of Words and Phrases and their Compositionality.pdf; ","notion://www.notion.so/Mikolov-et-al-2013-24896d1577df4e2dbfc292d0608753a7","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7ALQNQEN","preprint","2013","Mikolov, Tomas; Chen, Kai; Corrado, Greg; Dean, Jeffrey","Efficient Estimation of Word Representations in Vector Space","","","","","http://arxiv.org/abs/1301.3781","We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.","2013-09-06","2022-12-27 05:09:52","2023-05-23 01:26:15","2022-12-27 05:09:52","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1301.3781 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\QS3XMZ84\1301.html; C:\Users\ambreen.hanif\Zotero\storage\ZQYA3DV6\Mikolov et al_2013_Efficient Estimation of Word Representations in Vector Space.pdf; ","notion://www.notion.so/Mikolov-et-al-2013-c5d1983c94b14231abb84970965e09d9","notion","","","","","","","","","","","","","","","","","","","","arXiv:1301.3781","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8MJXSHEF","preprint","2022","Bhaskhar, Nandita; Rubin, Daniel L.; Lee-Messer, Christopher","TRUST-LAPSE: An Explainable & Actionable Mistrust Scoring Framework for Model Monitoring","","","","","http://arxiv.org/abs/2207.11290","Continuous monitoring of trained ML models to determine when their predictions should and should not be trusted is essential for their safe deployment. Such a framework ought to be high-performing, explainable, post-hoc and actionable. We propose TRUST-LAPSE, a ""mistrust"" scoring framework for continuous model monitoring. We assess the trustworthiness of each input sample's model prediction using a sequence of latent-space embeddings. Specifically, (a) our latent-space mistrust score estimates mistrust using distance metrics (Mahalanobis distance) and similarity metrics (cosine similarity) in the latent-space and (b) our sequential mistrust score determines deviations in correlations over the sequence of past input representations in a non-parametric, sliding-window based algorithm for actionable continuous monitoring. We evaluate TRUST-LAPSE via two downstream tasks: (1) distributionally shifted input detection and (2) data drift detection, across diverse domains -- audio & vision using public datasets and further benchmark our approach on challenging, real-world electroencephalograms (EEG) datasets for seizure detection. Our latent-space mistrust scores achieve state-of-the-art results with AUROCs of 84.1 (vision), 73.9 (audio), 77.1 (clinical EEGs), outperforming baselines by over 10 points. We expose critical failures in popular baselines that remain insensitive to input semantic content, rendering them unfit for real-world model monitoring. We show that our sequential mistrust scores achieve high drift detection rates: over 90% of the streams show < 20% error for all domains. Through extensive qualitative and quantitative evaluations, we show that our mistrust scores are more robust and provide explainability for easy adoption into practice.","2022-07-22","2022-12-21 03:19:09","2023-05-23 01:26:13","2022-12-21 03:19:08","","","","","","","TRUST-LAPSE","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2207.11290 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\EEAXYKBF\2207.html; C:\Users\ambreen.hanif\Zotero\storage\H7RICYD8\Bhaskhar et al_2022_TRUST-LAPSE.pdf; ","notion://www.notion.so/Bhaskhar-et-al-2022-b21ce442107d4d0a999b19614b017453","notion","","","","","","","","","","","","","","","","","","","","arXiv:2207.11290","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9FEDNPUV","journalArticle","2022","Narteni, Sara; Orani, Vanessa; Cambiaso, Enrico; Rucco, Matteo; Mongelli, Maurizio","On the Intersection of Explainable and Reliable AI for Physical Fatigue Prediction","IEEE Access","","2169-3536","10.1109/ACCESS.2022.3191907","","In the era of Industry 4.0, the use of Artificial Intelligence (AI) is widespread in occupational settings. Since dealing with human safety, explainability and trustworthiness of AI are even more important than achieving high accuracy. eXplainable AI (XAI) is investigated in this paper to detect physical fatigue during manual material handling task simulation. Besides comparing global rule-based XAI models (LLM and DT) to black-box models (NN, SVM, XGBoost) in terms of performance, we also compare global models with local ones (LIME over XGBoost). Surprisingly, global and local approaches achieve similar conclusions, in terms of feature importance. Moreover, an expansion from local rules to global rules is designed for Anchors, by posing an appropriate optimization method (Anchors coverage is enlarged from an original low value, 11%, up to 43%). As far as trustworthiness is concerned, rule sensitivity analysis drives the identification of optimized regions in the feature space, where physical fatigue is predicted with zero statistical error. The discovery of such “non-fatigue regions” helps certifying the organizational and clinical decision making.","2022","2022-12-20 01:09:59","2023-05-23 01:26:10","","76243-76260","","","10","","","","","","","","","","","","","","","IEEE Xplore","","0 citations (Semantic Scholar/DOI) [2022-12-20] 0 citations (Crossref) [2022-12-20] Conference Name: IEEE Access","","C:\Users\ambreen.hanif\Zotero\storage\ZBZCF7LX\9831436.html; C:\Users\ambreen.hanif\Zotero\storage\4UNG4S6G\Narteni et al. - 2022 - On the Intersection of Explainable and Reliable AI.pdf; ","notion://www.notion.so/Narteni-et-al-2022-732375dd3e7d48d6b069675a504b525b","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9CAMRFWE","journalArticle","1974","Friedman, Michael","Explanation and Scientific Understanding","The Journal of Philosophy","","0022-362X","10.2307/2024924","https://www.jstor.org/stable/2024924","","1974","2022-12-19 04:56:11","2023-05-23 01:26:09","2022-12-19 04:56:11","5-19","","1","71","","","","","","","","","","","","","","","JSTOR","","869 citations (Semantic Scholar/DOI) [2022-12-20] 491 citations (Crossref) [2022-12-20] Publisher: Journal of Philosophy, Inc.","","C:\Users\ambreen.hanif\Zotero\storage\JKUCUF5Q\Friedman_1974_Explanation and Scientific Understanding.pdf; ","notion://www.notion.so/Friedman-1974-f0d626aaa72443e78141b2a177056491","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9NDY3PCZ","journalArticle","2019","Sridharan, Mohan; Meadows, Ben","Towards a Theory of Explanations for Human–Robot Collaboration","KI - Künstliche Intelligenz","","1610-1987","10.1007/s13218-019-00616-y","https://doi.org/10.1007/s13218-019-00616-y","This paper makes two contributions towards enabling a robot to provide explanatory descriptions of its decisions, the underlying knowledge and beliefs, and the experiences that informed these beliefs. First, we present a theory of explanations comprising (i) claims about representing, reasoning with, and learning domain knowledge to support the construction of explanations; (ii) three fundamental axes to characterize explanations; and (iii) a methodology for constructing these explanations. Second, we describe an architecture for robots that implements this theory and supports scalability to complex domains and explanations. We demonstrate the architecture’s capabilities in the context of a simulated robot (a) moving target objects to desired locations or people; or (b) following recipes to bake biscuits.","2019-12-01","2022-12-16 01:51:35","2023-05-23 01:26:08","2022-12-16 01:51:35","331-342","","4","33","","Künstl Intell","","","","","","","","en","","","","","Springer Link","","28 citations (Semantic Scholar/DOI) [2022-12-20] 16 citations (Crossref) [2022-12-20]","","; C:\Users\ambreen.hanif\Zotero\storage\FDZ4P6QX\Sridharan_Meadows_2019_Towards a Theory of Explanations for Human–Robot Collaboration.pdf","notion://www.notion.so/Sridharan-Meadows-2019-7428e37d0e244345829f30362f50fa1d","notion; XAI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YFKVK6PB","book","","Casas, Pablo","Data Science Live Book","","","","","https://livebook.datascienceheroes.com/","An intuitive and practical approach to data analysis, data preparation and machine learning, suitable for all ages!","","2022-12-13 04:20:21","2023-05-23 01:26:03","2022-12-13 04:20:21","","","","","","","","","","","","","","","","","","","livebook.datascienceheroes.com","","","","; C:\Users\ambreen.hanif\Zotero\storage\8X5GHWZL\index.html","notion://www.notion.so/Casas-n-d-0046ce98d60641b0b5ecaf6af4918c6a","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RH37HUJY","webpage","2019","","Feature Selection using Genetic Algorithms in R","Data Science Heroes Blog","","","","https://blog.datascienceheroes.com/feature-selection-using-genetic-algorithms-in-r/","From a gentle introduction to a practical solution, this is a post about feature selection using genetic algorithms in R.","2019-01-15","2022-12-13 03:19:11","2023-05-23 01:26:01","2022-12-13 03:19:11","","","","","","","","","","","","","","en","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\FWQYNLKR\feature-selection-using-genetic-algorithms-in-r.html","notion://www.notion.so/Feature-Selection-Using-Genetic-Algorithms-in-R-2019-ae831665ca41438a8fc9c55fe2a95089","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LTCHXCCC","journalArticle","","Albom, Mitch","LIFE'S GREATEST LESSON","","","","","","","","2022-12-13 01:02:38","2023-05-23 01:25:59","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\MYF7YL49\Albom - LIFE'S GREATEST LESSON.pdf; ","notion://www.notion.so/Albom-n-d-ed9e9566c03c49cd91e2ac2993e737db","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B4SZHRTT","book","","Burzykowski, Przemyslaw Biecek and Tomasz","1 Introduction | Explanatory Model Analysis","","","","","https://ema.drwhy.ai/introduction.html","This book introduces unified language for exploration, explanation and examination of predictive machine learning models.","","2022-12-09 02:08:44","2023-05-23 01:25:56","2022-12-09 02:08:44","","","","","","","","","","","","","","","","","","","ema.drwhy.ai","","","","; C:\Users\ambreen.hanif\Zotero\storage\AUZBK92W\introduction.html","notion://www.notion.so/Burzykowski-n-d-bdb23c35074e4ab08970fd0eb142a4db","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"39N4SJM4","conferencePaper","2020","Suhel, Sasha Fathima; Shukla, Vinod Kumar; Vyas, Sonali; Mishra, Ved Prakash","Conversation to Automation in Banking Through Chatbot Using Artificial Machine Intelligence Language","2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)","","","10.1109/ICRITO48877.2020.9197825","","Artificial Machine Intelligence is a very complicated topic. It involves creating machines that are capable of simulating knowledge. This paper examines some of the latest AI patterns and activities and then provides alternative theory of change in some of the popular and widely accepted postulates of today. Based on basic A.I. (Artificial Intelligence) structuring and working for this, System-Chatbots are made (or chatter bots). The paper shows that A.I is ever improving. As of now there isn't enough information on A.I. however this paper provides a new concept which addresses machine intelligence and sheds light on the potential of intelligent systems. The rise of chatbots in the finance sector is the latest disruptive force that has changed the way customers interact. In the banking industry, the introduction of Artificial Intelligence has driven chatbots and changed the face of the interaction between bank and customers. The banking sector plays an important role in development into any country. It also explores the existing usability of chatbot to assess whether it can fulfill customers ever-changing needs.","2020-06","2022-12-02 02:12:01","2023-05-23 01:25:50","","611-618","","","","","","","","","","","","","","","","","","IEEE Xplore","","20 citations (Semantic Scholar/DOI) [2022-12-20] 18 citations (Crossref) [2022-12-20]","","C:\Users\ambreen.hanif\Zotero\storage\MKSRWVG3\9197825.html; ; C:\Users\ambreen.hanif\Zotero\storage\BYI74PAQ\Suhel et al_2020_Conversation to Automation in Banking Through Chatbot Using Artificial Machine.pdf","notion://www.notion.so/Suhel-et-al-2020-a9fa057afeaa4b16a21dd0c1904125b3","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)","","","","","","","","","","","","","","",""
"3MD2GJ8P","webpage","","","Conversation to Automation in Banking Through Chatbot Using Artificial Machine Intelligence Language | IEEE Conference Publication | IEEE Xplore","","","","","https://ieeexplore-ieee-org.simsrad.net.ocs.mq.edu.au/abstract/document/9197825","","","2022-12-02 02:10:48","2023-05-23 01:25:45","2022-12-02 02:10:48","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\RJ8IC3EH\9197825.html; ","notion://www.notion.so/Conversation-to-Automation-in-Banking-Through-Chatbot-Using-Artificial-Machine-Intelligence-Language-dda772d4b23f4a59933d2881c7ef97c6","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZUBPCNXL","journalArticle","2021","Eren, Berrin Arzu","Determinants of customer satisfaction in chatbot use: evidence from a banking application in Turkey","International Journal of Bank Marketing","","0265-2323","10.1108/IJBM-02-2020-0056","https://doi.org/10.1108/IJBM-02-2020-0056","Purpose This study aims to investigates customer satisfaction from the use of bank chatbots and the effect of perceived trust in chatbots and banks' reputation on customer satisfaction. Design/methodology/approach A survey is conducted in Turkey involving 240 customers who experienced banking transactions using a chatbot. Partial least squares structural equation modeling (PLS-SEM) is used to investigate the relationships between the variables. The data were analyzed using SPSS 21 and SmartPLS programs. Findings Perceived performance, perceived trust and corporate reputation significantly affect customer satisfaction with chatbot use. Customer expectations and confirmation of customer expectations have no direct impact on customer satisfaction, but customer expectations positively affect perceived performance. Customer expectations exert an indirect influence on customer satisfaction through perceived performance. Perceived performance has a positive impact on the confirmation of customer expectations, but customer expectations do not significantly impact the confirmation of customer expectations. Research limitations/implications This study relies on a limited number of participants. Moreover, its sample is not representative of the target population due to the convenience sampling technique. Even if the results may not be generalized to the entire population of Turkey, they reflect the reality of emerging markets with relatively high technology sensitivity and a young population. Practical implications The results provide new insights regarding banking service delivery channels, which may be of interest to professionals, academics, banks' top management, product development teams, design teams and customer satisfaction units. Social implications This study is believed to help the community make their lives easier by providing them with knowledge and awareness about chatbots. Originality/value This study extends expectations confirmation theory's predictions to chatbot use in banking.","2021-01-01","2022-12-02 02:03:11","2023-05-23 01:25:42","2022-12-02 02:03:11","294-311","","2","39","","","Determinants of customer satisfaction in chatbot use","","","","","","","","","","","","Emerald Insight","","34 citations (Semantic Scholar/DOI) [2022-12-20] 37 citations (Crossref) [2022-12-20] Publisher: Emerald Publishing Limited","","; C:\Users\ambreen.hanif\Zotero\storage\T5MIM7XU\html.html","notion://www.notion.so/Eren-2021-9862c6e769714807bfe5a75af402841d","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VRBIG458","book","","Burzykowski, Przemyslaw Biecek and Tomasz","Explanatory Model Analysis","","","","","https://ema.drwhy.ai/preface.html","This book introduces unified language for exploration, explanation and examination of predictive machine learning models.","","2022-11-30 00:18:36","2023-05-23 01:25:41","2022-11-30 00:18:36","","","","","","","","","","","","","","","","","","","ema.drwhy.ai","","","","; C:\Users\ambreen.hanif\Zotero\storage\SMLYYN3A\preface.html","notion://www.notion.so/Burzykowski-n-d-46a2051007294e32ba6ab9aa2f4ffc91","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K3D7J2WE","book","2023","","Proceedings of the Future Technologies Conference (FTC) 2022, Volume 1","","978-3-031-18460-4 978-3-031-18461-1","","","https://link.springer.com/10.1007/978-3-031-18461-1","","2023","2022-11-28 03:45:25","2023-05-23 01:25:39","2022-11-28 03:45:25","","","","559","","","","Lecture Notes in Networks and Systems","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","DOI: 10.1007/978-3-031-18461-1","","C:\Users\ambreen.hanif\Zotero\storage\BGBRJUDK\Arai - 2023 - Proceedings of the Future Technologies Conference .pdf; ","notion://www.notion.so/Arai-2023-7f99976a43ba40ce86621a07f81748f2","notion; first_pass","","Arai, Kohei","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9PLMC7K6","journalArticle","2022","Islam, Mir Riyanul; Ahmed, Mobyen Uddin; Barua, Shaibal; Begum, Shahina","A Systematic Review of Explainable Artificial Intelligence in Terms of Different Application Domains and Tasks","Applied Sciences","","2076-3417","10.3390/app12031353","https://www.mdpi.com/2076-3417/12/3/1353","Artificial intelligence (AI) and machine learning (ML) have recently been radically improved and are now being employed in almost every application domain to develop automated or semi-automated systems. To facilitate greater human acceptability of these systems, explainable artificial intelligence (XAI) has experienced significant growth over the last couple of years with the development of highly accurate models but with a paucity of explainability and interpretability. The literature shows evidence from numerous studies on the philosophy and methodologies of XAI. Nonetheless, there is an evident scarcity of secondary studies in connection with the application domains and tasks, let alone review studies following prescribed guidelines, that can enable researchers’ understanding of the current trends in XAI, which could lead to future research for domain- and application-specific method development. Therefore, this paper presents a systematic literature review (SLR) on the recent developments of XAI methods and evaluation metrics concerning different application domains and tasks. This study considers 137 articles published in recent years and identified through the prominent bibliographic databases. This systematic synthesis of research articles resulted in several analytical findings: XAI methods are mostly developed for safety-critical domains worldwide, deep learning and ensemble models are being exploited more than other types of AI/ML models, visual explanations are more acceptable to end-users and robust evaluation metrics are being developed to assess the quality of explanations. Research studies have been performed on the addition of explanations to widely used AI/ML models for expert users. However, more attention is required to generate explanations for general users from sensitive domains such as finance and the judicial system.","2022-01","2022-11-28 03:42:45","2023-05-23 01:25:37","2022-11-28 03:42:45","1353","","3","12","","","","","","","","","","en","http://creativecommons.org/licenses/by/3.0/","","","","www.mdpi.com","","22 citations (Semantic Scholar/DOI) [2022-12-20] 23 citations (Crossref) [2022-12-20] Number: 3 Publisher: Multidisciplinary Digital Publishing Institute","","C:\Users\ambreen.hanif\Zotero\storage\Z3KV2475\Islam et al_2022_A Systematic Review of Explainable Artificial Intelligence in Terms of.pdf; ; C:\Users\ambreen.hanif\Zotero\storage\LRIZ85D5\1353.html","notion://www.notion.so/Islam-et-al-2022-9232ddf66ad04189926a6645d9ea2213","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6A6AMCQ8","book","2020","Vilone, Giulia; Longo, Luca","Explainable Artificial Intelligence: a Systematic Review","","","","","","Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions.","2020-05-29","2022-11-28 03:22:12","2023-05-23 01:25:36","","","","","","","","Explainable Artificial Intelligence","","","","","","","","","","","","ResearchGate","","DOI: 10.48550/arXiv.2006.00093","","; ; C:\Users\ambreen.hanif\Zotero\storage\HL2Y3KFM\Vilone_Longo_2020_Explainable Artificial Intelligence.pdf","notion://www.notion.so/Vilone-Longo-2020-be2b74611de64b58ba681722b972d7cb; https://www.researchgate.net/publication/341817113_Explainable_Artificial_Intelligence_a_Systematic_Review","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CKCZ38PS","journalArticle","2020","Payrovnaziri, Seyedeh Neelufar; Chen, Zhaoyi; Rengifo-Moreno, Pablo; Miller, Tim; Bian, Jiang; Chen, Jonathan H.; Liu, Xiuwen; He, Zhe","Explainable artificial intelligence models using real-world electronic health record data: a systematic scoping review","Journal of the American Medical Informatics Association: JAMIA","","1527-974X","10.1093/jamia/ocaa053","","OBJECTIVE: To conduct a systematic scoping review of explainable artificial intelligence (XAI) models that use real-world electronic health record data, categorize these techniques according to different biomedical applications, identify gaps of current studies, and suggest future research directions. MATERIALS AND METHODS: We searched MEDLINE, IEEE Xplore, and the Association for Computing Machinery (ACM) Digital Library to identify relevant papers published between January 1, 2009 and May 1, 2019. We summarized these studies based on the year of publication, prediction tasks, machine learning algorithm, dataset(s) used to build the models, the scope, category, and evaluation of the XAI methods. We further assessed the reproducibility of the studies in terms of the availability of data and code and discussed open issues and challenges. RESULTS: Forty-two articles were included in this review. We reported the research trend and most-studied diseases. We grouped XAI methods into 5 categories: knowledge distillation and rule extraction (N = 13), intrinsically interpretable models (N = 9), data dimensionality reduction (N = 8), attention mechanism (N = 7), and feature interaction and importance (N = 5). DISCUSSION: XAI evaluation is an open issue that requires a deeper focus in the case of medical applications. We also discuss the importance of reproducibility of research work in this field, as well as the challenges and opportunities of XAI from 2 medical professionals' point of view. CONCLUSION: Based on our review, we found that XAI evaluation in medicine has not been adequately and formally practiced. Reproducibility remains a critical concern. Ample opportunities exist to advance XAI research in medicine.","2020-07-01","2022-11-28 03:18:42","2023-05-23 01:25:34","","1173-1185","","7","27","","J Am Med Inform Assoc","Explainable artificial intelligence models using real-world electronic health record data","","","","","","","eng","","","","","PubMed","","93 citations (Semantic Scholar/DOI) [2022-12-20] 88 citations (Crossref) [2022-12-20] PMID: 32417928 PMCID: PMC7647281 QID: Q94954757","","; ","notion://www.notion.so/Payrovnaziri-et-al-2020-6a2ecda9adc24a7d964432305412e1bd; http://www.ncbi.nlm.nih.gov/pubmed/32417928","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AXWQWA6R","book","2021","Williams, Oyin","Towards Human-Centred Explainable AI: A Systematic Literature Review","","","","","","The increasing complexity of AI systems has led to the black-box problem; this and user's need for transparency has spurred the field of Explainable AI (XAI). However, XAI research has been heavily inclined towards the technical, machine-centred approach, with inadequate emphasis on the human users of AI systems and how to make machine learning decisions more understandable for them. A systematic literature review was carried out to investigate the current state of the art of this ""whom"" approach to XAI, which this paper refers to as Human-centred Explainable AI (HCXAI). Using the PRISMA guideline, 32 papers were selected and reviewed to (i)understand the goals for HCXAI, (ii)identify the definition of a ""good"" explanation for users of AI systems, (iii)reveal the approaches and frameworks adapted for Human-centred XAI. The key findings of the analysis include: (i)The main goals of XAI is to foster the trust of users in AI systems; (ii) context-awareness and personalization are the most common approaches to HCXAI adopted in the studied papers; (iii) there are only few existing theoretical models/frameworks on HCXAI and its practical implementation strategies.","2021-05-24","2022-11-28 03:18:24","2023-05-23 01:25:32","","","","","","","","Towards Human-Centred Explainable AI","","","","","","","","","","","","ResearchGate","","DOI: 10.13140/RG.2.2.27885.92645","","; ; C:\Users\ambreen.hanif\Zotero\storage\U3NELZYL\Williams_2021_Towards Human-Centred Explainable AI.pdf","notion://www.notion.so/Williams-2021-fe46ef30c2044c74bd99d56eae96c32b; https://www.researchgate.net/publication/353847748_Towards_Human-Centred_Explainable_AI_A_Systematic_Literature_Review","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XY3875NP","journalArticle","2022","Kou, Ziyi; Zhang, Yang; Zhang, Daniel; Wang, Dong","CrowdGraph: A Crowdsourcing Multi-modal Knowledge Graph Approach to Explainable Fauxtography Detection","Proceedings of the ACM on Human-Computer Interaction","","","10.1145/3555178","http://doi.org/10.1145/3555178","Human-centric fauxtography is a category of multi-modal posts that spread misleading information on online information distribution and sharing platforms such as online social media. The reason of a human-centric post being fauxtography is closely related to its multi-modal content that consists of diversified human and non-human subjects with complex and implicit relationships. In this paper, we focus on an explainable fauxtography detection problem where the goal is to accurately identify and explain why a human-centric social media post is fauxtography (or not). Our problem is motivated by the limitations of current fauxtography detection solutions that focus primarily on the detection task but ignore the important aspect of explaining their results (e.g., why a certain component of the post delivers the misinformation). Two important challenges exist in solving our problem: 1) it is difficult to capture the implicit relations and attributions of different subjects in a fauxtography post given the fact that many of such knowledge is shared between different crowd workers; 2) it is not a trivial task to create a multi-modal knowledge graph from crowd workers to identify and explain human-centric fauxtography posts with multi-modal contents. To address the above challenges, we develop CrowdGraph, a crowdsourcing based multi-modal knowledge graph approach to address the explainable fauxtography detection problem. We evaluate the performance of CrowdGraph by creating a real-world dataset that consists of human-centric fauxtography posts from Twitter and Reddit. The results show that CrowdGraph not only detects the fauxtography posts more accurately than the state-of-the-arts but also provides well-justified explanations to the detection results with convincing evidence.","2022-11-11","2022-11-21 23:24:20","2023-05-23 01:25:31","2022-11-21 23:24:19","287:1–287:28","","CSCW2","6","","Proc. ACM Hum.-Comput. Interact.","CrowdGraph","","","","","","","","","","","","November 2022","","0 citations (Semantic Scholar/DOI) [2022-12-20] 0 citations (Crossref) [2022-12-20]","","C:\Users\ambreen.hanif\Zotero\storage\U9VU59VK\Kou et al_2022_CrowdGraph.pdf; ","notion://www.notion.so/Kou-et-al-2022-3e4dda5c8adb4db69f58ddff75c6c837","notion; Explainable; knowledge graphs; crowdgraph; fauxtography","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SKAXNK8Y","bookSection","2022","Gerlings, Julie; Jensen, Millie Søndergaard; Shollo, Arisa","Explainable AI, but explainable to whom?","","","","","http://arxiv.org/abs/2106.05568","Advances in AI technologies have resulted in superior levels of AI-based model performance. However, this has also led to a greater degree of model complexity, resulting in 'black box' models. In response to the AI black box problem, the field of explainable AI (xAI) has emerged with the aim of providing explanations catered to human understanding, trust, and transparency. Yet, we still have a limited understanding of how xAI addresses the need for explainable AI in the context of healthcare. Our research explores the differing explanation needs amongst stakeholders during the development of an AI-system for classifying COVID-19 patients for the ICU. We demonstrate that there is a constellation of stakeholders who have different explanation needs, not just the 'user'. Further, the findings demonstrate how the need for xAI emerges through concerns associated with specific stakeholder groups i.e., the development team, subject matter experts, decision makers, and the audience. Our findings contribute to the expansion of xAI by highlighting that different stakeholders have different explanation needs. From a practical perspective, the study provides insights on how AI systems can be adjusted to support different stakeholders needs, ensuring better implementation and operation in a healthcare context.","2022","2022-11-20 23:46:51","2023-05-23 01:25:30","2022-11-20 23:46:51","169-198","","","212","","","","","","","","","","","","","","","arXiv.org","","3 citations (Semantic Scholar/arXiv) [2022-12-20] DOI: 10.1007/978-3-030-83620-7_7 arXiv:2106.05568 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\NQU5T52M\2106.html; C:\Users\ambreen.hanif\Zotero\storage\TDA3FIR6\Gerlings et al_2022_Explainable AI, but explainable to whom.pdf; ","notion://www.notion.so/Gerlings-et-al-2022-b034145b5a7443ebbd37348fceec9c10","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JJCPXY23","journalArticle","2020","Heinrichs, Bert; Eickhoff, Simon B.","Your evidence? Machine learning algorithms for medical diagnosis and prediction","Human Brain Mapping","","1097-0193","10.1002/hbm.24886","https://onlinelibrary.wiley.com/doi/abs/10.1002/hbm.24886","Computer systems for medical diagnosis based on machine learning are not mere science fiction. Despite undisputed potential benefits, such systems may also raise problems. Two (interconnected) issues are particularly significant from an ethical point of view: The first issue is that epistemic opacity is at odds with a common desire for understanding and potentially undermines information rights. The second (related) issue concerns the assignment of responsibility in cases of failure. The core of the two issues seems to be that understanding and responsibility are concepts that are intrinsically tied to the discursive practice of giving and asking for reasons. The challenge is to find ways to make the outcomes of machine learning algorithms compatible with our discursive practice. This comes down to the claim that we should try to integrate discursive elements into machine learning algorithms. Under the title of “explainable AI” initiatives heading in this direction are already under way. Extensive research in this field is needed for finding adequate solutions.","2020","2022-11-18 00:49:32","2023-05-23 01:25:26","2022-11-18 00:49:32","1435-1444","","6","41","","","Your evidence?","","","","","","","en","","","","","Wiley Online Library","","45 citations (Semantic Scholar/DOI) [2022-12-20] 37 citations (Crossref) [2022-12-20] _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hbm.24886 QID: Q91714209","","C:\Users\ambreen.hanif\Zotero\storage\2T279DYH\Heinrichs_Eickhoff_2020_Your evidence.pdf; ; C:\Users\ambreen.hanif\Zotero\storage\JLAL3S3E\hbm.html","notion://www.notion.so/Heinrichs-Eickhoff-2020-8124dd30a04f42448aca0e2d287215b2","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KAY73N6Z","journalArticle","2019","Sousa, Weslei Gomes de; Melo, Elis Regina Pereira de; Bermejo, Paulo Henrique De Souza; Farias, Rafael Araújo Sousa; Gomes, Adalmir Oliveira","How and where is artificial intelligence in the public sector going? A literature review and research agenda","Government Information Quarterly","","0740-624X","10.1016/j.giq.2019.07.004","https://www.sciencedirect.com/science/article/pii/S0740624X18303113","To obtain benefits in the provision of public services, managers of public organizations have considerably increased the adoption of artificial intelligence (AI) systems. However, research on AI is still scarce, and the advance of this technology in the public sector, as well as the applications and results of this strategy, need to be systematized. With this goal in mind, this paper examines research related to AI as applied to the public sector. A review of the literature covering articles available in five research databases was completed using the PRISMA protocol for literature reviews. The search process yielded 59 articles within the scope of the study out of a total of 1682 studies. Results show a growing trend of interest in AI in the public sector, with India and the US as the most active countries. General public service, economic affairs, and environmental protection are the functions of government with the most studies related to AI. The Artificial Neural Networks (ANN) technique is the most recurrent in the investigated studies and was pointed out as a technique that provides positive results in several areas of its application. A research framework for AI solutions for the public sector is presented, where it is demonstrated that policies and ethical implications of the use of AI permeate all layers of application of this technology and the solutions can generate value for functions of government. However, for this, a prior debate with society about the use of AI in the public sector is recommended.","2019-10-01","2022-11-18 00:29:30","2023-05-23 01:25:21","2022-11-18 00:29:30","101392","","4","36","","Government Information Quarterly","How and where is artificial intelligence in the public sector going?","","","","","","","en","","","","","ScienceDirect","","128 citations (Semantic Scholar/DOI) [2022-12-20] 114 citations (Crossref) [2022-12-20]","","; C:\Users\ambreen.hanif\Zotero\storage\XXVJ5NQ2\S0740624X18303113.html; C:\Users\ambreen.hanif\Zotero\storage\3GYGU6V5\Sousa et al_2019_How and where is artificial intelligence in the public sector going.pdf","notion://www.notion.so/Sousa-et-al-2019-6d4bc53793694f03b3bcb54cba211c33","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RZRSBH4S","journalArticle","2022","de Bruijn, Hans; Warnier, Martijn; Janssen, Marijn","The perils and pitfalls of explainable AI: Strategies for explaining algorithmic decision-making","Government Information Quarterly","","0740-624X","10.1016/j.giq.2021.101666","https://www.sciencedirect.com/science/article/pii/S0740624X21001027","Governments look at explainable artificial intelligence's (XAI) potential to tackle the criticisms of the opaqueness of algorithmic decision-making with AI. Although XAI is appealing as a solution for automated decisions, the wicked nature of the challenges governments face complicates the use of XAI. Wickedness means that the facts that define a problem are ambiguous and that there is no consensus on the normative criteria for solving this problem. In such a situation, the use of algorithms can result in distrust. Whereas there is much research advancing XAI technology, the focus of this paper is on strategies for explainability. Three illustrative cases are used to show that explainable, data-driven decisions are often not perceived as objective by the public. The context might raise strong incentives to contest and distrust the explanation of AI, and as a consequence, fierce resistance from society is encountered. To overcome the inherent problems of XAI, decisions-specific strategies are proposed to lead to societal acceptance of AI-based decisions. We suggest strategies to embrace explainable decisions and processes, co-create decisions with societal actors, move away from an instrumental to an institutional approach, use competing and value-sensitive algorithms, and mobilize the tacit knowledge of professionals","2022-04-01","2022-11-18 00:28:33","2023-05-23 01:25:20","2022-11-18 00:28:33","101666","","2","39","","Government Information Quarterly","The perils and pitfalls of explainable AI","","","","","","","en","","","","","ScienceDirect","","12 citations (Semantic Scholar/DOI) [2022-12-20] 6 citations (Crossref) [2022-12-20]","","; C:\Users\ambreen.hanif\Zotero\storage\2C825ZGU\de Bruijn et al. - 2022 - The perils and pitfalls of explainable AI Strateg.pdf; C:\Users\ambreen.hanif\Zotero\storage\DKY49J8J\S0740624X21001027.html","notion://www.notion.so/de-Bruijn-et-al-2022-8ae7ad8d0b3f46d2bf6a795cead11dee","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQHWRJGH","book","","","1 What is machine learning? It is common sense, except done by a computer","","978-1-61729-591-1","","","https://learning.oreilly.com/library/view/grokking-machine-learning/9781617295911/Text/01.xhtml","1 What is machine learning? It is common sense, except done by a computer      In this chapter            what is machine learning        is machine learning hard (spoiler: no)        what do we...","","2022-11-14 05:33:10","2023-05-23 01:25:18","2022-11-14 05:33:10","","","","","","","1 What is machine learning?","","","","","","","en","","","","","learning.oreilly.com","","","","; C:\Users\ambreen.hanif\Zotero\storage\HYVLV8QR\01.html","notion://www.notion.so/1-What-Is-Machine-Learning-n-d-2cc9749681984ded8b524e96c0172b9a","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S2SXWKHT","preprint","2017","Shrikumar, Avanti; Greenside, Peyton; Shcherbina, Anna; Kundaje, Anshul","Not Just a Black Box: Learning Important Features Through Propagating Activation Differences","","","","10.48550/arXiv.1605.01713","http://arxiv.org/abs/1605.01713","Note: This paper describes an older version of DeepLIFT. See https://arxiv.org/abs/1704.02685 for the newer version. Original abstract follows: The purported ""black box"" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Learning Important FeaTures), an efficient and effective method for computing importance scores in a neural network. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. We apply DeepLIFT to models trained on natural images and genomic data, and show significant advantages over gradient-based methods.","2017-04-11","2022-11-14 02:29:12","2023-05-23 01:25:17","2022-11-14 02:29:11","","","","","","","Not Just a Black Box","","","","","arXiv","","","","","","","arXiv.org","","482 citations (Semantic Scholar/arXiv) [2022-12-20] arXiv:1605.01713 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\2JQLLEMD\1605.html; ; C:\Users\ambreen.hanif\Zotero\storage\3DDD2Q8X\Shrikumar et al_2017_Not Just a Black Box.pdf","notion://www.notion.so/Shrikumar-et-al-2017-a3e7a00985294ea5a0c9c8a42b04bdc6","notion","","","","","","","","","","","","","","","","","","","","arXiv:1605.01713","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P5D34YIR","preprint","2021","Wang, Jiaxuan; Wiens, Jenna; Lundberg, Scott","Shapley Flow: A Graph-based Approach to Interpreting Model Predictions","","","","","http://arxiv.org/abs/2010.14592","Many existing approaches for estimating feature importance are problematic because they ignore or hide dependencies among features. A causal graph, which encodes the relationships among input variables, can aid in assigning feature importance. However, current approaches that assign credit to nodes in the causal graph fail to explain the entire graph. In light of these limitations, we propose Shapley Flow, a novel approach to interpreting machine learning models. It considers the entire causal graph, and assigns credit to \textit{edges} instead of treating nodes as the fundamental unit of credit assignment. Shapley Flow is the unique solution to a generalization of the Shapley value axioms to directed acyclic graphs. We demonstrate the benefit of using Shapley Flow to reason about the impact of a model's input on its output. In addition to maintaining insights from existing approaches, Shapley Flow extends the flat, set-based, view prevalent in game theory based explanation methods to a deeper, \textit{graph-based}, view. This graph-based view enables users to understand the flow of importance through a system, and reason about potential interventions.","2021-02-26","2022-11-14 01:07:22","2023-05-23 01:25:13","2022-11-14 01:07:22","","","","","","","Shapley Flow","","","","","arXiv","","","","","","","arXiv.org","","40 citations (Semantic Scholar/arXiv) [2022-12-20] arXiv:2010.14592 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\WY9XLCCH\2010.html; ; C:\Users\ambreen.hanif\Zotero\storage\4CSSFUMT\Wang et al_2021_Shapley Flow.pdf","notion://www.notion.so/Wang-et-al-2021-efe2907d130e4092b460ade73ce6d728","notion","","","","","","","","","","","","","","","","","","","","arXiv:2010.14592","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YV5AD97H","computerProgram","2021","Hussein, Omar Magdy","Hate-Speech-Detector-Demo","","","","","https://github.com/omagdy/Hate-Speech-Detector-Demo","A Demo that demostrates a NN model that was trained for detecting Hate Speech.","2021-04-20","2022-09-30 00:36:14","2023-05-23 01:25:12","2022-09-30 00:36:14","","","","","","","","","","","","","","","","","","","GitHub","","original-date: 2019-10-13T16:41:44Z","","","notion://www.notion.so/Hussein-2019-2021-fe9de469acd344b9ab6756b80714891e","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Python","","","","","","","","",""
"2JQH2K33","journalArticle","","Saleh, Hind; Alhothali, Areej; Moria, Kawthar","Detection of Hate Speech using BERT and Hate Speech Word Embedding with Deep Model","","","","","","The enormous amount of data being generated on the web and social media has increased the demand for detecting online hate speech. Detecting hate speech will reduce their neg-ative impact and influence on others. A lot of effort in the Natural Language Processing (NLP) domain aimed to detect hate speech in general or detect specific hate speech such as religion, race, gender, or sexual orientation. Hate communities tend to use abbreviations, intentional spelling mistakes, and coded words in their communication to evade detection, adding more challenges to hate speech detec-tion tasks. Thus, word representation will play an increasingly pivotal role in detecting hate speech. This paper investigates the feasibil-ity of leveraging domain-specific word embed-ding in Bidirectional LSTM based deep model to automatically detect/classify hate speech. Furthermore, we investigate the use of the transfer learning language model (BERT) on hate speech problem as a binary classification task. The experiments showed that domain-specific word embedding with the Bidirec-tional LSTM based deep model achieved a 93% f1-score while BERT achieved up to 96% f1-score on a combined balanced dataset from available hate speech datasets.","","2022-09-30 00:35:43","2023-05-23 01:25:10","","12","","","","","","","","","","","","","en","","","","","Zotero","","","","; C:\Users\ambreen.hanif\Zotero\storage\HZPJHS6H\Saleh et al. - Detection of Hate Speech using BERT and Hate Speec.pdf","notion://www.notion.so/Saleh-et-al-n-d-b8bf26391de54cc38277745b446fa7e7","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BLRZELMJ","journalArticle","2022","Jiang, Xuchu; Zhang, Ying; Li, Ying; Zhang, Biao","Forecast and analysis of aircraft passenger satisfaction based on RF-RFE-LR model","Scientific Reports","","2045-2322","10.1038/s41598-022-14566-3","https://www.nature.com/articles/s41598-022-14566-3","Airplanes have always been one of the first choices for people to travel because of their convenience and safety. However, due to the outbreak of the new coronavirus epidemic in 2020, the civil aviation industry of various countries in the world has encountered severe challenges. Predicting aircraft passenger satisfaction and excavating the main influencing factors can help airlines improve their services and gain advantages in difficult situations and competition. This paper proposes a RF-RFE-Logistic feature selection model to extract the influencing factors of passenger satisfaction. First, preliminary feature selection is performed using recursive feature elimination based on random forest (RF-RFE). Second, based on different classification models, KNN, logistic regression, random forest, Gaussian Naive Bayes, and BP neural network, the classification performance of the models before and after feature selection is compared, and the prediction model with the best classification performance is selected. Finally, based on the RF-RFE feature selection, combined with the logistic model, the factors affecting customer satisfaction are further extracted. The experimental results show that the RF-RFE model selects a feature subset containing 17 variables. In the classification prediction model, the random forest after RF-RFE feature selection shows the best classification performance. Finally, combined with the four important variables extracted by RF-RFE and logistic regression, further discussion is carried out, and suggestions are given for airlines to improve passenger satisfaction.","2022-07-01","2022-09-30 00:32:26","2023-05-23 01:25:08","2022-09-30 00:32:26","11174","","1","12","","Sci Rep","","","","","","","","en","2022 The Author(s)","","","","www.nature.com","","0 citations (Semantic Scholar/DOI) [2022-12-20] 0 citations (Crossref) [2022-12-20] Number: 1 Publisher: Nature Publishing Group QID: Q115741754","","C:\Users\ambreen.hanif\Zotero\storage\69XU5XAB\Jiang et al_2022_Forecast and analysis of aircraft passenger satisfaction based on RF-RFE-LR.pdf; ","notion://www.notion.so/Jiang-et-al-2022-d5bd511905374b7394cc01f974b5cdc5","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G6A86VDM","computerProgram","2022","","Neural Machine Translation (seq2seq) Tutorial","","","","","https://github.com/tensorflow/nmt","TensorFlow Neural Machine Translation Tutorial","2022-09-26","2022-09-30 00:28:08","2023-05-23 01:25:01","2022-09-30 00:28:08","","","","","","","","","","","","tensorflow","","","Apache-2.0","","","","GitHub","","original-date: 2017-06-29T00:35:52Z","","","notion://www.notion.so/Neural-Machine-Translation-Seq2seq-Tutorial-2017-2022-5aaff3834ab84b8991f4d36bd0c58698","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Python","","","","","","","","",""
"4LKM3JW5","webpage","","","Google AI Blog: Transformer: A Novel Neural Network Architecture for Language Understanding","","","","","https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html","","","2022-09-30 00:25:57","2023-05-23 01:24:58","2022-09-30 00:25:57","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\BG4WIG95\transformer-novel-neural-network.html; ","notion://www.notion.so/Google-AI-Blog-Transformer-A-Novel-Neural-Network-Architecture-for-Language-Understanding-n-d-7cdcc34041b54b50a567c4cb8bb9e177","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W8SQTEBX","webpage","","","The Annotated Transformer","","","","","http://nlp.seas.harvard.edu/annotated-transformer/#prelims","","","2022-09-30 00:25:42","2023-05-23 01:24:55","2022-09-30 00:25:42","","","","","","","","","","","","","","","","","","","","","","","","notion://www.notion.so/The-Annotated-Transformer-n-d-e86727b1bd2a409e910a6509b1b89515","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3J6CCU94","preprint","2022","Ding, Frances; Hardt, Moritz; Miller, John; Schmidt, Ludwig","Retiring Adult: New Datasets for Fair Machine Learning","","","","10.48550/arXiv.2108.04884","http://arxiv.org/abs/2108.04884","Although the fairness community has recognized the importance of data, researchers in the area primarily rely on UCI Adult when it comes to tabular data. Derived from a 1994 US Census survey, this dataset has appeared in hundreds of research papers where it served as the basis for the development and comparison of many algorithmic fairness interventions. We reconstruct a superset of the UCI Adult data from available US Census sources and reveal idiosyncrasies of the UCI Adult dataset that limit its external validity. Our primary contribution is a suite of new datasets derived from US Census surveys that extend the existing data ecosystem for research on fair machine learning. We create prediction tasks relating to income, employment, health, transportation, and housing. The data span multiple years and all states of the United States, allowing researchers to study temporal shift and geographic variation. We highlight a broad initial sweep of new empirical insights relating to trade-offs between fairness criteria, performance of algorithmic interventions, and the role of distribution shift based on our new datasets. Our findings inform ongoing debates, challenge some existing narratives, and point to future research directions. Our datasets are available at https://github.com/zykls/folktables.","2022-01-09","2022-09-30 00:07:17","2023-05-23 01:24:53","2022-09-30 00:07:17","","","","","","","Retiring Adult","","","","","arXiv","","","","","","","arXiv.org","","89 citations (Semantic Scholar/arXiv) [2022-12-20] arXiv:2108.04884 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\IG6GG98D\2108.html; C:\Users\ambreen.hanif\Zotero\storage\IW39VQHZ\Ding et al_2022_Retiring Adult.pdf; ","notion://www.notion.so/Ding-et-al-2022-acab1da7c009426cb1bcacd2575f1e81","notion","","","","","","","","","","","","","","","","","","","","arXiv:2108.04884","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CAK77C6S","webpage","","","valeman/awesome-conformal-prediction: A professionally curated list of awesome Conformal Prediction videos, tutorials, books, papers, PhD and MSc theses, articles and open-source libraries.","","","","","https://github.com/valeman/awesome-conformal-prediction","","","2022-09-29 23:58:12","2023-05-23 01:24:50","2022-09-29 23:58:12","","","","","","","","","","","","","","","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\A2EJTXS5\awesome-conformal-prediction.html","notion://www.notion.so/Valeman-Awesome-Conformal-Prediction-A-Professionally-Curated-List-of-Awesome-Conformal-Prediction--b3f190413d0540a2b179220dc4c02298","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PFFG73F6","preprint","2022","Angelopoulos, Anastasios N.; Bates, Stephen","A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification","","","","10.48550/arXiv.2107.07511","http://arxiv.org/abs/2107.07511","Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Conformal prediction is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-specified probability, such as 90%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on. This hands-on introduction is aimed to provide the reader a working understanding of conformal prediction and related distribution-free uncertainty quantification techniques with one self-contained document. We lead the reader through practical theory for and examples of conformal prediction and describe its extensions to complex machine learning tasks involving structured outputs, distribution shift, time-series, outliers, models that abstain, and more. Throughout, there are many explanatory illustrations, examples, and code samples in Python. With each code sample comes a Jupyter notebook implementing the method on a real-data example; the notebooks can be accessed and easily run using our codebase.","2022-09-03","2022-09-29 23:53:36","2023-05-23 01:24:45","2022-09-29 23:53:36","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","8 citations (Semantic Scholar/arXiv) [2022-12-20] arXiv:2107.07511 [cs, math, stat]","","C:\Users\ambreen.hanif\Zotero\storage\GGJ5M9QQ\Angelopoulos_Bates_2022_A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty.pdf; C:\Users\ambreen.hanif\Zotero\storage\DHXIURE4\2107.html; ","notion://www.notion.so/Angelopoulos-Bates-2022-2b98021a829044fc9b054a9e55f37664","notion","","","","","","","","","","","","","","","","","","","","arXiv:2107.07511","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BHWAGFB2","webpage","","","scikit-learn-contrib/MAPIE: A scikit-learn-compatible module for estimating prediction intervals.","","","","","https://github.com/scikit-learn-contrib/MAPIE","","","2022-09-29 23:48:59","2023-05-23 01:24:43","2022-09-29 23:48:59","","","","","","","","","","","","","","","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\Q9UGMK59\MAPIE.html","notion://www.notion.so/Scikit-Learn-Contrib-MAPIE-A-Scikit-Learn-Compatible-Module-for-Estimating-Prediction-Intervals-n-52932d8b532d43dea72fbca21c28453c","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S5YRBGZ3","thesis","2020","Chandra, Sarath","VIRTUAL BANK ASSISTANCE: AN AI BASED VOICE BOT FOR BETTER BANKING","","","","","","A banking bot project is built using Artificial Intelligence algorithms that analyze the user's queries and understand the user's message. The system is designed for banks to use where users can ask any bank related questions like loan, account, policy, etc which are bank related queries. This application is developed for devices that have internet connectivity. The system recognizes the user's query and understands what he wants to convey and simultaneously answers them appropriately. At present, there are chat applications for banks. The questions asked by the users can be in any format. There is no specific format for users to ask questions. The built-in artificial intelligence system realizes users' requirements and provides suitable answers to the user. These voice bots can be built from scratch or they can be deployed on existing chat-bots by enabling then with voice services. It also uses a graphical representation (if necessary) of a person speaking while giving answers as a real person would do as an employee. Bank bot solves the issues a user has and clarifies it with its knowledge.","2020-01-21","2022-09-12 04:23:45","2023-05-23 01:24:39","","","","","","","","VIRTUAL BANK ASSISTANCE","","","","","","","","","","","","ResearchGate","","DOI: 10.13140/RG.2.2.21535.10405","","; ","notion://www.notion.so/Chandra-2020-9bc92ea4a42245bc8be8a046e3bec1d8; https://www.researchgate.net/profile/Sarath-Chandra-11/publication/339500060_VIRTUAL_BANK_ASSISTANCE_AN_AI_BASED_VOICE_BOT_FOR_BETTER_BANKING/links/5e564ab1299bf1bdb83b2c21/VIRTUAL-BANK-ASSISTANCE-AN-AI-BASED-VOICE-BOT-FOR-BETTER-BANKING.pdf","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XTCEHJW6","webpage","","","Machine learning systems design","","","","","https://huyenchip.com/machine-learning-systems-design/design-a-machine-learning-system.html","","","2022-09-12 00:47:54","2023-05-23 01:24:37","2022-09-12 00:47:54","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\9CJRRG3X\design-a-machine-learning-system.html; ","notion://www.notion.so/Machine-Learning-Systems-Design-n-d-d1f83443fecd4097916099c48690d478","notion; Machine Learning; System Design","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZUP7YI5F","conferencePaper","2021","Ghodratnama, Samira; Zakershahrak, Mehrdad; Beheshti, Amin","Summary2vec: Learning Semantic Representation of Summaries for Healthcare Analytics","2021 International Joint Conference on Neural Networks (IJCNN)","","","10.1109/IJCNN52387.2021.9533922","","The ever-increasing amount of text data makes it challenging for humans to extract the required information. This problem is even more vital for the medical domain, where accessing up-to-date information is essential. Physicians and researchers face diverse and extensive medical sources such as medical journal articles, websites, or patient records. Therefore, they require to analyze them based on their interests and needs quickly. Intelligent content summarization approaches are assisting tools in such situations to provides an overview of a set of documents. However, the summary is required to be tailored to two different users type preferences: the physician and the patient. This paper proposes a novel embedding method, called Summary2vec, where each summary is presented by a fixed -length vector covering various aspects of information space. Summary2vec is remedial to design automatic services for various analytic purposes that require information-seeking activity. We leverage Summary2vec to produce a hierarchical summarization structure to enable users navigating through the hierarchy to gain more elaborated information upon request by engaging them.","2021-07","2022-09-06 08:11:35","2023-05-23 01:24:34","","1-8","","","","","","Summary2vec","","","","","IEEE","","","","","","","IEEE Xplore","","4 citations (Semantic Scholar/DOI) [2022-12-20] 1 citations (Crossref) [2022-12-20] ISSN: 2161-4407","","C:\Users\ambreen.hanif\Zotero\storage\DA5QB458\Ghodratnama et al_2021_Summary2vec.pdf; C:\Users\ambreen.hanif\Zotero\storage\PLAGJLRR\9533922.html; ","notion://www.notion.so/Ghodratnama-et-al-2021-9f5429856e594d93888dfb033787040e","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2021 International Joint Conference on Neural Networks (IJCNN)","","","","","","","","","","","","","","",""
"MFYGFGWD","conferencePaper","2019","Tabebordbar, Alireza; Beheshti, Amin; Benatallah, Boualem","ConceptMap: A Conceptual Approach for Formulating User Preferences in Large Information Spaces","Web Information Systems Engineering – WISE 2019","978-3-030-34223-4","","10.1007/978-3-030-34223-4_49","","In a large information space a user needs to iteratively investigate the data to formulate her preferences for IR systems. In recent years several visualization techniques have been proposed to help a user to better formulate her preferences. However, using these solutions a user needs to explicitly specify her preferences for IR systems in forms of keywords or phrases. In this paper we present ConceptMap, a system that takes the advantage of deep learning and a knowledge lake to provide a conceptual summary of the information space. ConceptMap allows a user to specify her preferences implicitly as a set of concepts without the need to iteratively investigate the information space. It provides a 2D Radial Map of concepts where a user can rank items relevant to her preferences through dragging and dropping. Our experiment results shows that ConceptMap can help users to better formulate their preferences when they need to retrieve varied and comprehensive list of information across a large amount of data.","2019","2022-09-06 03:09:14","2023-05-23 01:24:33","","779-794","","","","","","ConceptMap","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","16 citations (Semantic Scholar/DOI) [2022-12-20] 5 citations (Crossref) [2022-12-20]","","; C:\Users\ambreen.hanif\Zotero\storage\TSIDF4DL\Tabebordbar et al_2019_ConceptMap.pdf","notion://www.notion.so/Tabebordbar-et-al-2019-fcd0efcdc2d24f4baced543987a15b96","notion","","Cheng, Reynold; Mamoulis, Nikos; Sun, Yizhou; Huang, Xin","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y5US4Q9G","journalArticle","2017","Goodman, Bryce; Flaxman, Seth","European Union Regulations on Algorithmic Decision-Making and a “Right to Explanation”","AI Magazine","","2371-9621","10.1609/aimag.v38i3.2741","https://ojs.aaai.org/index.php/aimagazine/article/view/2741","We summarize the potential impact that the European Union’s new General Data Protection Regulation will have on the routine use of machine learning algorithms. Slated to take effect as law across the EU in 2018, it will restrict automated individual decision-making (that is, algorithms that make decisions based on user-level predictors) which “significantly affect” users. The law will also effectively create a “right to explanation,” whereby a user can ask for an explanation of an algorithmic decision that was made about them. We argue that while this law will pose large challenges for industry, it highlights opportunities for computer scientists to take the lead in designing algorithms and evaluation frameworks which avoid discrimination and enable explanation.","2017-10-02","2022-09-06 00:35:03","2023-05-23 01:24:31","2022-09-06 00:35:03","50-57","","3","38","","","","","","","","","","en","Copyright (c) 2017 AI Magazine","","","","ojs.aaai.org","","1295 citations (Semantic Scholar/DOI) [2022-12-20] 548 citations (Crossref) [2022-12-20] Number: 3 QID: Q30302901","","C:\Users\ambreen.hanif\Zotero\storage\G36JL7ST\Goodman_Flaxman_2017_European Union Regulations on Algorithmic Decision-Making and a “Right to.pdf; ","notion://www.notion.so/Goodman-Flaxman-2017-91b40589bb2249c787bbdee5847e37c9","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GSV852WY","webpage","","","Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","ACM Conferences","","","","https://dl-acm-org.simsrad.net.ocs.mq.edu.au/doi/proceedings/10.1145/2939672","","","2022-09-06 00:34:45","2023-05-23 01:24:29","2022-09-06 00:34:45","","","","","","","","","","","","","","en","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\ZVG3Z724\2939672.html","notion://www.notion.so/Proceedings-of-the-22nd-ACM-SIGKDD-International-Conference-on-Knowledge-Discovery-and-Data-Mining--9a4899737fae4819814db73aa12c001e","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XPMEBVFY","book","2021","Soofastaei, Ali","Virtual Assistant","","978-1-83968-807-2","","","","An intelligent virtual assistant (IVA) or intelligent personal assistant (IPA) is a software agent that can perform tasks or services for an individual based on commands or questions. Improving the quality of artificial intelligence (AI) learning algorithms increases the application of IVAs in different areas. The capabilities and usage of IVAs are expanding rapidly. IVAs, such as Siri, Alexa, and chatbots, help individuals and companies to make better decisions. They learn from collected historical data, and the quality of their recommendations depends on the size of the database they are using. Modern technology has provided a huge capacity for data collection and storage. This means that the new generation of IVAs can help people much better than the previous one. This book examines the applications of IVAs in different areas and presents a clear vision of how this new technology can be used in current and future activities. Chapters cover such topics as the scientific development of VA technology, generating voices for IVAs, the ethics of using IVAs, and using IVAs in banking and finance.","2021-10-13","2022-09-01 06:42:34","2023-05-23 01:24:26","","","123","","","","","","","","","","BoD – Books on Demand","","en","","","","","Google Books","","Google-Books-ID: H7daEAAAQBAJ","","; ","https://books.google.com.au/books?id=H7daEAAAQBAJ; notion://www.notion.so/Soofastaei-2021-0af0e1ac52b9476ebc2ce12b889d4e5c","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"348VXI2J","journalArticle","2014","Rong, Xin","word2vec Parameter Learning Explained","","","","10.48550/arXiv.1411.2738","https://arxiv.org/abs/1411.2738v4","The word2vec model and application by Mikolov et al. have attracted a great amount of attention in recent two years. The vector representations of words learned by word2vec models have been shown to carry semantic meanings and are useful in various NLP tasks. As an increasing number of researchers would like to experiment with word2vec or similar techniques, I notice that there lacks a material that comprehensively explains the parameter learning process of word embedding models in details, thus preventing researchers that are non-experts in neural networks from understanding the working mechanism of such models. This note provides detailed derivations and explanations of the parameter update equations of the word2vec models, including the original continuous bag-of-word (CBOW) and skip-gram (SG) models, as well as advanced optimization techniques, including hierarchical softmax and negative sampling. Intuitive interpretations of the gradient equations are also provided alongside mathematical derivations. In the appendix, a review on the basics of neuron networks and backpropagation is provided. I also created an interactive demo, wevi, to facilitate the intuitive understanding of the model.","2014-11-11","2022-08-31 00:54:01","2023-05-23 01:24:11","2022-08-31 00:54:01","","","","","","","","","","","","","","en","","","","","arxiv.org","","634 citations (Semantic Scholar/arXiv) [2022-12-20]","","; C:\Users\ambreen.hanif\Zotero\storage\VTTDLR9C\Rong_2014_word2vec Parameter Learning Explained.pdf; C:\Users\ambreen.hanif\Zotero\storage\VCYNJWLF\1411.html","notion://www.notion.so/Rong-2014-37344ec3fe8044c4bdf278f8e60b030c","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V4SJXT4D","journalArticle","2021","Berro, Auday; Fard, Mohammad-Ali Yaghub Zade; Baez, Marcos; Benatallah, Boualem; Benabdeslem, Khalid","An extensible and reusable pipeline for automated utterance paraphrases","Proceedings of the VLDB Endowment","","2150-8097","10.14778/3476311.3476358","https://doi.org/10.14778/3476311.3476358","In this demonstration paper we showcase an extensible and reusable pipeline for automatic paraphrase generation, i.e., reformulating sentences using different words. Capturing the nuances of human language is fundamental to the effectiveness of Conversational AI systems, as it allows them to deal with the different ways users can utter their requests in natural language. Traditional approaches to utterance paraphrasing acquisition, such as hiring experts or crowd-sourcing, involve processes that are often costly or time consuming, and with their own trade-offs in terms of quality. Automatic paraphrasing is emerging as an attractive alternative that promises a fast, scalable and cost-effective process. In this paper we showcase how our extensible and reusable pipeline for automated utterance paraphrasing can support the development of Conversational AI systems by integrating and extending existing techniques under an unified and configurable framework.","2021-07-01","2022-08-26 01:00:15","2023-05-23 01:24:09","2022-08-26 01:00:15","2839–2842","","12","14","","Proc. VLDB Endow.","","","","","","","","","","","","","July 2021","","1 citations (Semantic Scholar/DOI) [2022-12-20] 1 citations (Crossref) [2022-12-20]","","C:\Users\ambreen.hanif\Zotero\storage\EJ82S8HT\Berro et al_2021_An extensible and reusable pipeline for automated utterance paraphrases.pdf; ","notion://www.notion.so/Berro-et-al-2021-ae87153341cd4d10bc6ae94b332c7db6","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BMLCXR6Z","journalArticle","2006","Lombrozo, Tania","The structure and function of explanations","Trends in Cognitive Sciences","","1364-6613","10.1016/j.tics.2006.08.004","https://www.sciencedirect.com/science/article/pii/S1364661306002117","Generating and evaluating explanations is spontaneous, ubiquitous and fundamental to our sense of understanding. Recent evidence suggests that in the course of an individual's reasoning, engaging in explanation can have profound effects on the probability assigned to causal claims, on how properties are generalized and on learning. These effects follow from two properties of the structure of explanations: explanations accommodate novel information in the context of prior beliefs, and do so in a way that fosters generalization. The study of explanation thus promises to shed light on core cognitive issues, such as learning, induction and conceptual representation. Moreover, the influence of explanation on learning and inference presents a challenge to theories that neglect the roles of prior knowledge and explanation-based reasoning.","2006-10-01","2022-08-18 01:59:43","2023-05-23 01:24:06","2022-08-18 01:59:43","464-470","","10","10","","Trends in Cognitive Sciences","","","","","","","","en","","","","","ScienceDirect","","","","; ; ; C:\Users\ambreen.hanif\Zotero\storage\WSZWEPNB\S1364661306002117.html","notion://www.notion.so/Lombrozo-2006-78afb7556ae14f3e8c782d9a3680c8be; http://www.ncbi.nlm.nih.gov/pubmed/16942895; http://www.ncbi.nlm.nih.gov/pubmed/16942895","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WXB2MBEK","webpage","","","Responsible AI practices","Google AI","","","","https://ai.google/responsibilities/responsible-ai-practices/","We're committed to progress in the responsible development of AI and to sharing knowledge, research, tools, datasets, and other resources. Learn more about recommended practices and our current work.","","2022-08-18 01:36:03","2023-05-23 01:24:01","2022-08-18 01:36:03","","","","","","","","","","","","","","en","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\YWCLFG5T\responsible-ai-practices.html","notion://www.notion.so/Responsible-AI-Practices-n-d-f41b26262f134689b61ee3a3f5fcdb43","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UNHVBUI6","preprint","2020","Rohe, Karl; Zeng, Muzhe","Vintage Factor Analysis with Varimax Performs Statistical Inference","","","","","http://arxiv.org/abs/2004.05387","Psychologists developed Multiple Factor Analysis to decompose multivariate data into a small number of interpretable factors without any a priori knowledge about those factors. In this form of factor analysis, the Varimax ""factor rotation"" is a key step to make the factors interpretable. Charles Spearman and many others objected to factor rotations because the factors seem to be rotationally invariant. These objections are still reported in all contemporary multivariate statistics textbooks. This is an engima because this vintage form of factor analysis has survived and is widely popular because, empirically, the factor rotation often makes the factors easier to interpret. We argue that the rotation makes the factors easier to interpret because, in fact, the Varimax factor rotation performs statistical inference. We show that Principal Components Analysis (PCA) with the Varimax rotation provides a unified spectral estimation strategy for a broad class of modern factor models, including the Stochastic Blockmodel and a natural variation of Latent Dirichlet Allocation (i.e., ""topic modeling""). In addition, we show that Thurstone's widely employed sparsity diagnostics implicitly assess a key ""leptokurtic"" condition that makes the rotation statistically identifiable in these models. Taken together, this shows that the know-how of Vintage Factor Analysis performs statistical inference, reversing nearly a century of statistical thinking on the topic. With a sparse eigensolver, PCA with Varimax is both fast and stable. Combined with Thurstone's straightforward diagnostics, this vintage approach is suitable for a wide array of modern applications.","2020-04-20","2022-08-17 23:44:17","2023-05-23 01:23:58","2022-08-17 23:44:16","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","18 citations (Semantic Scholar/arXiv) [2022-12-20] arXiv:2004.05387 [math, stat]","","C:\Users\ambreen.hanif\Zotero\storage\W4I47L8M\Rohe and Zeng - 2020 - Vintage Factor Analysis with Varimax Performs Stat.pdf; C:\Users\ambreen.hanif\Zotero\storage\EWMMANRP\2004.html; ","notion://www.notion.so/Rohe-Zeng-2020-2ccdc0ccfeba44679f433ea4b82dc515","notion","","","","","","","","","","","","","","","","","","","","arXiv:2004.05387","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KTIT3U5J","bookSection","2022","Hitchcock, Christopher","Causal Models","The Stanford Encyclopedia of Philosophy","","","","https://plato.stanford.edu/archives/spr2022/entries/causal-models/","Causal models are mathematical models representing causalrelationships within an individual system or population. Theyfacilitate inferences about causal relationships from statisticaldata. They can teach us a good deal about the epistemology ofcausation, and about the relationship between causation andprobability. They have also been applied to topics of interest tophilosophers, such as the logic of counterfactuals, decision theory,and the analysis of actual causation.","2022","2022-08-17 23:44:09","2023-05-23 01:23:57","2022-08-17 23:44:09","","","","","","","","","","","","Metaphysics Research Lab, Stanford University","","","","","","","Stanford Encyclopedia of Philosophy","","","","; C:\Users\ambreen.hanif\Zotero\storage\LYQ69AYU\causal-models.html","notion://www.notion.so/Hitchcock-2022-6a01b6140e2c4f1098eb861a241c7187","notion","","Zalta, Edward N.","","","","","","","","","","","","","","","","","","","Spring 2022","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NCIXNSIQ","conferencePaper","2016","Fast, Ethan; Chen, Binbin; Bernstein, Michael","Empath: Understanding Topic Signals in Large-Scale Text","Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems","","","10.1145/2858036.2858535","http://arxiv.org/abs/1602.06979","Human language is colored by a broad range of topics, but existing text analysis tools only focus on a small number of them. We present Empath, a tool that can generate and validate new lexical categories on demand from a small set of seed terms (like ""bleed"" and ""punch"" to generate the category violence). Empath draws connotations between words and phrases by deep learning a neural embedding across more than 1.8 billion words of modern fiction. Given a small set of seed words that characterize a category, Empath uses its neural embedding to discover new related terms, then validates the category with a crowd-powered filter. Empath also analyzes text across 200 built-in, pre-validated categories we have generated from common topics in our web dataset, like neglect, government, and social media. We show that Empath's data-driven, human validated categories are highly correlated (r=0.906) with similar categories in LIWC.","2016-05-07","2022-08-15 04:18:41","2023-05-23 01:23:54","2022-08-15 04:18:41","4647-4657","","","","","","Empath","","","","","","","","","","","","arXiv.org","","302 citations (Semantic Scholar/arXiv) [2022-12-20] 302 citations (Semantic Scholar/DOI) [2022-12-20] 150 citations (Crossref) [2022-12-20] arXiv:1602.06979 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\S5KFFUAI\1602.html; C:\Users\ambreen.hanif\Zotero\storage\WLUWVG2R\Fast et al_2016_Empath.pdf; ","notion://www.notion.so/Fast-et-al-2016-1c9eedc49e9446a4bd36c952f52058d3","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5U5X5UMC","journalArticle","","Chromik, Michael","Human-centric Explanation Facilities: Explainable AI for the Pragmatic Understanding of Non-expert End Users","","","","","","","","2022-08-12 02:24:43","2023-05-23 01:23:52","","125","","","","","","","","","","","","","de","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\DFB49B6S\Chromik - Human-centric Explanation Facilities Explainable .pdf; ","notion://www.notion.so/Chromik-n-d-e31e8aae90b642c48c4d1f6e8047ca2d","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6MIQKJZG","webpage","","","9 Transformers · Deep Learning for Natural Language Processing MEAP V11","","","","","https://livebook.manning.com/book/deep-learning-for-natural-language-processing/chapter-1/v-11/","Taking a short road trip through machine learning applied to NLP; Learning about the historical roots of deep learning; Introducing vector-based representations of language;","","2022-08-12 02:24:06","2023-05-23 01:23:48","2022-08-12 02:24:06","","","","","","","","","","","","","","en-US","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\UQPDUR5K\v-11.html","notion://www.notion.so/9-Transformers-Deep-Learning-for-Natural-Language-Processing-MEAP-V11-n-d-28be79813f9b403e87a0f3c394701744","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EZF2PLGJ","conferencePaper","2019","Gade, Krishna; Geyik, Sahin Cem; Kenthapadi, Krishnaram; Mithal, Varun; Taly, Ankur","Explainable AI in Industry","Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining","978-1-4503-6201-6","","10.1145/3292500.3332281","https://dl.acm.org/doi/10.1145/3292500.3332281","","2019-07-25","2022-08-12 02:23:30","2023-05-23 01:23:46","2022-08-12 02:23:30","3203-3204","","","","","","","","","","","ACM","Anchorage AK USA","en","","","","","DOI.org (Crossref)","","68 citations (Semantic Scholar/DOI) [2022-12-20] 36 citations (Crossref) [2022-12-20]","","C:\Users\ambreen.hanif\Zotero\storage\P286ERGQ\Gade et al. - 2019 - Explainable AI in Industry.pdf; ","notion://www.notion.so/Gade-et-al-2019-92be9fa716d04028b3a05fa54377fd6d","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","KDD '19: The 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining","","","","","","","","","","","","","","",""
"QSG57PPX","journalArticle","2022","Laato, Samuli; Tiainen, Miika; Najmul Islam, A.K.M.; Mäntymäki, Matti","How to explain AI systems to end users: a systematic literature review and research agenda","Internet Research","","1066-2243","10.1108/INTR-08-2021-0600","https://doi.org/10.1108/INTR-08-2021-0600","Purpose Inscrutable machine learning (ML) models are part of increasingly many information systems. Understanding how these models behave, and what their output is based on, is a challenge for developers let alone non-technical end users. Design/methodology/approach The authors investigate how AI systems and their decisions ought to be explained for end users through a systematic literature review. Findings The authors’ synthesis of the literature suggests that AI system communication for end users has five high-level goals: (1) understandability, (2) trustworthiness, (3) transparency, (4) controllability and (5) fairness. The authors identified several design recommendations, such as offering personalized and on-demand explanations and focusing on the explainability of key functionalities instead of aiming to explain the whole system. There exists multiple trade-offs in AI system explanations, and there is no single best solution that fits all cases. Research limitations/implications Based on the synthesis, the authors provide a design framework for explaining AI systems to end users. The study contributes to the work on AI governance by suggesting guidelines on how to make AI systems more understandable, fair, trustworthy, controllable and transparent. Originality/value This literature review brings together the literature on AI system communication and explainable AI (XAI) for end users. Building on previous academic literature on the topic, it provides synthesized insights, design recommendations and future research agenda.","2022-01-01","2022-08-12 02:23:21","2023-05-23 01:23:45","2022-08-12 02:23:21","1-31","","7","32","","","How to explain AI systems to end users","","","","","","","","","","","","Emerald Insight","","5 citations (Semantic Scholar/DOI) [2022-12-20] 4 citations (Crossref) [2022-12-20] Publisher: Emerald Publishing Limited","","C:\Users\ambreen.hanif\Zotero\storage\62JMBLBL\Laato et al_2022_How to explain AI systems to end users.pdf; ","notion://www.notion.so/Laato-et-al-2022-57fbd135992e4cdd9a0fc8f323331513","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6TK42W4E","journalArticle","2022","Miró-Nicolau, Miquel; Moyà-Alcover, Gabriel; Jaume-i-Capó, Antoni","Evaluating Explainable Artificial Intelligence for X-ray Image Analysis","","","","10.3390/app12094459","","The lack of justiﬁcation of the results obtained by artiﬁcial intelligence (AI) algorithms has limited their usage in the medical context. To increase the explainability of the existing AI methods, explainable artiﬁcial intelligence (XAI) is proposed. We performed a systematic literature review, based on the guidelines proposed by Kitchenham and Charters, of studies that applied XAI methods in X-ray-image-related tasks. We identiﬁed 141 studies relevant to the objective of this research from ﬁve different databases. For each of these studies, we assessed the quality and then analyzed them according to a speciﬁc set of research questions. We determined two primary purposes for X-ray images: the detection of bone diseases and lung diseases. We found that most of the AI methods used were based on a CNN. We identiﬁed the different techniques to increase the explainability of the models and grouped them depending on the kind of explainability obtained. We found that most of the articles did not evaluate the quality of the explainability obtained, causing problems of conﬁdence in the explanation. Finally, we identiﬁed the current challenges and future directions of this subject and provide guidelines to practitioners and researchers to improve the limitations and the weaknesses that we detected.","2022","2022-08-12 02:21:07","2023-05-23 01:23:44","","28","","","","","","","","","","","","","en","","","","","Zotero","","1 citations (Crossref) [2022-12-20]","","C:\Users\ambreen.hanif\Zotero\storage\NAXWRSUT\Miró-Nicolau et al. - 2022 - Evaluating Explainable Artificial Intelligence for.pdf; ","notion://www.notion.so/Mir-Nicolau-et-al-2022-457fc5fc54904736963cb7eaa5aede36","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NN9GYNQ6","journalArticle","","Lécué, Freddy","XAI - Explanation in AI: From Machine Learning to Knowledge Representation & Reasoning and Beyond","","","","","","","","2022-08-12 02:20:57","2023-05-23 01:23:41","","81","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\YERXVBRC\Lécué - XAI - Explanation in AI From Machine Learning to .pdf; ","notion://www.notion.so/L-cu-n-d-d6907e9ef0384bcfb4fd95faf3dc6f79","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7QXZQ44E","journalArticle","2020","Chromik, Michael; Schuessler, Martin","A Taxonomy for Human Subject Evaluation of Black-Box Explanations in XAI","","","","","","The interdisciplinary field of explainable artificial intelligence (XAI) aims to foster human understanding of black-box machine learning models through explanation methods. However, there is no consensus among the involved disciplines regarding the evaluation of their effectiveness - especially concerning the involvement of human subjects. For our community, such involvement is a prerequisite for rigorous evaluation. To better understand how researchers across the disciplines approach human subject XAI evaluation, we propose developing a taxonomy that is iterated with a systematic literature review. Approaching them from an HCI perspective, we analyze which study designs scholar chose for different explanation goals. Based on our preliminary analysis, we present a taxonomy that provides guidance for researchers and practitioners on the design and execution of XAI evaluations. With this position paper, we put our survey approach and preliminary results up for discussion with our fellow researchers.","2020","2022-08-12 02:19:33","2023-05-23 01:23:38","","7","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\W658IEHU\Chromik and Schuessler - 2020 - A Taxonomy for Human Subject Evaluation of Black-B.pdf; ","notion://www.notion.so/Chromik-Schuessler-2020-6adff47c763242a6a06e7657f2ae5fee","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TNIT99G8","journalArticle","","Chakrobartty, Shuvro; El-Gayar, Omar","Explainable Artificial Intelligence in the Medical Domain: A Systematic Review","","","","","","The applications of Artificial Intelligence (AI) and Machine Learning (ML) techniques in different medical fields is rapidly growing. AI holds great promise in terms of beneficial, accurate and effective preventive and curative interventions. At the same time, there is also concerns regarding potential risks, harm and trust issues arising from the opacity of some AI algorithms because of their un-explainability. Overall, how can the decisions from these AI-based systems be trusted if the decision-making logic cannot be properly explained? Explainable Artificial Intelligence (XAI) tries to shed light to these questions. We study the recent development on this topic within the medical domain. The objective of this study is to provide a systematic review of the methods and techniques of explainable AI within the medical domain as observed within the literature while identifying future research opportunities.","","2022-08-12 02:18:59","2023-05-23 01:23:36","","12","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\H849LS25\Chakrobartty and El-Gayar - Explainable Artificial Intelligence in the Medical.pdf; ","notion://www.notion.so/Chakrobartty-El-Gayar-n-d-d2b71e99ad6f4e76b52d45465bbac139","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"92CTNE8K","journalArticle","2019","Anjomshoae, Sule; Najjar, Amro; Calvaresi, Davide; Främling, Kary","Explainable Agents and Robots: Results from a Systematic Literature Review","","","","","","Humans are increasingly relying on complex systems that heavily adopts Artificial Intelligence (AI) techniques. Such systems are employed in a growing number of domains, and making them explainable is an impelling priority. Recently, the domain of eXplainable Artificial Intelligence (XAI) emerged with the aims of fostering transparency and trustworthiness. Several reviews have been conducted. Nevertheless, most of them deal with data-driven XAI to overcome the opaqueness of black-box algorithms. Contributions addressing goal-driven XAI (e.g., explainable agency for robots and agents) are still missing. This paper aims at filling this gap, proposing a Systematic Literature Review. The main findings are (i) a considerable portion of the papers propose conceptual studies, or lack evaluations or tackle relatively simple scenarios; (ii) almost all of the studied papers deal with robots/agents explaining their behaviors to the human users, and very few works addressed inter-robot (inter-agent) explainability. Finally, (iii) while providing explanations to non-expert users has been outlined as a necessity, only a few works addressed the issues of personalization and context-awareness.","2019","2022-08-12 02:18:51","2023-05-23 01:23:35","","11","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\5AP4GVYL\Anjomshoae et al. - 2019 - Explainable Agents and Robots Results from a Syst.pdf; ","notion://www.notion.so/Anjomshoae-et-al-2019-bdec26f4142241f4a831390baeb4e4e7","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GAK3XFBJ","journalArticle","2022","Qasim, Iqbal; Awan, Mujtaba; Ali, Sikandar; Khan, Shumaila; Mosleh, Mogeeb A. A.; Alsanad, Ahmed; Khattak, Hizbullah; Alam, Mahmood","Affinity Propagation-Based Hybrid Personalized Recommender System","Complexity","","1076-2787","10.1155/2022/6958596","https://www.hindawi.com/journals/complexity/2022/6958596/","A personalized recommender system is broadly accepted as a helpful tool to handle the information overload issue while recommending a related piece of information. This work proposes a hybrid personalized recommender system based on affinity propagation (AP), namely, APHPRS. Affinity propagation is a semisupervised machine learning algorithm used to cluster items based on similarities among them. In our approach, we first calculate the cluster quality and density and then combine their outputs to generate a new ranking score among clusters for the personalized recommendation. In the first phase, user preferences are collected and normalized as items rating matrix. This generated matrix is then clustered offline using affinity propagation and kept in a database for future recommendations. In the second phase, online recommendations are generated by applying the offline model. Negative Euclidian similarity and the quality of clusters are used together to select the best clusters for recommendations. The proposed APHPRS system alleviates problems such as sparsity and cold-start problems. The use of affinity propagation and the hybrid recommendation technique used in the proposed approach helps in improving results against sparsity. Experiments reveal that the proposed APHPRS performs better than most of the existing recommender systems.","2022-01-28","2022-08-12 02:09:15","2023-05-23 01:23:33","2022-08-12 02:09:15","e6958596","","","2022","","","","","","","","","","en","","","","","www.hindawi.com","","0 citations (Semantic Scholar/DOI) [2022-12-20] 0 citations (Crossref) [2022-12-20] Publisher: Hindawi","","; C:\Users\ambreen.hanif\Zotero\storage\Y253US2N\Qasim et al_2022_Affinity Propagation-Based Hybrid Personalized Recommender System.pdf; C:\Users\ambreen.hanif\Zotero\storage\WKEWYWI2\6958596.html","notion://www.notion.so/Qasim-et-al-2022-b466fa23d5ae46a3ba7933a331f97783","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FWQ55IKX","preprint","2022","Bommasani, Rishi; Hudson, Drew A.; Adeli, Ehsan; Altman, Russ; Arora, Simran; von Arx, Sydney; Bernstein, Michael S.; Bohg, Jeannette; Bosselut, Antoine; Brunskill, Emma; Brynjolfsson, Erik; Buch, Shyamal; Card, Dallas; Castellon, Rodrigo; Chatterji, Niladri; Chen, Annie; Creel, Kathleen; Davis, Jared Quincy; Demszky, Dora; Donahue, Chris; Doumbouya, Moussa; Durmus, Esin; Ermon, Stefano; Etchemendy, John; Ethayarajh, Kawin; Fei-Fei, Li; Finn, Chelsea; Gale, Trevor; Gillespie, Lauren; Goel, Karan; Goodman, Noah; Grossman, Shelby; Guha, Neel; Hashimoto, Tatsunori; Henderson, Peter; Hewitt, John; Ho, Daniel E.; Hong, Jenny; Hsu, Kyle; Huang, Jing; Icard, Thomas; Jain, Saahil; Jurafsky, Dan; Kalluri, Pratyusha; Karamcheti, Siddharth; Keeling, Geoff; Khani, Fereshte; Khattab, Omar; Koh, Pang Wei; Krass, Mark; Krishna, Ranjay; Kuditipudi, Rohith; Kumar, Ananya; Ladhak, Faisal; Lee, Mina; Lee, Tony; Leskovec, Jure; Levent, Isabelle; Li, Xiang Lisa; Li, Xuechen; Ma, Tengyu; Malik, Ali; Manning, Christopher D.; Mirchandani, Suvir; Mitchell, Eric; Munyikwa, Zanele; Nair, Suraj; Narayan, Avanika; Narayanan, Deepak; Newman, Ben; Nie, Allen; Niebles, Juan Carlos; Nilforoshan, Hamed; Nyarko, Julian; Ogut, Giray; Orr, Laurel; Papadimitriou, Isabel; Park, Joon Sung; Piech, Chris; Portelance, Eva; Potts, Christopher; Raghunathan, Aditi; Reich, Rob; Ren, Hongyu; Rong, Frieda; Roohani, Yusuf; Ruiz, Camilo; Ryan, Jack; Ré, Christopher; Sadigh, Dorsa; Sagawa, Shiori; Santhanam, Keshav; Shih, Andy; Srinivasan, Krishnan; Tamkin, Alex; Taori, Rohan; Thomas, Armin W.; Tramèr, Florian; Wang, Rose E.; Wang, William; Wu, Bohan; Wu, Jiajun; Wu, Yuhuai; Xie, Sang Michael; Yasunaga, Michihiro; You, Jiaxuan; Zaharia, Matei; Zhang, Michael; Zhang, Tianyi; Zhang, Xikun; Zhang, Yuhui; Zheng, Lucia; Zhou, Kaitlyn; Liang, Percy","On the Opportunities and Risks of Foundation Models","","","","10.48550/arXiv.2108.07258","http://arxiv.org/abs/2108.07258","AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.","2022-07-12","2022-08-12 02:06:36","2023-05-23 01:23:31","2022-08-12 02:06:36","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","593 citations (Semantic Scholar/arXiv) [2022-12-20] arXiv:2108.07258 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\LUB4W6CB\2108.html; C:\Users\ambreen.hanif\Zotero\storage\NNAY26DW\Bommasani et al_2022_On the Opportunities and Risks of Foundation Models.pdf; ","notion://www.notion.so/Bommasani-et-al-2022-bda11feccd144076b583ccc333c391a0","notion","","","","","","","","","","","","","","","","","","","","arXiv:2108.07258","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SNTFKH8V","webpage","","","Table of Contents","","","","","https://e2eml.school/blog.html#101","","","2022-08-08 04:10:15","2023-05-23 01:23:30","2022-08-08 04:10:15","","","","","","","","","","","","","","","","","","","","","","","","notion://www.notion.so/Table-of-Contents-n-d-9a7a96b542d14e588697a7c7f59e404d","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MLUNJY6M","computerProgram","2022","","Package Dependencies","","","","","https://github.com/harvardnlp/annotated-transformer","An annotated implementation of the Transformer paper.","2022-08-05","2022-08-08 04:07:10","2023-05-23 01:23:27","2022-08-08 04:07:10","","","","","","","","","","","","HNLP","","","MIT","","","","GitHub","","original-date: 2018-03-21T16:23:15Z","","","notion://www.notion.so/Package-Dependencies-2018-2022-463aba47543244cfb97f47b6bc8c817d","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Jupyter Notebook","","","","","","","","",""
"LI9GRL24","preprint","2021","Duval, Alexandre; Malliaros, Fragkiskos D.","GraphSVX: Shapley Value Explanations for Graph Neural Networks","","","","","http://arxiv.org/abs/2104.10482","Graph Neural Networks (GNNs) achieve significant performance for various learning tasks on geometric data due to the incorporation of graph structure into the learning of node representations, which renders their comprehension challenging. In this paper, we first propose a unified framework satisfied by most existing GNN explainers. Then, we introduce GraphSVX, a post hoc local model-agnostic explanation method specifically designed for GNNs. GraphSVX is a decomposition technique that captures the ""fair"" contribution of each feature and node towards the explained prediction by constructing a surrogate model on a perturbed dataset. It extends to graphs and ultimately provides as explanation the Shapley Values from game theory. Experiments on real-world and synthetic datasets demonstrate that GraphSVX achieves state-of-the-art performance compared to baseline models while presenting core theoretical and human-centric properties.","2021-07-13","2022-08-08 02:33:44","2023-05-23 01:23:25","2022-08-08 02:33:44","","","","","","","GraphSVX","","","","","arXiv","","","","","","","arXiv.org","","28 citations (Semantic Scholar/arXiv) [2022-12-20] arXiv:2104.10482 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\GDJVBX5S\2104.html; C:\Users\ambreen.hanif\Zotero\storage\R7RU6H8L\Duval_Malliaros_2021_GraphSVX.pdf; ","notion://www.notion.so/Duval-Malliaros-2021-50c1afc4b5754891b330d44975526af4","notion","","","","","","","","","","","","","","","","","","","","arXiv:2104.10482","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AHMQ5IEB","preprint","2017","Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, Lukasz; Polosukhin, Illia","Attention Is All You Need","","","","","http://arxiv.org/abs/1706.03762","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","2017-12-05","2022-08-08 02:33:12","2023-05-23 01:23:09","2022-08-08 02:33:12","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","9999 citations (Semantic Scholar/arXiv) [2022-12-20] arXiv:1706.03762 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\QBF7VB4F\1706.html; ; C:\Users\ambreen.hanif\Zotero\storage\3AVW8JS4\Vaswani et al_2017_Attention Is All You Need.pdf","notion://www.notion.so/Vaswani-et-al-2017-4f7ee127ad1a484dbbd49aeb871644a6","notion","","","","","","","","","","","","","","","","","","","","arXiv:1706.03762","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"68FX4PPS","computerProgram","2022","","Hands-On Explainable AI (XAI) with Python","","","","","https://github.com/PacktPublishing/Hands-On-Explainable-AI-XAI-with-Python","Explainable AI with Python, published by Packt","2022-08-03","2022-08-08 02:25:08","2023-05-23 01:23:07","2022-08-08 02:25:08","","","","","","","","","","","","Packt","","","MIT","","","","GitHub","","original-date: 2020-02-21T09:05:25Z","","","notion://www.notion.so/Hands-On-Explainable-AI-XAI-with-Python-2020-2022-d8d30f2f93ad441bb82c94531377dce0","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Jupyter Notebook","","","","","","","","",""
"UPLZRVKY","computerProgram","2022","Yasunaga, Michihiro","QA-GNN: Question Answering using Language Models and Knowledge Graphs","","","","","https://github.com/michiyasunaga/qagnn","[NAACL 2021] QAGNN: Question Answering using Language Models and Knowledge Graphs 🤖","2022-08-01","2022-08-05 05:55:00","2023-05-23 01:23:05","2022-08-05 05:55:00","","","","","","","QA-GNN","","","","","","","","MIT","","","","GitHub","","original-date: 2021-04-05T19:02:12Z","","","notion://www.notion.so/Yasunaga-2021-2022-a68826f509f64a33a210b40eadd26860","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Python","","","","","","","","",""
"WD3SHIDC","preprint","2021","Yasunaga, Michihiro; Ren, Hongyu; Bosselut, Antoine; Liang, Percy; Leskovec, Jure","QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering","","","","10.48550/arXiv.2104.06378","http://arxiv.org/abs/2104.06378","The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. In this work, we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph neural networks. We evaluate our model on QA benchmarks in the commonsense (CommonsenseQA, OpenBookQA) and biomedical (MedQA-USMLE) domains. QA-GNN outperforms existing LM and LM+KG models, and exhibits capabilities to perform interpretable and structured reasoning, e.g., correctly handling negation in questions.","2021-11-05","2022-08-05 05:43:17","2023-05-23 01:23:03","2022-08-05 05:43:17","","","","","","","QA-GNN","","","","","arXiv","","","","","","","arXiv.org","","7 citations (Semantic Scholar/arXiv) [2022-12-20] arXiv:2104.06378 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\6PHM2JL5\2104.html; ; C:\Users\ambreen.hanif\Zotero\storage\7JBZJKGY\Yasunaga et al_2021_QA-GNN.pdf","notion://www.notion.so/Yasunaga-et-al-2021-3bd0843e4b7e4e69b20d796d20137a7b","notion","","","","","","","","","","","","","","","","","","","","arXiv:2104.06378","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FJF7N2BT","journalArticle","","Roscoe, Timothy","Writing reviews for systems conferences","","","","","","","","2022-08-05 03:47:33","2023-05-23 01:22:58","","6","","","","","","","","","","","","","en","","","","","Zotero","","","","; C:\Users\ambreen.hanif\Zotero\storage\KP9THFU4\Roscoe - Writing reviews for systems conferences.pdf","notion://www.notion.so/Roscoe-n-d-f17c347849424311bc4a81c667866fd9","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X7IU53C5","webpage","","","Untitled spreadsheet","Google Docs","","","","https://docs.google.com/spreadsheets/u/2/d/1qLHxJBocIiMfrDSM0xBTrQJyEeqgDt7eEQp7fUnbMZk/edit?usp=embed_facebook","Sheet1","","2022-08-05 02:55:30","2023-05-23 01:22:56","2022-08-05 02:55:30","","","","","","","","","","","","","","en","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\GDEU2XFA\edit.html","notion://www.notion.so/Untitled-Spreadsheet-n-d-2a284c4a34504bc5bd12c5cdb2be26ec","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I7F37J3S","webpage","","","The explainable AI boom: Why is XAI important? And why now? - Tim Leers","","","","","https://dataroots.io/research/contributions/why-xai-and-why-now","As we alluded to in our trends post [https://dataroots.io/research/contributions/what-we-are-excited-about-for-2022-2] , the number of researchers, developers and companies that focus on eXplainable AI (XAI) is growing faster each year. 💡XAI [https://dataroots.io/research/contributions/machine-learning-explainability-an-introduction-to-cracking-open-the-black-box]  is an umbrella term for methods, algorithms and tools that increase insight into the inner workings of AI. This is in contrast wit","","2022-08-05 02:06:02","2023-05-23 01:22:54","2022-08-05 02:06:02","","","","","","","The explainable AI boom","","","","","","","en","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\YH26HB8X\why-xai-and-why-now.html","notion://www.notion.so/The-Explainable-AI-Boom-n-d-66cceb7c8483461181b37f04212afde9","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DWKGD74V","webpage","","","Introducing AI Explainability 360 | IBM Research Blog","","","","","https://www.ibm.com/blogs/research/2019/08/ai-explainability-360/","","","2022-08-05 01:56:10","2023-05-23 01:22:52","2022-08-05 01:56:10","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\SIQBZZEM\ai-explainability-360.html; ","notion://www.notion.so/Introducing-AI-Explainability-360-IBM-Research-Blog-n-d-926a3ffd8b1d42c197e3baefc76a63a2","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T9WJ5XY2","blogPost","2020","Zimolag, Agnieszka","Customizing Explainable AI (XAI) and adding trust-as-a-service to your product design","Medium","","","","https://agnieszkazimolag.medium.com/customizing-explainable-ai-xai-and-adding-trust-as-a-service-to-your-product-design-d9e3a0ffef94","Let your black box predictions interactive and part of customer experience","2020-03-25","2022-08-05 01:28:10","2023-05-23 01:22:50","2022-08-05 01:28:10","","","","","","","","","","","","","","en","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\DQ3TFNBR\customizing-explainable-ai-xai-and-adding-trust-as-a-service-to-your-product-design-d9e3a0ffef9.html","notion://www.notion.so/Zimolag-2020-5881f086a9db4e5ca232e480e9b82740","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NIHG4XAG","preprint","2016","Beheshti, Seyed-Mehdi-Reza; Tabebordbar, Alireza; Benatallah, Boualem; Nouri, Reza","Data Curation APIs","","","","10.48550/arXiv.1612.03277","http://arxiv.org/abs/1612.03277","Understanding and analyzing big data is firmly recognized as a powerful and strategic priority. For deeper interpretation of and better intelligence with big data, it is important to transform raw data (unstructured, semi-structured and structured data sources, e.g., text, video, image data sets) into curated data: contextualized data and knowledge that is maintained and made available for use by end-users and applications. In particular, data curation acts as the glue between raw data and analytics, providing an abstraction layer that relieves users from time consuming, tedious and error prone curation tasks. In this context, the data curation process becomes a vital analytics asset for increasing added value and insights. In this paper, we identify and implement a set of curation APIs and make them available (on GitHub) to researchers and developers to assist them transforming their raw data into curated data. The curation APIs enable developers to easily add features - such as extracting keyword, part of speech, and named entities such as Persons, Locations, Organizations, Companies, Products, Diseases, Drugs, etc.; providing synonyms and stems for extracted information items leveraging lexical knowledge bases for the English language such as WordNet; linking extracted entities to external knowledge bases such as Google Knowledge Graph and Wikidata; discovering similarity among the extracted information items, such as calculating similarity between string, number, date and time data; classifying, sorting and categorizing data into various types, forms or any other distinct class; and indexing structured and unstructured data - into their applications.","2016-12-10","2022-08-05 01:21:08","2023-05-23 01:22:48","2022-08-05 01:21:08","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","6 citations (Semantic Scholar/arXiv) [2022-12-20] arXiv:1612.03277 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\C3JEGHBX\1612.html; C:\Users\ambreen.hanif\Zotero\storage\2VN22573\Beheshti et al_2016_Data Curation APIs.pdf; ","notion://www.notion.so/Beheshti-et-al-2016-6f6323478ea247e5a5a375fb32d5969a","notion","","","","","","","","","","","","","","","","","","","","arXiv:1612.03277","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IJDTTIT6","conferencePaper","2018","Beheshti, Amin; Schiliro, Francesco; Ghodratnama, Samira; Amouzgar, Farhad; Benatallah, Boualem; Yang, Jian; Sheng, Quan Z.; Casati, Fabio; Motahari-Nezhad, Hamid Reza","iProcess: Enabling IoT Platforms in Data-Driven Knowledge-Intensive Processes","Business Process Management Forum","978-3-319-98651-7","","10.1007/978-3-319-98651-7_7","","The Internet of Things (IoT), the network of physical objects augmented with Internet-enabled computing devices to enable those objects sense the real world, has the potential to transform many industries. This includes harnessing real-time intelligence to improve risk-based decision making and supporting adaptive processes from core to edge. For example, modern police investigation processes are often extremely complex, data-driven and knowledge-intensive. In such processes, it is not sufficient to focus on data storage and data analysis; and the knowledge workers (e.g., investigators) will need to collect, understand and relate the big data (scattered across various systems) to process analysis: in order to communicate analysis findings, supporting evidences and to make decisions. In this paper, we present a scalable and extensible IoT-Enabled Process Data Analytics Pipeline (namely iProcess) to enable analysts ingest data from IoT devices, extract knowledge from this data and link them to process (execution) data. We introduce the notion of process Knowledge Lake and present novel techniques to summarize the linked IoT and process data to construct process narratives. This enables us to put the first step towards enabling storytelling with process data.","2018","2022-08-05 01:20:50","2023-05-23 01:22:46","","108-126","","","","","","iProcess","Lecture Notes in Business Information Processing","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","27 citations (Semantic Scholar/DOI) [2022-12-20] 17 citations (Crossref) [2022-12-20]","","","notion://www.notion.so/Beheshti-et-al-2018-3ba4f455ac5f4adfac3bf7694e41b799","notion","","Weske, Mathias; Montali, Marco; Weber, Ingo; vom Brocke, Jan","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N7XG4BPL","journalArticle","2018","Beheshti, Amin; Benatallah, Boualem; Nouri, Reza; Tabebordbar, Alireza","CoreKG: a knowledge lake service","Proceedings of the VLDB Endowment","","2150-8097","10.14778/3229863.3236230","https://dl.acm.org/doi/10.14778/3229863.3236230","With Data Science continuing to emerge as a powerful diﬀerentiator across industries, organisations are now focused on transforming their data into actionable insights. This task is challenging as in today’s knowledge-, service-, and cloudbased economy, businesses accumulate massive amounts of raw data from a variety of sources. Data Lakes introduced as a storage repository to organize this raw data in its native format (supporting from relational to NoSQL DBs) until it is needed. The rationale behind a Data Lake is to store raw data and let the data analyst decide how to cook/curate them later. In this paper, we present the notion of Knowledge Lake, i.e. a contextualized Data Lake. The Knowledge Lake will provide the foundation for big data analytics by automatically curating the raw data in the Data Lake and to prepare them for deriving insights. We present CoreKG -an open source Data and Knowledge Lake service- which oﬀers researchers and developers a single REST API to organize, curate, index and query their data and metadata in the Lake and over time. CoreKG manages multiple database technologies (from Relational to NoSQL) and oﬀers a builtin design for data curation, security and provenance.","2018-08","2022-08-05 01:20:07","2023-05-23 01:22:43","2022-08-05 01:20:07","1942-1945","","12","11","","Proc. VLDB Endow.","CoreKG","","","","","","","en","","","","","DOI.org (Crossref)","","5 citations (Semantic Scholar/DOI) [2022-12-20] 35 citations (Crossref) [2022-12-20]","","C:\Users\ambreen.hanif\Zotero\storage\RR9PAGAU\Beheshti et al. - 2018 - CoreKG a knowledge lake service.pdf; ","notion://www.notion.so/Beheshti-et-al-2018-3b9e3fe4393f4da0b5b2a9e682b0c75a","notion; Knowledge Lake; Service","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MQH4C7PC","conferencePaper","2020","Beheshti, Amin; Tabebordbar, Alireza; Benatallah, Boualem","iStory: Intelligent Storytelling with Social Data","Companion Proceedings of the Web Conference 2020","978-1-4503-7024-0","","10.1145/3366424.3383553","https://dl.acm.org/doi/10.1145/3366424.3383553","The production of knowledge from ever increasing amount of social data is seen by many organizations as an increasingly important capability that can complement the traditional analytics sources. Examples include extracting knowledge and deriving insights from social data to improve government services, predict intelligence activities, personalize the advertisements in elections and improve national security and public health. Understanding social data can be challenging as the analysis goal can be subjective. In this context, storytelling is considered as an appropriate metaphor as it facilitates understanding and surfacing insights which is embedded within the data. In this paper, we focus on the research problem of ‘understanding the social data’ in general and more particularly the curation, summarization and presentation of large amounts of social data. The goal is to enable intelligent narrative construction based on the important features (extracted and ranked automatically) and enable storytelling at multiple levels and from different views using novel summarization techniques. We implement an interactive storytelling dashboard, namely iStory, and focus on a motivating scenario for analyzing Urban Social Issues from Twitter as it relates to the Australian Government Budget, to highlight how storytelling can significantly facilitate understanding social data.","2020-04-20","2022-08-05 01:19:57","2023-05-23 01:22:35","2022-08-05 01:19:57","253-256","","","","","","iStory","","","","","ACM","Taipei Taiwan","en","","","","","DOI.org (Crossref)","","18 citations (Semantic Scholar/DOI) [2022-12-20] 8 citations (Crossref) [2022-12-20]","","C:\Users\ambreen.hanif\Zotero\storage\YLWIMKGK\Beheshti et al. - 2020 - iStory Intelligent Storytelling with Social Data.pdf; ","notion://www.notion.so/Beheshti-et-al-2020-6a07cc079de54746bb869ef53e116c6c","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","WWW '20: The Web Conference 2020","","","","","","","","","","","","","","",""
"HM3GD8NQ","journalArticle","2018","Zaeri-Amirani, Mohammad; Afghah, Fatemeh; Mousavi, Sajad","A Feature Selection Method Based on Shapley Value to False Alarm Reduction in ICUs A Genetic-Algorithm Approach","Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference","","2694-0604","10.1109/EMBC.2018.8512266","","High false alarm rate in intensive care units (ICUs) has been identified as one of the most critical medical challenges in recent years. This often results in overwhelming the clinical staff by numerous false or unurgent alarms and decreasing the quality of care through enhancing the probability of missing true alarms as well as causing delirium, stress, sleep deprivation and depressed immune systems for patients. One major cause of false alarms in clinical practice is that the collected signals from different devices are processed individually to trigger an alarm, while there exists a considerable chance that the signal collected from one device is corrupted by noise or motion artifacts. In this paper, we propose a low-computational complexity yet accurate game-theoretic feature selection method which is based on a genetic algorithm that identifies the most informative biomarkers across the signals collected from various monitoring devices and can considerably reduce the rate of false alarms 1.","2018-07","2023-04-21 05:39:25","2023-05-23 01:32:51","","319-323","","","2018","","Annu Int Conf IEEE Eng Med Biol Soc","","","","","","","","eng","","","","","PubMed","","11 citations (Crossref) [2023-04-21] PMID: 30440402","","; ; C:\Users\ambreen.hanif\Zotero\storage\CWP89AJ5\Zaeri-Amirani et al_2018_A Feature Selection Method Based on Shapley Value to False Alarm Reduction in.pdf","notion://www.notion.so/Zaeri-Amirani-et-al-2018-2cfb510a91f946be93d2063803ecc5ce; http://www.ncbi.nlm.nih.gov/pubmed/30440402","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XWWYEP5L","journalArticle","2021","Horvatić, Davor; Lipic, Tomislav","Human-Centric AI: The Symbiosis of Human and Artificial Intelligence","Entropy","","1099-4300","10.3390/e23030332","https://www.mdpi.com/1099-4300/23/3/332","Well-evidenced advances of data-driven complex machine learning approaches emerging within the so-called second wave of artificial intelligence (AI) fostered the exploration of possible AI applications in various domains and aspects of human life, practices, and society [...]","2021-03","2023-04-21 05:37:57","2023-05-23 01:32:50","2023-04-21 05:37:57","332","","3","23","","","Human-Centric AI","","","","","","","en","http://creativecommons.org/licenses/by/3.0/","","","","www.mdpi.com","","5 citations (Crossref) [2023-04-21] Number: 3 Publisher: Multidisciplinary Digital Publishing Institute","","C:\Users\ambreen.hanif\Zotero\storage\I8KYIAYB\Horvatić_Lipic_2021_Human-Centric AI.pdf; ","notion://www.notion.so/Horvati-Lipic-2021-c1a548e3500f4526848f1e8da6800381","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GR8BVNGA","conferencePaper","2022","Cugny, Robin; Aligon, Julien; Chevalier, Max; Jimenez, Geoffrey Roman; Teste, Olivier","Why Should I Choose You? AutoXAI: A Framework for Selecting and Tuning eXplainable AI Solutions","Proceedings of the 31st ACM International Conference on Information & Knowledge Management","","","10.1145/3511808.3557247","http://arxiv.org/abs/2210.02795","In recent years, a large number of XAI (eXplainable Artificial Intelligence) solutions have been proposed to explain existing ML (Machine Learning) models or to create interpretable ML models. Evaluation measures have recently been proposed and it is now possible to compare these XAI solutions. However, selecting the most relevant XAI solution among all this diversity is still a tedious task, especially when meeting specific needs and constraints. In this paper, we propose AutoXAI, a framework that recommends the best XAI solution and its hyperparameters according to specific XAI evaluation metrics while considering the user's context (dataset, ML model, XAI needs and constraints). It adapts approaches from context-aware recommender systems and strategies of optimization and evaluation from AutoML (Automated Machine Learning). We apply AutoXAI to two use cases, and show that it recommends XAI solutions adapted to the user's needs with the best hyperparameters matching the user's constraints.","2022-10-17","2023-04-21 05:37:12","2023-05-23 01:32:49","2023-04-21 05:37:12","315-324","","","","","","Why Should I Choose You?","","","","","","","","","","","","arXiv.org","","0 citations (Crossref) [2023-04-21] arXiv:2210.02795 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\KCW3ZBWJ\2210.html; C:\Users\ambreen.hanif\Zotero\storage\5V64GD76\Cugny et al_2022_Why Should I Choose You.pdf; ","notion://www.notion.so/Cugny-et-al-2022-eda5cadd370d4fe28094647d153cef71","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I2DEH35Z","bookSection","2022","Manresa-Yee, Cristina; Roig-Maimó, Maria Francesca; Ramis, Silvia; Mas-Sansó, Ramon","Advances in XAI: Explanation Interfaces in Healthcare","Handbook of Artificial  Intelligence in Healthcare: Vol 2: Practicalities and Prospects","978-3-030-83620-7","","","https://doi.org/10.1007/978-3-030-83620-7_15","Artificial Intelligence based algorithms are gaining a main role in healthcare. However, the black-box nature of models such as deep neural networks challenges the users’ trust. Explainable Artificial Intelligence (XAI) strives for more transparent and interpretable AI, achieving intelligent systems that help the user understand the AI predictions and decisions increasing the trustfulness and reliability of the systems. In this work, we present an overview of contexts in healthcare where explanation interfaces are used. We conduct a search in main research databases and compile works related to healthcare to show the widespread applicability of the intelligent systems and how researchers offer explanations in form of natural text, parameters influence, visualizations of data graphs or saliency maps.","2022","2023-04-21 05:35:54","2023-05-23 01:32:47","2023-04-21 05:35:54","357-369","","","","","","Advances in XAI","Intelligent Systems Reference Library","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","DOI: 10.1007/978-3-030-83620-7_15","","C:\Users\ambreen.hanif\Zotero\storage\AHPPUQDX\Manresa-Yee et al_2022_Advances in XAI.pdf; ","notion://www.notion.so/Manresa-Yee-et-al-2022-51e8a202c93d41a59f08d330409bf11a","notion","","Lim, Chee-Peng; Chen, Yen-Wei; Vaidya, Ashlesha; Mahorkar, Charu; Jain, Lakhmi C.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TR8Z8N32","preprint","2020","Mohseni, Sina; Zarei, Niloofar; Ragan, Eric D.","A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems","","","","","http://arxiv.org/abs/1811.11839","The need for interpretable and accountable intelligent systems grows along with the prevalence of artificial intelligence applications used in everyday life. Explainable intelligent systems are designed to self-explain the reasoning behind system decisions and predictions, and researchers from different disciplines work together to define, design, and evaluate interpretable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of interpretable machine learning research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this paper presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of interpretable machine learning design goals and evaluation methods to show a mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.","2020-08-05","2023-04-21 05:32:19","2023-05-23 01:32:43","2023-04-21 05:32:19","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1811.11839 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\N8BPJGTJ\1811.html; C:\Users\ambreen.hanif\Zotero\storage\II9RD5UA\Mohseni et al_2020_A Multidisciplinary Survey and Framework for Design and Evaluation of.pdf; ","notion://www.notion.so/Mohseni-et-al-2020-7e416a834eb34d2d95c0ec4e791b1021","notion","","","","","","","","","","","","","","","","","","","","arXiv:1811.11839","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KCIIUMMH","conferencePaper","2022","Wang, Pingfan; Woo, Wailok; Jin, Nanlin; Davies, Duncan","Concept Drift Detection by Tracking Weighted Prediction Confidence of Incremental Learning","2022 4th International Conference on Image, Video and Signal Processing","978-1-4503-8741-5","","10.1145/3531232.3531264","https://dl.acm.org/doi/10.1145/3531232.3531264","Data stream mining is great significant in many real-world scenarios, especially in the big data area. However, conventional machine learning algorithms are incapable to process because of its two characteristics (1) potential unlimited number of data is generated in real-time way, it is impossible to store all the data (2) evolving over time, namely, concept drift, will influence the performance of predictor trained on previous data. Concept drift detection method could detect and locate the concept drift in data stream. However, existing methods only utilize the prediction result as indicator. In this article, we propose a weighted concept drift indicator based on incremental ensemble learning to detect the concept. The indicator not only considers the prediction result, but the change of prediction stability of predictor with occurs of concept drift. Also, an incremental ensemble learning based on vote mechanism is especially used to get constantly updated value of indicator. Based on the experiment result on both benchmark and real-world dataset, our method could effectively detect concept drift and outperform other existing methods.","2022-06-01","2023-04-19 23:40:13","2023-05-23 01:32:42","2023-04-19","218–223","","","","","","","IVSP 2022","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","1 citations (Crossref) [2023-04-20]","","; C:\Users\ambreen.hanif\Zotero\storage\JNWFUL5K\Wang et al_2022_Concept Drift Detection by Tracking Weighted Prediction Confidence of.pdf","notion://www.notion.so/Wang-et-al-2022-2474a6b551a64ab0a7cf93d81d23ef4a","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CZFAE5VL","conferencePaper","2022","Hinder, Fabian; Vaquet, Valerie; Brinkrolf, Johannes; Artelt, André; Hammer, Barbara","Localization of Concept Drift: Identifying the Drifting Datapoints","2022 International Joint Conference on Neural Networks (IJCNN)","","","10.1109/IJCNN55064.2022.9892374","","The notion of concept drift refers to the phenomenon that the distribution which is underlying the observed data changes over time. As a consequence machine learning models may become inaccurate and need adjustment. While there do exist methods to detect concept drift, to find change points in data streams, or to adjust models in the presence of observed drift, the problem of localizing drift, i.e. identifying it in data space, is yet widely unsolved - in particular from a formal perspective. This problem however is of importance, since it enables an inspection of the most prominent characteristics, e.g. features, where drift manifests itself and can therefore be used to make informed decisions, e.g. efficient updates of the training set of online learning algorithms, and perform precise adjustments of the learning model. In this paper we present a general theoretical framework that reduces drift localization to a supervised machine learning problem. We construct a new method for drift localization thereon and demonstrate the usefulness of our theory and the performance of our algorithm by comparing it to other methods from the literature.","2022-07","2023-04-19 23:40:04","2023-05-23 01:32:40","","1-9","","","","","","Localization of Concept Drift","","","","","","","","","","","","IEEE Xplore","","0 citations (Crossref) [2023-04-20] ISSN: 2161-4407","","C:\Users\ambreen.hanif\Zotero\storage\IWX4C8YJ\Hinder et al_2022_Localization of Concept Drift.pdf; C:\Users\ambreen.hanif\Zotero\storage\PYCUMRYI\stamp.html; ","notion://www.notion.so/Hinder-et-al-2022-428348a9e619462d8ae406f259af455a","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2022 International Joint Conference on Neural Networks (IJCNN)","","","","","","","","","","","","","","",""
"JGXZNRTT","webpage","","","Concept Drift Detection by Tracking Weighted Prediction Confidence of Incremental Learning | 2022 4th International Conference on Image, Video and Signal Processing","","","","","https://dl-acm-org.simsrad.net.ocs.mq.edu.au/doi/10.1145/3531232.3531264","","","2023-04-19 23:39:29","2023-05-23 01:32:37","2023-04-19 23:39:29","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\TQC82AGS\3531232.html; ","notion://www.notion.so/Concept-Drift-Detection-by-Tracking-Weighted-Prediction-Confidence-of-Incremental-Learning-2022-4t-e73102d4fdc6437b95323836d581b942","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ELNWWZP4","preprint","2023","Zhengxin, Fang; Yi, Yuan; Jingyu, Zhang; Yue, Liu; Yuechen, Mu; Qinghua, Lu; Xiwei, Xu; Jeff, Wang; Chen, Wang; Shuai, Zhang; Shiping, Chen","MLOps Spanning Whole Machine Learning Life Cycle: A Survey","","","","10.48550/arXiv.2304.07296","http://arxiv.org/abs/2304.07296","Google AlphaGos win has significantly motivated and sped up machine learning (ML) research and development, which led to tremendous ML technical advances and wider adoptions in various domains (e.g., Finance, Health, Defense, and Education). These advances have resulted in numerous new concepts and technologies, which are too many for people to catch up to and even make them confused, especially for newcomers to the ML area. This paper is aimed to present a clear picture of the state-of-the-art of the existing ML technologies with a comprehensive survey. We lay out this survey by viewing ML as a MLOps (ML Operations) process, where the key concepts and activities are collected and elaborated with representative works and surveys. We hope that this paper can serve as a quick reference manual (a survey of surveys) for newcomers (e.g., researchers, practitioners) of ML to get an overview of the MLOps process, as well as a good understanding of the key technologies used in each step of the ML process, and know where to find more details.","2023-04-13","2023-04-19 23:23:56","2023-05-23 01:32:34","2023-04-19 23:23:56","","","","","","","MLOps Spanning Whole Machine Learning Life Cycle","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2304.07296 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\X3JC3ZEF\2304.html; ; C:\Users\ambreen.hanif\Zotero\storage\AK4FW5YR\Zhengxin et al_2023_MLOps Spanning Whole Machine Learning Life Cycle.pdf","notion://www.notion.so/Zhengxin-et-al-2023-81230059241343ca9328fc87dac17a18","notion","","","","","","","","","","","","","","","","","","","","arXiv:2304.07296","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4SKW6KIB","journalArticle","2020","Pinagé, Felipe; dos Santos, Eulanda M.; Gama, João","A drift detection method based on dynamic classifier selection","Data Mining and Knowledge Discovery","","1573-756X","10.1007/s10618-019-00656-w","https://doi.org/10.1007/s10618-019-00656-w","Machine learning algorithms can be applied to several practical problems, such as spam, fraud and intrusion detection, and customer preferences, among others. In most of these problems, data come in streams, which mean that data distribution may change over time, leading to concept drift. The literature is abundant on providing supervised methods based on error monitoring for explicit drift detection. However, these methods may become infeasible in some real-world applications—where there is no fully labeled data available, and may depend on a significant decrease in accuracy to be able to detect drifts. There are also methods based on blind approaches, where the decision model is updated constantly. However, this may lead to unnecessary system updates. In order to overcome these drawbacks, we propose in this paper a semi-supervised drift detector that uses an ensemble of classifiers based on self-training online learning and dynamic classifier selection. For each unknown sample, a dynamic selection strategy is used to choose among the ensemble’s component members, the classifier most likely to be the correct one for classifying it. The prediction assigned by the chosen classifier is used to compute an estimate of the error produced by the ensemble members. The proposed method monitors such a pseudo-error in order to detect drifts and to update the decision model only after drift detection. The achievement of this method is relevant in that it allows drift detection and reaction and is applicable in several practical problems. The experiments conducted indicate that the proposed method attains high performance and detection rates, while reducing the amount of labeled data used to detect drift.","2020-01-01","2023-04-19 23:20:16","2023-05-23 01:32:31","2023-04-19 23:20:16","50-74","","1","34","","Data Min Knowl Disc","","","","","","","","en","","","","","Springer Link","","20 citations (Crossref) [2023-04-20]","","; C:\Users\ambreen.hanif\Zotero\storage\6772BZMA\Pinagé et al_2020_A drift detection method based on dynamic classifier selection.pdf","notion://www.notion.so/Pinag-et-al-2020-abd1c8445f674354805377ba93aebd7c","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R6NVH6VC","preprint","2022","Fumagalli, Fabian; Muschalik, Maximilian; Hüllermeier, Eyke; Hammer, Barbara","Incremental Permutation Feature Importance (iPFI): Towards Online Explanations on Data Streams","","","","10.48550/arXiv.2209.01939","http://arxiv.org/abs/2209.01939","Explainable Artificial Intelligence (XAI) has mainly focused on static learning scenarios so far. We are interested in dynamic scenarios where data is sampled progressively, and learning is done in an incremental rather than a batch mode. We seek efficient incremental algorithms for computing feature importance (FI) measures, specifically, an incremental FI measure based on feature marginalization of absent features similar to permutation feature importance (PFI). We propose an efficient, model-agnostic algorithm called iPFI to estimate this measure incrementally and under dynamic modeling conditions including concept drift. We prove theoretical guarantees on the approximation quality in terms of expectation and variance. To validate our theoretical findings and the efficacy of our approaches compared to traditional batch PFI, we conduct multiple experimental studies on benchmark data with and without concept drift.","2022-09-07","2023-04-19 05:50:20","2023-05-23 01:32:27","2023-04-19 05:50:20","","","","","","","Incremental Permutation Feature Importance (iPFI)","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2209.01939 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\SGRSWDKB\2209.html; C:\Users\ambreen.hanif\Zotero\storage\TMVSHZJF\Fumagalli et al_2022_Incremental Permutation Feature Importance (iPFI).pdf; ","notion://www.notion.so/Fumagalli-et-al-2022-686930554f774cb9a645974cf1e825db","notion","","","","","","","","","","","","","","","","","","","","arXiv:2209.01939","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7PTSJXN6","conferencePaper","2019","Jaigirdar, Fariha Tasmin; Rudolph, Carsten; Bain, Chris","Can I Trust the Data I See? A Physician's Concern on Medical Data in IoT Health Architectures","Proceedings of the Australasian Computer Science Week Multiconference","978-1-4503-6603-8","","10.1145/3290688.3290731","https://dl.acm.org/doi/10.1145/3290688.3290731","With the increasing advancement of Internet of Things (IoT) enabled systems, smart medical devices open numerous opportunities for the healthcare sector. The success of using such devices in the healthcare industry depends strongly on secured and reliable medical data transmission. Physicians diagnose that data and prescribe medicines and/or give guidelines/instructions/treatment plans for the patients. Therefore, a physician is always concerned about the medical data trustworthiness, because if it is not guaranteed, a savior can become an involuntary foe! This paper analyses two different scenarios to understand the real-life consequences in IoT-based healthcare (IoT-Health) application. Appropriate sequence diagrams for both scenarios show data movement as a basis for determining necessary security requirements in each layer of IoT-Health. We analyse the individual entities of the overall system and develop a system-wide view of trust in IoT-Health. The security analysis pinpoints the research gap in end-to-end trust and indicates the necessity to treat the whole IoT-Health system as an integrated entity. This study highlights the importance of integrated cross-layer security solutions that can deal with the heterogeneous security architectures of IoT healthcare system and finally identifies a possible solution for the open question raised in the security analysis with appropriate future research directions.","2019-01-29","2023-04-19 05:04:12","2023-05-23 01:32:25","2023-04-18","1–10","","","","","","Can I Trust the Data I See?","ACSW '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","24 citations (Crossref) [2023-04-19]","","C:\Users\ambreen.hanif\Zotero\storage\UAHP6GYA\Jaigirdar et al_2019_Can I Trust the Data I See.pdf; ","notion://www.notion.so/Jaigirdar-et-al-2019-746d69e74cec4ae0ab373887b67f38d9","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YVBJIHZC","conferencePaper","2019","Jentzsch, Sophie F.; Hochgeschwender, Nico","Don't Forget Your Roots! Using Provenance Data for Transparent and Explainable Development of Machine Learning Models","2019 34th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW)","","","10.1109/ASEW.2019.00025","","Explaining reasoning and behaviour of artificial intelligent systems to human users becomes increasingly urgent, especially in the field of machine learning. Many recent contributions approach this issue with post-hoc methods, meaning they consider the final system and its outcomes, while the roots of included artefacts are widely neglected. However, we argue in this position paper that there needs to be a stronger focus on the development process. Without insights into specific design decisions and meta information that accrue during the development an accurate explanation of the resulting model is hardly possible. To remedy this situation we propose to increase process transparency by applying provenance methods, which serves also as a basis for increased explainability.","2019-11","2023-04-19 02:09:07","2023-05-23 01:32:22","","37-40","","","","","","","","","","","","","","","","","","IEEE Xplore","","ISSN: 2151-0830","","C:\Users\ambreen.hanif\Zotero\storage\59QCYYKW\8967411.html; C:\Users\ambreen.hanif\Zotero\storage\4VG69LNM\Jentzsch_Hochgeschwender_2019_Don't Forget Your Roots.pdf; ","notion://www.notion.so/Jentzsch-Hochgeschwender-2019-a20811d22629422490a27084b1fe94bc","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 34th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW)","","","","","","","","","","","","","","",""
"3P2T858Z","preprint","2017","Liu, Shixia; Wang, Xiting; Liu, Mengchen; Zhu, Jun","Towards Better Analysis of Machine Learning Models: A Visual Analytics Perspective","","","","10.48550/arXiv.1702.01226","http://arxiv.org/abs/1702.01226","Interactive model analysis, the process of understanding, diagnosing, and refining a machine learning model with the help of interactive visualization, is very important for users to efficiently solve real-world artificial intelligence and data mining problems. Dramatic advances in big data analytics has led to a wide variety of interactive model analysis tasks. In this paper, we present a comprehensive analysis and interpretation of this rapidly developing area. Specifically, we classify the relevant work into three categories: understanding, diagnosis, and refinement. Each category is exemplified by recent influential work. Possible future research opportunities are also explored and discussed.","2017-02-03","2023-04-19 02:09:02","2023-05-23 01:32:20","2023-04-19 02:09:02","","","","","","","Towards Better Analysis of Machine Learning Models","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1702.01226 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\6SGUZKIM\1702.html; C:\Users\ambreen.hanif\Zotero\storage\KKFBYR4C\Liu et al_2017_Towards Better Analysis of Machine Learning Models.pdf; ","notion://www.notion.so/Liu-et-al-2017-551ae6eb4f9744d2ad1f283ccb22b8cc","notion","","","","","","","","","","","","","","","","","","","","arXiv:1702.01226","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3JK6G4KW","book","2020","Jaigirdar, Fariha; Rudolph, Carsten; Oliver, Gillian; Watts, David; Bain, Chris","What Information is Required for Explainable AI? : A Provenance-based Research Agenda and Future Challenges","","","","","","Deriving explanations of an Artificial Intelligence-based system's decision making is becoming increasingly essential to address requirements that meet quality standards and operate in a transparent, comprehensive, understandable, and explainable manner. Furthermore, more security issues as well as concerns from human perspectives emerge in describing the explainability properties of AI. A full system view is required to enable humans to properly estimate risks when dealing with such systems. This paper introduces open issues in this research area to present the overall picture of explainability and the required information needed for the explanation to make a decision-oriented AI system transparent to humans. It illustrates the potential contribution of proper provenance data to AI-based systems by describing a provenance graph-based design. This paper proposes a six-Ws framework to demonstrate how a security-aware provenance graph-based design can build the basis for providing end-users with sufficient meta-information on AI-based decision systems. An example scenario is then presented that highlights the required information for better explainability both from human and security-aware aspects. Finally, associated challenges are discussed to provoke further research and commentary.","2020-12-01","2023-04-19 02:07:57","2023-05-23 01:32:18","","","177","","","","","What Information is Required for Explainable AI?","","","","","","","","","","","","ResearchGate","","Pages: 183 DOI: 10.1109/CIC50333.2020.00030","","C:\Users\ambreen.hanif\Zotero\storage\HJ75GHDX\Jaigirdar et al_2020_What Information is Required for Explainable AI.pdf; ; ","notion://www.notion.so/Jaigirdar-et-al-2020-f5d75d01de614decaf2d790ac33db0b7; https://www.researchgate.net/publication/348637265_What_Information_is_Required_for_Explainable_AI_A_Provenance-based_Research_Agenda_and_Future_Challenges","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TJ5SU8UZ","preprint","2019","Arya, Vijay; Bellamy, Rachel K. E.; Chen, Pin-Yu; Dhurandhar, Amit; Hind, Michael; Hoffman, Samuel C.; Houde, Stephanie; Liao, Q. Vera; Luss, Ronny; Mojsilović, Aleksandra; Mourad, Sami; Pedemonte, Pablo; Raghavendra, Ramya; Richards, John; Sattigeri, Prasanna; Shanmugam, Karthikeyan; Singh, Moninder; Varshney, Kush R.; Wei, Dennis; Zhang, Yunfeng","One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques","","","","","http://arxiv.org/abs/1909.03012","As artificial intelligence and machine learning algorithms make further inroads into society, calls are increasing from multiple stakeholders for these algorithms to explain their outputs. At the same time, these stakeholders, whether they be affected citizens, government regulators, domain experts, or system developers, present different requirements for explanations. Toward addressing these needs, we introduce AI Explainability 360 (http://aix360.mybluemix.net/), an open-source software toolkit featuring eight diverse and state-of-the-art explainability methods and two evaluation metrics. Equally important, we provide a taxonomy to help entities requiring explanations to navigate the space of explanation methods, not only those in the toolkit but also in the broader literature on explainability. For data scientists and other users of the toolkit, we have implemented an extensible software architecture that organizes methods according to their place in the AI modeling pipeline. We also discuss enhancements to bring research innovations closer to consumers of explanations, ranging from simplified, more accessible versions of algorithms, to tutorials and an interactive web demo to introduce AI explainability to different audiences and application domains. Together, our toolkit and taxonomy can help identify gaps where more explainability methods are needed and provide a platform to incorporate them as they are developed.","2019-09-14","2023-04-17 23:41:40","2023-05-23 01:32:13","2023-04-17 23:41:40","","","","","","","One Explanation Does Not Fit All","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1909.03012 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\EAFRWRF2\1909.html; C:\Users\ambreen.hanif\Zotero\storage\HJIEZTIN\Arya et al_2019_One Explanation Does Not Fit All.pdf; ","notion://www.notion.so/Arya-et-al-2019-406bed44c0294424bd52e3d56675db32","notion","","","","","","","","","","","","","","","","","","","","arXiv:1909.03012","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9XHUWCJ2","conferencePaper","2022","Speith, Timo","A Review of Taxonomies of Explainable Artificial Intelligence (XAI) Methods","2022 ACM Conference on Fairness, Accountability, and Transparency","978-1-4503-9352-2","","10.1145/3531146.3534639","https://dl.acm.org/doi/10.1145/3531146.3534639","","2022-06-21","2023-04-17 23:38:37","2023-05-23 01:32:12","2023-04-17 23:38:37","2239-2250","","","","","","","","","","","ACM","Seoul Republic of Korea","en","","","","","DOI.org (Crossref)","","14 citations (Crossref) [2023-04-18]","","; C:\Users\ambreen.hanif\Zotero\storage\QVKLWW6P\Speith_2022_A Review of Taxonomies of Explainable Artificial Intelligence (XAI) Methods.pdf","notion://www.notion.so/Speith-2022-695d7baf55db41058b427f4276db6836","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","FAccT '22: 2022 ACM Conference on Fairness, Accountability, and Transparency","","","","","","","","","","","","","","",""
"TIX25G44","preprint","2019","Frankle, Jonathan; Carbin, Michael","The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks","","","","10.48550/arXiv.1803.03635","http://arxiv.org/abs/1803.03635","Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the ""lottery ticket hypothesis:"" dense, randomly-initialized, feed-forward networks contain subnetworks (""winning tickets"") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.","2019-03-04","2023-04-16 22:22:43","2023-05-23 01:32:04","2023-04-16 22:22:43","","","","","","","The Lottery Ticket Hypothesis","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1803.03635 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\LM2ZVXSC\1803.html; C:\Users\ambreen.hanif\Zotero\storage\9NQDVR75\Frankle_Carbin_2019_The Lottery Ticket Hypothesis.pdf; ","notion://www.notion.so/Frankle-Carbin-2019-41a4a37ad5a14f3689d9ed9da22a9913","notion","","","","","","","","","","","","","","","","","","","","arXiv:1803.03635","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BEVIDL6P","preprint","2018","Zhang, Quanshi; Zhu, Song-Chun","Visual Interpretability for Deep Learning: a Survey","","","","10.48550/arXiv.1802.00614","http://arxiv.org/abs/1802.00614","This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, the interpretability is always the Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of low interpretability of their black-box representations. We believe that high model interpretability may help people to break several bottlenecks of deep learning, e.g., learning from very few annotations, learning via human-computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and we revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.","2018-02-07","2023-04-16 22:15:28","2023-05-23 01:32:02","2023-04-16 22:15:28","","","","","","","Visual Interpretability for Deep Learning","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1802.00614 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\ZT5MLRX2\1802.html; ; C:\Users\ambreen.hanif\Zotero\storage\MX2Z6UVC\Zhang_Zhu_2018_Visual Interpretability for Deep Learning.pdf","notion://www.notion.so/Zhang-Zhu-2018-dfd93c77f84545d986e2b233871fc16a","notion","","","","","","","","","","","","","","","","","","","","arXiv:1802.00614","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FT7ZSW5W","preprint","2015","Zhou, Bolei; Khosla, Aditya; Lapedriza, Agata; Oliva, Aude; Torralba, Antonio","Object Detectors Emerge in Deep Scene CNNs","","","","10.48550/arXiv.1412.6856","http://arxiv.org/abs/1412.6856","With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classification. As scenes are composed of objects, the CNN for scene classification automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.","2015-04-15","2023-04-16 22:04:16","2023-05-23 01:32:00","2023-04-16 22:04:16","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1412.6856 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\CDQ7DGIJ\1412.html; ; C:\Users\ambreen.hanif\Zotero\storage\4UY9RASZ\Zhou et al_2015_Object Detectors Emerge in Deep Scene CNNs.pdf","notion://www.notion.so/Zhou-et-al-2015-8c45c53bc49d43569819ae999af9543f","notion","","","","","","","","","","","","","","","","","","","","arXiv:1412.6856","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TGVJAIUJ","webpage","","","[1412.6856] Object Detectors Emerge in Deep Scene CNNs","","","","","https://arxiv.org/abs/1412.6856","","","2023-04-16 22:04:00","2023-05-23 01:31:58","2023-04-16 22:04:00","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\L2Q4RNJI\1412.html; ","notion://www.notion.so/1412-6856-Object-Detectors-Emerge-in-Deep-Scene-CNNs-n-d-654839298cf646c39d99e45b48c9937a","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SETB4HHA","preprint","2020","Huang, Qiang; Yamada, Makoto; Tian, Yuan; Singh, Dinesh; Yin, Dawei; Chang, Yi","GraphLIME: Local Interpretable Model Explanations for Graph Neural Networks","","","","10.48550/arXiv.2001.06216","http://arxiv.org/abs/2001.06216","Graph structured data has wide applicability in various domains such as physics, chemistry, biology, computer vision, and social networks, to name a few. Recently, graph neural networks (GNN) were shown to be successful in effectively representing graph structured data because of their good performance and generalization ability. GNN is a deep learning based method that learns a node representation by combining specific nodes and the structural/topological information of a graph. However, like other deep models, explaining the effectiveness of GNN models is a challenging task because of the complex nonlinear transformations made over the iterations. In this paper, we propose GraphLIME, a local interpretable model explanation for graphs using the Hilbert-Schmidt Independence Criterion (HSIC) Lasso, which is a nonlinear feature selection method. GraphLIME is a generic GNN-model explanation framework that learns a nonlinear interpretable model locally in the subgraph of the node being explained. More specifically, to explain a node, we generate a nonlinear interpretable model from its $N$-hop neighborhood and then compute the K most representative features as the explanations of its prediction using HSIC Lasso. Through experiments on two real-world datasets, the explanations of GraphLIME are found to be of extraordinary degree and more descriptive in comparison to the existing explanation methods.","2020-09-27","2023-04-14 05:59:09","2023-05-23 01:31:56","2023-04-14 05:59:09","","","","","","","GraphLIME","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2001.06216 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\655TU6P8\2001.html; C:\Users\ambreen.hanif\Zotero\storage\GRAEC6VA\Huang et al_2020_GraphLIME.pdf; ","notion://www.notion.so/Huang-et-al-2020-6e56ea26cb7945f1bc6df4cb7645d6f1","notion","","","","","","","","","","","","","","","","","","","","arXiv:2001.06216","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4RTZ9FM8","journalArticle","2022","Adebayo, Julius; Muelly, Michael; Abelson, Hal; Kim, Been","POST HOC EXPLANATIONS MAY BE INEFFECTIVE FOR DETECTING UNKNOWN SPURIOUS CORRELATION","","","","","","","2022","2023-04-14 02:30:15","2023-05-23 01:31:50","","","","","","","","","","","","","","","en","","","","","Zotero","","","","; C:\Users\ambreen.hanif\Zotero\storage\GXQ7RRJX\pdf.pdf","notion://www.notion.so/Adebayo-et-al-2022-f39dad5cba2e4af8ae64d5c26d5154a2","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5LVJNN8R","journalArticle","2015","Aldeen, Yousra Abdul Alsahib S.; Salleh, Mazleena; Razzaque, Mohammad Abdur","A comprehensive review on privacy preserving data mining","SpringerPlus","","2193-1801","10.1186/s40064-015-1481-x","https://doi.org/10.1186/s40064-015-1481-x","Preservation of privacy in data mining has emerged as an absolute prerequisite for exchanging confidential information in terms of data analysis, validation, and publishing. Ever-escalating internet phishing posed severe threat on widespread propagation of sensitive information over the web. Conversely, the dubious feelings and contentions mediated unwillingness of various information providers towards the reliability protection of data from disclosure often results utter rejection in data sharing or incorrect information sharing. This article provides a panoramic overview on new perspective and systematic interpretation of a list published literatures via their meticulous organization in subcategories. The fundamental notions of the existing privacy preserving data mining methods, their merits, and shortcomings are presented. The current privacy preserving data mining techniques are classified based on distortion, association rule, hide association rule, taxonomy, clustering, associative classification, outsourced data mining, distributed, and k-anonymity, where their notable advantages and disadvantages are emphasized. This careful scrutiny reveals the past development, present research challenges, future trends, the gaps and weaknesses. Further significant enhancements for more robust privacy protection and preservation are affirmed to be mandatory.","2015-11-12","2023-04-12 03:32:28","2023-05-23 01:31:49","2023-04-12 03:32:28","694","","1","4","","SpringerPlus","","","","","","","","","","","","","BioMed Central","","62 citations (Crossref) [2023-04-12]","","C:\Users\ambreen.hanif\Zotero\storage\KNQS56AM\Aldeen et al_2015_A comprehensive review on privacy preserving data mining.pdf; ; C:\Users\ambreen.hanif\Zotero\storage\ZDWH4ML8\s40064-015-1481-x.html","notion://www.notion.so/Aldeen-et-al-2015-8c48e5bfee5d4780ab88d65fc70b0ccb","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FXXASLLX","journalArticle","2014","Romei, Andrea; Ruggieri, Salvatore","A multidisciplinary survey on discrimination analysis","The Knowledge Engineering Review","","0269-8889, 1469-8005","10.1017/S0269888913000039","https://www.cambridge.org/core/product/identifier/S0269888913000039/type/journal_article","Abstract             The collection and analysis of observational and experimental data represent the main tools for assessing the presence, the extent, the nature, and the trend of discrimination phenomena. Data analysis techniques have been proposed in the last 50 years in the economic, legal, statistical, and, recently, in the data mining literature. This is not surprising, since discrimination analysis is a multidisciplinary problem, involving sociological causes, legal argumentations, economic models, statistical techniques, and computational issues. The objective of this survey is to provide a guidance and a glue for researchers and anti-discrimination data analysts on concepts, problems, application areas, datasets, methods, and approaches from a multidisciplinary perspective. We organize the approaches according to their method of data collection as observational, quasi-experimental, and experimental studies. A fourth line of recently blooming research on knowledge discovery based methods is also covered. Observational methods are further categorized on the basis of their application context: labor economics, social profiling, consumer markets, and others.","2014-11","2023-04-12 03:29:38","2023-05-23 01:31:47","2023-04-12 03:29:38","582-638","","5","29","","The Knowledge Engineering Review","","","","","","","","en","","","","","Semantic Scholar","","123 citations (Crossref) [2023-04-12]","","; C:\Users\ambreen.hanif\Zotero\storage\Q5RGVD3F\Romei_Ruggieri_2014_A multidisciplinary survey on discrimination analysis.pdf; ","notion://www.notion.so/Romei-Ruggieri-2014-6c2620873aa7452191a2b23af21137d2; https://www.semanticscholar.org/paper/A-multidisciplinary-survey-on-discrimination-Romei-Ruggieri/b0d3968e7a630d3661cd0878acc38c85f1828a9c","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9HNBTA4L","webpage","","","A multidisciplinary survey on discrimination analysis | The Knowledge Engineering Review | Cambridge Core","","","","","https://www.cambridge.org/core/journals/knowledge-engineering-review/article/abs/multidisciplinary-survey-on-discrimination-analysis/D69E925AC96CDEC643C18A07F2A326D7","","","2023-04-12 03:27:19","2023-05-23 01:31:41","2023-04-12 03:27:19","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\PR6TKH68\D69E925AC96CDEC643C18A07F2A326D7.html; ","notion://www.notion.so/A-Multidisciplinary-Survey-on-Discrimination-Analysis-The-Knowledge-Engineering-Review-Cambridge-5699c24640684846b2273dcb02d65e96","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IAEXII9X","journalArticle","2017","Chattopadhyay, Subhagata","A neuro-fuzzy approach for the diagnosis of depression","Applied Computing and Informatics","","2210-8327","10.1016/j.aci.2014.01.001","https://www.sciencedirect.com/science/article/pii/S2210832714000027","Depression is considered to be a chronic mood disorder. This paper attempts to mathematically model how psychiatrists clinically perceive the symptoms and then diagnose depression states. According to Diagnostic and Statistical Manual (DSM)-IV-TR, fourteen symptoms of adult depression have been considered. The load of each symptom and the corresponding severity of depression are measured by the psychiatrists (i.e. the domain experts). Using the Principal Component Analysis (PCA) out of fourteen symptoms (as features) seven has been extracted as latent factors. Using these features as inputs, a hybrid system consisting of Mamdani’s Fuzzy logic controller (FLC) on a Feed Forward Multilayer Neural Net (FFMNN) has been developed. The output of the hybrid system was tuned by a back propagation (BPNN) algorithm. Finally, the model is validated using 302 real-world adult depression cases and 50 controls (i.e. normal population). The study concludes that the hybrid controller can diagnose and grade depression with an average accuracy of 95.50%. Finally, it is compared with the accuracies obtained by other techniques.","2017-01-01","2023-04-12 00:49:06","2023-05-23 01:31:35","2023-04-12 00:49:06","10-18","","1","13","","Applied Computing and Informatics","","","","","","","","en","","","","","ScienceDirect","","19 citations (Crossref) [2023-04-12]","","C:\Users\ambreen.hanif\Zotero\storage\87QFN4BD\Chattopadhyay_2017_A neuro-fuzzy approach for the diagnosis of depression.pdf; ; C:\Users\ambreen.hanif\Zotero\storage\6P5T9H26\S2210832714000027.html","notion://www.notion.so/Chattopadhyay-2017-0cf4b7a30c044454888dcc1828862511","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QFGCAHF5","conferencePaper","2018","Raju, Cincy; Philipsy, E; Chacko, Siji; Padma Suresh, L; Deepa Rajan, S","A Survey on Predicting Heart Disease using Data Mining Techniques","2018 Conference on Emerging Devices and Smart Systems (ICEDSS)","","","10.1109/ICEDSS.2018.8544333","","Heart disease is a most harmful one that will cause death. It has a serious long term disability. This disease attacks a person so instantly. Medical data is still information rich but knowledge poor. Therefore diagnosing patients correctly on the basis of time is an exigent function for medical support. An invalid diagnosis done by the hospital leads for losing reputation. The precise diagnosis of heart disease is the dominant biomedical issue. The motivation of this paper is to develop an efficacious treatment using data mining techniques that can help remedial situations. Further data mining classification algorithms like decision trees, neural networks, Bayesian classifiers, Support vector machines, Association Rule, K- nearest neighbour classification are used to diagnosis the heart diseases. Among these algorithms Support Vector Machine (SVM) gives best result.","2018-03","2023-04-11 23:52:13","2023-05-23 01:31:21","","253-255","","","","","","","","","","","","","","","","","","IEEE Xplore","","34 citations (Crossref) [2023-04-12]","","C:\Users\ambreen.hanif\Zotero\storage\HVJS3TC9\8544333.html; ; C:\Users\ambreen.hanif\Zotero\storage\M69LMJ4Q\Raju et al_2018_A Survey on Predicting Heart Disease using Data Mining Techniques.pdf","notion://www.notion.so/Raju-et-al-2018-de5d68a110d64f68b868932a760272f1","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2018 Conference on Emerging Devices and Smart Systems (ICEDSS)","","","","","","","","","","","","","","",""
"FS8WDQYB","conferencePaper","2017","Pouriyeh, Seyedamin; Vahid, Sara; Sannino, Giovanna; De Pietro, Giuseppe; Arabnia, Hamid; Gutierrez, Juan","A comprehensive investigation and comparison of Machine Learning Techniques in the domain of heart disease","2017 IEEE Symposium on Computers and Communications (ISCC)","","","10.1109/ISCC.2017.8024530","","This paper aims to investigate and compare the accuracy of different data mining classification schemes, employing Ensemble Machine Learning Techniques, for the prediction of heart disease. The Cleveland data set for heart diseases, containing 303 instances, has been used as the main database for the training and testing of the developed system. 10-Fold Cross-Validation has been applied in order to increase the amount of data, which would otherwise have been limited. Different classifiers, namely Decision Tree (DT), Naïve Bayes (NB), Multilayer Perceptron (MLP), K-Nearest Neighbor (K-NN), Single Conjunctive Rule Learner (SCRL), Radial Basis Function (RBF) and Support Vector Machine (SVM), have been employed. Moreover, the ensemble prediction of classifiers, bagging, boosting and stacking, has been applied to the dataset. The results of the experiments indicate that the SVM method using the boosting technique outperforms the other aforementioned methods.","2017-07","2023-04-11 23:48:30","2023-05-23 01:31:17","","204-207","","","","","","","","","","","","","","","","","","IEEE Xplore","","100 citations (Crossref) [2023-04-12]","","C:\Users\ambreen.hanif\Zotero\storage\GVAQQ7HJ\8024530.html; ; C:\Users\ambreen.hanif\Zotero\storage\DTY3WEA6\Pouriyeh et al_2017_A comprehensive investigation and comparison of Machine Learning Techniques in.pdf","notion://www.notion.so/Pouriyeh-et-al-2017-8b74bd9575f84f269abf2c2620c9ec30","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2017 IEEE Symposium on Computers and Communications (ISCC)","","","","","","","","","","","","","","",""
"BZQ8LZQL","webpage","","","[1711.11279] Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)","","","","","https://arxiv.org/abs/1711.11279","","","2023-04-11 03:53:57","2023-05-23 01:31:13","2023-04-11 03:53:57","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\P3I7LSFA\1711.html; ","notion://www.notion.so/1711-11279-Interpretability-Beyond-Feature-Attribution-Quantitative-Testing-with-Concept-Activatio-979820456ac84f649cd702dff7029ac2","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FB6EE4GV","conferencePaper","2014","Yosinski, Jason; Clune, Jeff; Bengio, Yoshua; Lipson, Hod","How transferable are features in deep neural networks?","Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2","","","","","Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.","2014-12-08","2023-04-11 02:00:08","2023-05-23 01:31:04","2023-04-10","3320–3328","","","","","","","NIPS'14","","","","MIT Press","Cambridge, MA, USA","","","","","","ACM Digital Library","","","","","notion://www.notion.so/Yosinski-et-al-2014-e4620545fedf4ae590ab248117a9aff8","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MBEQW28C","webpage","","","How transferable are features in deep neural networks? | Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2","","","","","https://dl-acm-org.simsrad.net.ocs.mq.edu.au/doi/10.5555/2969033.2969197","","","2023-04-11 00:27:10","2023-05-23 01:30:59","2023-04-11 00:27:10","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\LLAXVNTZ\2969033.html; ","notion://www.notion.so/How-Transferable-Are-Features-in-Deep-Neural-Networks-Proceedings-of-the-27th-International-Confe-0ba11b436f484d4eac4bf586393fdd1a","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FWG4LVS4","conferencePaper","2014","Razavian, Ali Sharif; Azizpour, Hossein; Sullivan, Josephine; Carlsson, Stefan","CNN Features Off-the-Shelf: An Astounding Baseline for Recognition","2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops","","","10.1109/CVPRW.2014.131","","Recent results indicate that the generic descriptors extracted from the convolutional neural networks are very powerful. This paper adds to the mounting evidence that this is indeed the case. We report on a series of experiments conducted for different recognition tasks using the publicly available code and model of the OverFeat network which was trained to perform object classification on ILSVRC13. We use features extracted from the OverFeat network as a generic image representation to tackle the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. We selected these tasks and datasets as they gradually move further away from the original task and data the OverFeat network was trained to solve. Astonishingly, we report consistent superior results compared to the highly tuned state-of-the-art systems in all the visual classification tasks on various datasets. For instance retrieval it consistently outperforms low memory footprint methods except for sculptures dataset. The results are achieved using a linear SVM classifier (or L2 distance in case of retrieval) applied to a feature representation of size 4096 extracted from a layer in the net. The representations are further modified using simple augmentation techniques e.g. jittering. The results strongly suggest that features obtained from deep learning with convolutional nets should be the primary candidate in most visual recognition tasks.","2014-06","2023-04-11 00:27:05","2023-05-23 01:30:49","","512-519","","","","","","CNN Features Off-the-Shelf","","","","","","","","","","","","IEEE Xplore","","2118 citations (Crossref) [2023-04-11] ISSN: 2160-7516","","C:\Users\ambreen.hanif\Zotero\storage\89NJZIRF\6910029.html; ; C:\Users\ambreen.hanif\Zotero\storage\6XILRLNX\Razavian et al_2014_CNN Features Off-the-Shelf.pdf","notion://www.notion.so/Razavian-et-al-2014-273ef59e3cfd4dc3a85eef9ed2818ffe","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops","","","","","","","","","","","","","","",""
"4UXCAUHI","conferencePaper","1994","Thrun, Sebastian","Extracting rules from artificial neural networks with distributed representations","Proceedings of the 7th International Conference on Neural Information Processing Systems","","","","","Although artificial neural networks have been applied in a variety of real-world scenarios with remarkable success, they have often been criticized for exhibiting a low degree of human comprehensibility. Techniques that compile compact sets of symbolic rules out of artificial neural networks offer a promising perspective to overcome this obvious deficiency of neural network representations. This paper presents an approach to the extraction of if-then rules from artificial neural networks. Its key mechanism is validity interval analysis, which is a generic tool for extracting symbolic knowledge by propagating rule-like knowledge through Backpropagation-style neural networks. Empirical studies in a robot arm domain illustrate the appropriateness of the proposed method for extracting rules from networks with real-valued and distributed representations.","1994-01-01","2023-04-11 00:15:40","2023-05-23 01:30:33","2023-04-10","505–512","","","","","","","NIPS'94","","","","MIT Press","Cambridge, MA, USA","","","","","","ACM Digital Library","","","","","notion://www.notion.so/Thrun-1994-1e6c2480d6b9431bbce990688e708c34","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V7KZR5UG","journalArticle","2015","Bach, Sebastian; Binder, Alexander; Montavon, Grégoire; Klauschen, Frederick; Müller, Klaus-Robert; Samek, Wojciech","On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation","PLOS ONE","","1932-6203","10.1371/journal.pone.0130140","https://dx.plos.org/10.1371/journal.pone.0130140","Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.","2015-07-10","2023-04-11 00:15:35","2023-05-23 01:30:31","2023-04-11 00:15:35","e0130140","","7","10","","PLoS ONE","","","","","","","","en","","","","","Semantic Scholar","","1621 citations (Crossref) [2023-04-11]","","C:\Users\ambreen.hanif\Zotero\storage\UWGY7PHV\Bach et al_2015_On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise.pdf; ; ","notion://www.notion.so/Bach-et-al-2015-c50059203678467f946d103c713ebe02; https://www.semanticscholar.org/paper/On-Pixel-Wise-Explanations-for-Non-Linear-Decisions-Bach-Binder/17a273bbd4448083b01b5a9389b3c37f5425aac0","notion","","Suarez, Oscar Deniz","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MID9DELW","preprint","2021","Luss, Ronny; Chen, Pin-Yu; Dhurandhar, Amit; Sattigeri, Prasanna; Zhang, Yunfeng; Shanmugam, Karthikeyan; Tu, Chun-Chen","Leveraging Latent Features for Local Explanations","","","","10.48550/arXiv.1905.12698","http://arxiv.org/abs/1905.12698","As the application of deep neural networks proliferates in numerous areas such as medical imaging, video surveillance, and self driving cars, the need for explaining the decisions of these models has become a hot research topic, both at the global and local level. Locally, most explanation methods have focused on identifying relevance of features, limiting the types of explanations possible. In this paper, we investigate a new direction by leveraging latent features to generate contrastive explanations; predictions are explained not only by highlighting aspects that are in themselves sufficient to justify the classification, but also by new aspects which if added will change the classification. The key contribution of this paper lies in how we add features to rich data in a formal yet humanly interpretable way that leads to meaningful results. Our new definition of ""addition"" uses latent features to move beyond the limitations of previous explanations and resolve an open question laid out in Dhurandhar, et. al. (2018), which creates local contrastive explanations but is limited to simple datasets such as grayscale images. The strength of our approach in creating intuitive explanations that are also quantitatively superior to other methods is demonstrated on three diverse image datasets (skin lesions, faces, and fashion apparel). A user study with 200 participants further exemplifies the benefits of contrastive information, which can be viewed as complementary to other state-of-the-art interpretability methods.","2021-05-29","2023-04-10 23:25:23","2023-05-23 01:30:30","2023-04-10 23:25:23","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1905.12698 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\42TR7N3I\1905.html; C:\Users\ambreen.hanif\Zotero\storage\RQY4WT79\Luss et al_2021_Leveraging Latent Features for Local Explanations.pdf; ","notion://www.notion.so/Luss-et-al-2021-f1983c378cdd4f19a75a33ab0d2b9dac","notion","","","","","","","","","","","","","","","","","","","","arXiv:1905.12698","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GD49JTCD","preprint","2021","Hanif, Ambreen","Towards Explainable Artificial Intelligence in Banking and Financial Services","","","","10.48550/arXiv.2112.08441","http://arxiv.org/abs/2112.08441","Artificial intelligence (AI) enables machines to learn from human experience, adjust to new inputs, and perform human-like tasks. AI is progressing rapidly and is transforming the way businesses operate, from process automation to cognitive augmentation of tasks and intelligent process/data analytics. However, the main challenge for human users would be to understand and appropriately trust the result of AI algorithms and methods. In this paper, to address this challenge, we study and analyze the recent work done in Explainable Artificial Intelligence (XAI) methods and tools. We introduce a novel XAI process, which facilitates producing explainable models while maintaining a high level of learning performance. We present an interactive evidence-based approach to assist human users in comprehending and trusting the results and output created by AI-enabled algorithms. We adopt a typical scenario in the Banking domain for analyzing customer transactions. We develop a digital dashboard to facilitate interacting with the algorithm results and discuss how the proposed XAI method can significantly improve the confidence of data scientists in understanding the result of AI-enabled algorithms.","2021-12-14","2023-04-10 23:25:19","2023-05-23 01:30:28","2023-04-10 23:25:19","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2112.08441 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\WCPDL3XB\2112.html; C:\Users\ambreen.hanif\Zotero\storage\MLJ7MJPK\Hanif_2021_Towards Explainable Artificial Intelligence in Banking and Financial Services.pdf; ","notion://www.notion.so/Hanif-2021-bdb794dd20c34c7aa4814ccea6b31897","notion","","","","","","","","","","","","","","","","","","","","arXiv:2112.08441","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G4SWWPYS","preprint","2021","Chen, Lili; Lu, Kevin; Rajeswaran, Aravind; Lee, Kimin; Grover, Aditya; Laskin, Michael; Abbeel, Pieter; Srinivas, Aravind; Mordatch, Igor","Decision Transformer: Reinforcement Learning via Sequence Modeling","","","","10.48550/arXiv.2106.01345","http://arxiv.org/abs/2106.01345","We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.","2021-06-24","2023-03-29 23:10:03","2023-05-23 01:30:26","2023-03-29 23:10:03","","","","","","","Decision Transformer","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2106.01345 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\94PTXSL3\2106.html; C:\Users\ambreen.hanif\Zotero\storage\MHZ9DS3E\Chen et al_2021_Decision Transformer.pdf; ","notion://www.notion.so/Chen-et-al-2021-330c24a377844597be9f484c52166f61","notion","","","","","","","","","","","","","","","","","","","","arXiv:2106.01345","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5VS9DGN7","journalArticle","2022","Heldens, Stijn; Sclocco, Alessio; Dreuning, Henk; van Werkhoven, Ben; Hijma, Pieter; Maassen, Jason; van Nieuwpoort, Rob V.","litstudy: A Python package for literature reviews","SoftwareX","","2352-7110","10.1016/j.softx.2022.101207","https://www.sciencedirect.com/science/article/pii/S235271102200125X","Researchers are often faced with exploring new research domains. Broad questions about the research domain, such as who are the influential authors or what are important topics, are difficult to answer due to the overwhelming number of relevant publications. Therefore, we present litstudy: a Python package that enables answering such questions using simple scripts or Jupyter notebooks. The package enables selecting scientific publications and studying their metadata using visualizations, bibliographic network analysis, and natural language processing. The software was previously used in a publication on the landscape of Exascale computing, and we envision great potential for reuse.","2022-12-01","2023-03-29 05:57:36","2023-05-23 01:30:25","2023-03-29 05:57:36","101207","","","20","","SoftwareX","litstudy","","","","","","","en","","","","","ScienceDirect","","1 citations (Crossref) [2023-03-29]","","C:\Users\ambreen.hanif\Zotero\storage\LJJKHEY3\Heldens et al_2022_litstudy.pdf; ; C:\Users\ambreen.hanif\Zotero\storage\3NJXP8WR\S235271102200125X.html","notion://www.notion.so/Heldens-et-al-2022-6461a993b0e249628f110b388e45d21a","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JRLDJ4CR","preprint","2023","Nie, Mingshuo; Chen, Dongming; Wang, Dongqi","Reinforcement learning on graphs: A survey","","","","10.48550/arXiv.2204.06127","http://arxiv.org/abs/2204.06127","Graph mining tasks arise from many different application domains, ranging from social networks, transportation to E-commerce, etc., which have been receiving great attention from the theoretical and algorithmic design communities in recent years, and there has been some pioneering work employing the research-rich Reinforcement Learning (RL) techniques to address graph data mining tasks. However, these graph mining methods and RL models are dispersed in different research areas, which makes it hard to compare them. In this survey, we provide a comprehensive overview of RL and graph mining methods and generalize these methods to Graph Reinforcement Learning (GRL) as a unified formulation. We further discuss the applications of GRL methods across various domains and summarize the method descriptions, open-source codes, and benchmark datasets of GRL methods. Furthermore, we propose important directions and challenges to be solved in the future. As far as we know, this is the latest work on a comprehensive survey of GRL, this work provides a global view and a learning resource for scholars. In addition, we create an online open-source for both interested scholars who want to enter this rapidly developing domain and experts who would like to compare GRL methods.","2023-01-14","2023-03-29 03:16:37","2023-05-23 01:30:17","2023-03-29 03:16:37","","","","","","","Reinforcement learning on graphs","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2204.06127 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\D2MHVMCB\2204.html; C:\Users\ambreen.hanif\Zotero\storage\4G8V8J5U\Nie et al_2023_Reinforcement learning on graphs.pdf; ","notion://www.notion.so/Nie-et-al-2023-a01081d09df24bea9ae189df08de061f","notion","","","","","","","","","","","","","","","","","","","","arXiv:2204.06127","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PQIPE2SU","journalArticle","2022","Zhang, Ziwei; Cui, Peng; Zhu, Wenwu","Deep Learning on Graphs: A Survey","IEEE Transactions on Knowledge and Data Engineering","","1558-2191","10.1109/TKDE.2020.2981333","","Deep learning has been shown to be successful in a number of domains, ranging from acoustics, images, to natural language processing. However, applying deep learning to the ubiquitous graph data is non-trivial because of the unique characteristics of graphs. Recently, substantial research efforts have been devoted to applying deep learning methods to graphs, resulting in beneficial advances in graph analysis techniques. In this survey, we comprehensively review the different types of deep learning methods on graphs. We divide the existing methods into five categories based on their model architectures and training strategies: graph recurrent neural networks, graph convolutional networks, graph autoencoders, graph reinforcement learning, and graph adversarial methods. We then provide a comprehensive overview of these methods in a systematic manner mainly by following their development history. We also analyze the differences and compositions of different methods. Finally, we briefly outline the applications in which they have been used and discuss potential future research directions.","2022-01","2023-03-29 03:15:45","2023-05-23 01:30:16","","249-270","","1","34","","","Deep Learning on Graphs","","","","","","","","","","","","IEEE Xplore","","274 citations (Crossref) [2023-03-29] Conference Name: IEEE Transactions on Knowledge and Data Engineering","","; C:\Users\ambreen.hanif\Zotero\storage\KFWT8AWV\Zhang et al_2022_Deep Learning on Graphs.pdf","notion://www.notion.so/Zhang-et-al-2022-f636ce6f2eac4138acf6bc84939c66c8","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G7WIPPPL","conferencePaper","2021","Lin, Wanyu; Lan, Hao; Li, Baochun","Generative Causal Explanations for Graph Neural Networks","Proceedings of the 38th International Conference on Machine Learning","","","","https://proceedings.mlr.press/v139/lin21d.html","This paper presents {\em Gem}, a model-agnostic approach for providing interpretable explanations for any GNNs on various graph learning tasks. Specifically, we formulate the problem of providing explanations for the decisions of GNNs as a causal learning task. Then we train a causal explanation model equipped with a loss function based on Granger causality. Different from existing explainers for GNNs, {\em Gem} explains GNNs on graph-structured data from a causal perspective. It has better generalization ability as it has no requirements on the internal structure of the GNNs or prior knowledge on the graph learning tasks. In addition, {\em Gem}, once trained, can be used to explain the target GNN very quickly. Our theoretical analysis shows that several recent explainers fall into a unified framework of {\em additive feature attribution methods}. Experimental results on synthetic and real-world datasets show that {\em Gem} achieves a relative increase of the explanation accuracy by up to 303030% and speeds up the explanation process by up to 110×110×110\times as compared to its state-of-the-art alternatives.","2021-07-01","2023-03-29 03:13:31","2023-05-23 01:30:14","2023-03-29 03:13:31","6666-6679","","","","","","","","","","","PMLR","","en","","","","","proceedings.mlr.press","","ISSN: 2640-3498","","C:\Users\ambreen.hanif\Zotero\storage\HI39C4SG\Lin et al_2021_Generative Causal Explanations for Graph Neural Networks.pdf; ","notion://www.notion.so/Lin-et-al-2021-d8706b404e47420b9ac65348db79db32","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Machine Learning","","","","","","","","","","","","","","",""
"Z3ICSJPM","preprint","2022","Miller, Tim","Are we measuring trust correctly in explainability, interpretability, and transparency research?","","","","","http://arxiv.org/abs/2209.00651","This paper presents an argument for why we are not measuring trust sufficiently in explainability, interpretability, and transparency research. Most studies ask participants to complete a trust scale to rate their trust of a model that has been explained/interpreted. If the trust is increased, we consider this a positive. However, there are two issues with this. First, we usually have no way of knowing whether participants should trust the model. Trust should surely decrease if a model is of poor quality. Second, these scales measure perceived trust rather than demonstrated trust. This paper showcases three methods that do a good job at measuring perceived and demonstrated trust. It is intended to be starting point for discussion on this topic, rather than to be the final say. The author invites critique and discussion.","2022-08-31","2023-03-29 03:11:10","2023-05-23 01:30:12","2023-03-29 03:11:10","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2209.00651 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\RRMIN3JZ\2209.html; C:\Users\ambreen.hanif\Zotero\storage\WKURGIFS\Miller_2022_Are we measuring trust correctly in explainability, interpretability, and.pdf; ","notion://www.notion.so/Miller-2022-b615108b0a2a4c4f9cf0f654f8e42dff","notion","","","","","","","","","","","","","","","","","","","","arXiv:2209.00651","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FMNKD665","journalArticle","2021","Vassiliades, Alexandros; Bassiliades, Nick; Patkos, Theodore","Argumentation and explainable artificial intelligence: a survey","The Knowledge Engineering Review","","0269-8889, 1469-8005","10.1017/S0269888921000011","https://www.cambridge.org/core/journals/knowledge-engineering-review/article/argumentation-and-explainable-artificial-intelligence-a-survey/DC6841ED8C7A80DC9EFADF87E4558A1F#figures","Argumentation and eXplainable Artificial Intelligence (XAI) are closely related, as in the recent years, Argumentation has been used for providing Explainability to AI. Argumentation can show step by step how an AI System reaches a decision; it can provide reasoning over uncertainty and can find solutions when conflicting information is faced. In this survey, we elaborate over the topics of Argumentation and XAI combined, by reviewing all the important methods and studies, as well as implementations that use Argumentation to provide Explainability in AI. More specifically, we show how Argumentation can enable Explainability for solving various types of problems in decision-making, justification of an opinion, and dialogues. Subsequently, we elaborate on how Argumentation can help in constructing explainable systems in various applications domains, such as in Medical Informatics, Law, the Semantic Web, Security, Robotics, and some general purpose systems. Finally, we present approaches that combine Machine Learning and Argumentation Theory, toward more interpretable predictive models.","2021","2023-03-29 03:09:58","2023-05-23 01:30:11","2023-03-29 03:09:58","e5","","","36","","","Argumentation and explainable artificial intelligence","","","","","","","en","","","","","Cambridge University Press","","31 citations (Crossref) [2023-03-29] Publisher: Cambridge University Press","","; C:\Users\ambreen.hanif\Zotero\storage\ZJ4BA2GM\Vassiliades et al_2021_Argumentation and explainable artificial intelligence.pdf","notion://www.notion.so/Vassiliades-et-al-2021-bf5afc29c981468b9c8ec9af9ef23e5a","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WH9HSUX2","preprint","2022","Khosla, Megha","Privacy and Transparency in Graph Machine Learning: A Unified Perspective","","","","","http://arxiv.org/abs/2207.10896","Graph Machine Learning (GraphML), whereby classical machine learning is generalized to irregular graph domains, has enjoyed a recent renaissance, leading to a dizzying array of models and their applications in several domains. With its growing applicability to sensitive domains and regulations by governmental agencies for trustworthy AI systems, researchers have started looking into the issues of transparency and privacy of graph learning. However, these topics have been mainly investigated independently. In this position paper, we provide a unified perspective on the interplay of privacy and transparency in GraphML. In particular, we describe the challenges and possible research directions for a formal investigation of privacy-transparency tradeoffs in GraphML.","2022-10-19","2023-03-29 01:52:51","2023-05-23 01:30:10","2023-03-29 01:52:51","","","","","","","Privacy and Transparency in Graph Machine Learning","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2207.10896 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\B2AFDI5U\2207.html; C:\Users\ambreen.hanif\Zotero\storage\LXNHS5FK\Khosla_2022_Privacy and Transparency in Graph Machine Learning.pdf; ","notion://www.notion.so/Khosla-2022-e0bc082639454114af86f5a1dbfa9115","notion","","","","","","","","","","","","","","","","","","","","arXiv:2207.10896","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZGYI6CSY","conferencePaper","2019","Ying, Zhitao; Bourgeois, Dylan; You, Jiaxuan; Zitnik, Marinka; Leskovec, Jure","GNNExplainer: Generating Explanations for Graph Neural Networks","Advances in Neural Information Processing Systems","","","","https://proceedings.neurips.cc/paper_files/paper/2019/hash/d80b7040b773199015de6d3b4293c8ff-Abstract.html","Graph Neural Networks (GNNs) are a powerful tool for machine learning on graphs.GNNs combine node feature information with the graph structure by  recursively passing neural messages along edges of the input graph. However, incorporating both graph structure and feature information leads to complex models, and explaining predictions made by GNNs remains unsolved. Here we propose GNNExplainer, the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task. Given an instance, GNNExplainer identifies a compact subgraph structure and a small subset of node features that have a crucial role in GNN's prediction.  Further, GNNExplainer  can generate consistent and concise explanations for an entire class of instances. We formulate GNNExplainer as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures. Experiments on synthetic and real-world graphs show that our approach can identify important graph structures as well as node features, and outperforms baselines by 17.1% on average. GNNExplainer  provides a variety of benefits, from the ability to visualize semantically relevant structures to interpretability, to giving insights into errors of faulty GNNs.","2019","2023-03-29 01:51:21","2023-05-23 01:30:08","2023-03-29 01:51:21","","","","32","","","GNNExplainer","","","","","Curran Associates, Inc.","","","","","","","Neural Information Processing Systems","","","","; C:\Users\ambreen.hanif\Zotero\storage\3WGE992K\Ying et al_2019_GNNExplainer.pdf","notion://www.notion.so/Ying-et-al-2019-3c749599d11247a4a57645f8e399e60d","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H7N54TRF","journalArticle","2023","Agarwal, Chirag; Queen, Owen; Lakkaraju, Himabindu; Zitnik, Marinka","Evaluating explainability for graph neural networks","Scientific Data","","2052-4463","10.1038/s41597-023-01974-x","https://www.nature.com/articles/s41597-023-01974-x","As explanations are increasingly used to understand the behavior of graph neural networks (GNNs), evaluating the quality and reliability of GNN explanations is crucial. However, assessing the quality of GNN explanations is challenging as existing graph datasets have no or unreliable ground-truth explanations. Here, we introduce a synthetic graph data generator, ShapeGGen, which can generate a variety of benchmark datasets (e.g., varying graph sizes, degree distributions, homophilic vs. heterophilic graphs) accompanied by ground-truth explanations. The flexibility to generate diverse synthetic datasets and corresponding ground-truth explanations allows ShapeGGen to mimic the data in various real-world areas. We include ShapeGGen and several real-world graph datasets in a graph explainability library, GraphXAI. In addition to synthetic and real-world graph datasets with ground-truth explanations, GraphXAI provides data loaders, data processing functions, visualizers, GNN model implementations, and evaluation metrics to benchmark GNN explainability methods.","2023-03-18","2023-03-29 01:47:50","2023-05-23 01:30:07","2023-03-29 01:47:50","144","","1","10","","Sci Data","","","","","","","","en","2023 The Author(s)","","","","www-nature-com.simsrad.net.ocs.mq.edu.au","","2 citations (Crossref) [2023-03-29] Number: 1 Publisher: Nature Publishing Group","","C:\Users\ambreen.hanif\Zotero\storage\IXLTC9UA\Agarwal et al_2023_Evaluating explainability for graph neural networks.pdf; ","notion://www.notion.so/Agarwal-et-al-2023-50058497430f4c248b0214b0ea95aca9","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7WKEXBYR","journalArticle","2022","Aviyente, Selin; Karaaslanli, Abdullah","Explainability in Graph Data Science: Interpretability, replicability, and reproducibility of community detection","IEEE Signal Processing Magazine","","1558-0792","10.1109/MSP.2022.3149471","","In many modern data science problems, data are represented by a graph (network), e.g., social, biological, and communication networks. Over the past decade, numerous signal processing and machine learning (ML) algorithms have been introduced for analyzing graph structured data. With the growth of interest in graphs and graph-based learning tasks in a variety of applications, there is a need to explore explainability in graph data science. In this article, we aim to approach the issue of explainable graph data science, focusing on one of the most fundamental learning tasks, community detection, as it is usually the first step in extracting information from graphs. A community is a dense subnetwork within a larger network that corresponds to a specific function. Despite the success of different community detection methods on synthetic networks with strong modular structure, much remains unknown about the quality and significance of the outputs of these algorithms when applied to real-world networks with unknown modular structure. Inspired by recent advances in explainable artificial intelligence (AI) and ML, in this article, we present methods and metrics from network science to quantify three different aspects of explainability, i.e., interpretability, replicability, and reproducibility, in the context of community detection.","2022-07","2023-03-29 01:46:11","2023-05-23 01:30:02","","25-39","","4","39","","","Explainability in Graph Data Science","","","","","","","","","","","","IEEE Xplore","","0 citations (Crossref) [2023-03-29] Conference Name: IEEE Signal Processing Magazine","","C:\Users\ambreen.hanif\Zotero\storage\DQFBPMR6\Aviyente_Karaaslanli_2022_Explainability in Graph Data Science.pdf; ","notion://www.notion.so/Aviyente-Karaaslanli-2022-74a253c9ee7647678c2a81cc7caa802b","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A887VTLT","webpage","","","Graph Learning: A Survey | IEEE Journals & Magazine | IEEE Xplore","","","","","https://ieeexplore-ieee-org.simsrad.net.ocs.mq.edu.au/abstract/document/9416834","","","2023-03-29 01:42:45","2023-05-23 01:29:58","2023-03-29 01:42:45","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\U9WMBHN4\9416834.html; ","notion://www.notion.so/Graph-Learning-A-Survey-IEEE-Journals-Magazine-IEEE-Xplore-n-d-4e9f25a4968c4551bbe96fc211f5ec53","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MSXTJ942","journalArticle","2021","Xia, Feng; Sun, Ke; Yu, Shuo; Aziz, Abdul; Wan, Liangtian; Pan, Shirui; Liu, Huan","Graph Learning: A Survey","IEEE Transactions on Artificial Intelligence","","2691-4581","10.1109/TAI.2021.3076021","","Graphs are widely used as a popular representation of the network structure of connected data. Graph data can be found in a broad spectrum of application domains such as social systems, ecosystems, biological networks, knowledge graphs, and information systems. With the continuous penetration of artificial intelligence technologies, graph learning (i.e., machine learning on graphs) is gaining attention from both researchers and practitioners. Graph learning proves effective for many tasks, such as classification, link prediction, and matching. Generally, graph learning methods extract relevant features of graphs by taking advantage of machine learning algorithms. In this survey, we present a comprehensive overview on the state-of-the-art of graph learning. Special attention is paid to four categories of existing graph learning methods, including graph signal processing, matrix factorization, random walk, and deep learning. Major models and algorithms under these categories are reviewed, respectively. We examine graph learning applications in areas such as text, images, science, knowledge graphs, and combinatorial optimization. In addition, we discuss several promising research directions in this field.","2021-04","2023-03-29 01:37:03","2023-05-23 01:29:56","","109-127","","2","2","","","Graph Learning","","","","","","","","","","","","IEEE Xplore","","72 citations (Crossref) [2023-03-29] Conference Name: IEEE Transactions on Artificial Intelligence","","; C:\Users\ambreen.hanif\Zotero\storage\Y3A837A6\Xia et al_2021_Graph Learning.pdf","notion://www.notion.so/Xia-et-al-2021-092a70b5b4f040828277b4594893bec5","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"P2KIIT9S","preprint","2023","Zhao, Tong; Jin, Wei; Liu, Yozen; Wang, Yingheng; Liu, Gang; Günnemann, Stephan; Shah, Neil; Jiang, Meng","Graph Data Augmentation for Graph Machine Learning: A Survey","","","","","http://arxiv.org/abs/2202.08871","Data augmentation has recently seen increased interest in graph machine learning given its demonstrated ability to improve model performance and generalization by added training data. Despite this recent surge, the area is still relatively under-explored, due to the challenges brought by complex, non-Euclidean structure of graph data, which limits the direct analogizing of traditional augmentation operations on other types of image, video or text data. Our work aims to give a necessary and timely overview of existing graph data augmentation methods; notably, we present a comprehensive and systematic survey of graph data augmentation approaches, summarizing the literature in a structured manner. We first introduce three different taxonomies for categorizing graph data augmentation methods from the data, task, and learning perspectives, respectively. Next, we introduce recent advances in graph data augmentation, differentiated by their methodologies and applications. We conclude by outlining currently unsolved challenges and directions for future research. Overall, our work aims to clarify the landscape of existing literature in graph data augmentation and motivates additional work in this area, providing a helpful resource for researchers and practitioners in the broader graph machine learning domain. Additionally, we provide a continuously updated reading list at https://github.com/zhao-tong/graph-data-augmentation-papers.","2023-01-18","2023-03-29 01:31:06","2023-05-23 01:29:54","2023-03-29 01:31:06","","","","","","","Graph Data Augmentation for Graph Machine Learning","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2202.08871 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\26FAPMMX\2202.html; ; C:\Users\ambreen.hanif\Zotero\storage\SZNBFMZ6\Zhao et al_2023_Graph Data Augmentation for Graph Machine Learning.pdf","notion://www.notion.so/Zhao-et-al-2023-b0e110009d22417c8e091e87d6c0f819","notion","","","","","","","","","","","","","","","","","","","","arXiv:2202.08871","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VD2G94W6","journalArticle","2021","Sarker, Iqbal H.","Deep Learning: A Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions","SN Computer Science","","2661-8907","10.1007/s42979-021-00815-1","https://doi.org/10.1007/s42979-021-00815-1","Deep learning (DL), a branch of machine learning (ML) and artificial intelligence (AI) is nowadays considered as a core technology of today’s Fourth Industrial Revolution (4IR or Industry 4.0). Due to its learning capabilities from data, DL technology originated from artificial neural network (ANN), has become a hot topic in the context of computing, and is widely applied in various application areas like healthcare, visual recognition, text analytics, cybersecurity, and many more. However, building an appropriate DL model is a challenging task, due to the dynamic nature and variations in real-world problems and data. Moreover, the lack of core understanding turns DL methods into black-box machines that hamper development at the standard level. This article presents a structured and comprehensive view on DL techniques including a taxonomy considering various types of real-world tasks like supervised or unsupervised. In our taxonomy, we take into account deep networks for supervised or discriminative learning, unsupervised or generative learning as well as hybrid learning and relevant others. We also summarize real-world application areas where deep learning techniques can be used. Finally, we point out ten potential aspects for future generation DL modeling with research directions. Overall, this article aims to draw a big picture on DL modeling that can be used as a reference guide for both academia and industry professionals.","2021-08-18","2023-03-28 23:41:22","2023-05-23 01:29:53","2023-03-28 23:41:22","420","","6","2","","SN COMPUT. SCI.","Deep Learning","","","","","","","en","","","","","Springer Link","","185 citations (Crossref) [2023-03-29]","","; C:\Users\ambreen.hanif\Zotero\storage\3QQKBWIE\Sarker_2021_Deep Learning.pdf","notion://www.notion.so/Sarker-2021-099a843aa4754f5f8a60ba748061addc","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V7WJC98Z","conferencePaper","2019","Amouzgar, Farhad; Beheshti, Amin; Ghodratnama, Samira; Benatallah, Boualem; Yang, Jian; Sheng, Quan Z.","iSheets: A Spreadsheet-Based Machine Learning Development Platform for Data-Driven Process Analytics","Service-Oriented Computing – ICSOC 2018 Workshops","978-3-030-17642-6","","10.1007/978-3-030-17642-6_43","","In the era of big data, the quality of services any organization provides largely depends on the quality of their data-driven processes. In this context, the goal of process data science, is to enable innovative forms of information processing that enable enhanced insight and decision making. For example, consider the data-driven and knowledge-intensive processes in Australian government’s office of the e-Safety commissioner, where the goal is to empowering all citizens to have safer, more positive experiences online. An example process, is to analyze the large amount of data generated every second on social networks to understand patterns of suicidal thoughts, online bullying and criminal/exterimist behaviour. Current processes leverage machine learning systems, e.g., to perform automatic mental-health-disorders detection from social networks. This approach is challenging for knowledge workers (end-user analysts) who have little knowledge of computer science to use machine learning solutions in their data-driven processes. In this paper, we present a novel platform, namely iSheets, that makes it easy for knowledge workers of all skill levels to use machine learning technology, the way people use spreadsheet. We present and develop a Machine Learning (ML) as a service framework and a spreadsheet-based ML development platform to enable knowledge workers in data-driven processes engage with ML tasks and uncover hidden insights through learning in an easy way.","2019","2023-03-28 23:22:52","2023-05-23 01:29:51","","453-457","","","","","","iSheets","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","9 citations (Crossref) [2023-03-29]","","C:\Users\ambreen.hanif\Zotero\storage\L9E9FKG8\Amouzgar et al_2019_iSheets.pdf; ","notion://www.notion.so/Amouzgar-et-al-2019-b85ac935e81b4962903e74ec37964282","notion","","Liu, Xiao; Mrissa, Michael; Zhang, Liang; Benslimane, Djamal; Ghose, Aditya; Wang, Zhongjie; Bucchiarone, Antonio; Zhang, Wei; Zou, Ying; Yu, Qi","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FN34QAT9","thesis","2021","Bofarull Cabello, Antoni","Feature engineering, dimensionality reduction and interpretability through autoencoders for structured data","","","","","https://upcommons.upc.edu/handle/2117/356971","","2021-06-30","2023-03-24 05:49:04","2023-05-23 01:29:49","2023-03-24 05:49:04","","","","","","","","","","","","Universitat Politècnica de Catalunya","","eng","Open Access","Master thesis","","","upcommons.upc.edu","","Accepted: 2021-11-23T14:19:36Z","","C:\Users\ambreen.hanif\Zotero\storage\8ICLMCSM\Bofarull Cabello_2021_Feature engineering, dimensionality reduction and interpretability through.pdf; ","notion://www.notion.so/Bofarull-Cabello-2021-7cc5a0ae927b498980e389c2c4d7b79c","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VHLP9RY8","webpage","2021","","Hopsworks - The Python-Centric Feature Store","","","","","https://www.hopsworks.ai/","Hopsworks is the best python-centric feature store with the most complete MLOps capabilities. Create an effective pipeline on all of your AI data, and bring your models to production.","2021","2023-03-24 02:53:58","2023-05-23 01:29:43","2023-03-24 02:53:58","","","","","","","","","","","","","","en","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\WG4INQ4V\www.hopsworks.ai.html","notion://www.notion.so/Hopsworks-The-Python-Centric-Feature-Store-2021-dc970d1d27444acf9dbb8576ea8d3897","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9RDPU442","webpage","2021","","Feature Store For ML","","","","","https://www.featurestore.org/","The feature store is the central place to store curated features for machine learning pipelines, FSML aims to create content for information and knowledge in the ever evolving feature store's world and surrounding data and AI environment.","2021","2023-03-24 02:53:51","2023-05-23 01:29:41","2023-03-24 02:53:51","","","","","","","","","","","","","","en","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\6CZVNCB9\www.featurestore.org.html","notion://www.notion.so/Feature-Store-For-ML-2021-ecceeff96a73407d9749091c7512277c","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VS8J3J6F","preprint","2020","Alkowaileet, Wail Y.; Alsubaiee, Sattam; Carey, Michael J.","An LSM-based Tuple Compaction Framework for Apache AsterixDB (Extended Version)","","","","","http://arxiv.org/abs/1910.08185","Document database systems store self-describing semi-structured records, such as JSON, ""as-is"" without requiring the users to pre-define a schema. This provides users with the flexibility to change the structure of incoming records without worrying about taking the system offline or hindering the performance of currently running queries. However, the flexibility of such systems does not free. The large amount of redundancy in the records can introduce an unnecessary storage overhead and impact query performance. Our focus in this paper is to address the storage overhead issue by introducing a tuple compactor framework that infers and extracts the schema from self-describing semi-structured records during the data ingestion. As many prominent document stores, such as MongoDB and Couchbase, adopt Log Structured Merge (LSM) trees in their storage engines, our framework exploits LSM lifecycle events to piggyback the schema inference and extraction operations. We have implemented and empirically evaluated our approach to measure its impact on storage, data ingestion, and query performance in the context of Apache AsterixDB.","2020-05-11","2023-03-24 02:52:54","2023-05-23 01:29:40","2023-03-24 02:52:54","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1910.08185 [cs] version: 2","","C:\Users\ambreen.hanif\Zotero\storage\F3CM733M\Alkowaileet et al_2020_An LSM-based Tuple Compaction Framework for Apache AsterixDB (Extended Version).pdf; C:\Users\ambreen.hanif\Zotero\storage\URNEDWX3\1910.html; ","notion://www.notion.so/Alkowaileet-et-al-2020-454fffc87a3f4f528079d03e4bc01947","notion","","","","","","","","","","","","","","","","","","","","arXiv:1910.08185","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CKHGXZJW","conferencePaper","2019","Xian, Yikun; Fu, Zuohui; Muthukrishnan, S.; de Melo, Gerard; Zhang, Yongfeng","Reinforcement Knowledge Graph Reasoning for Explainable Recommendation","Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval","978-1-4503-6172-9","","10.1145/3331184.3331203","https://dl.acm.org/doi/10.1145/3331184.3331203","Recent advances in personalized recommendation have sparked great interest in the exploitation of rich structured information provided by knowledge graphs. Unlike most existing approaches that only focus on leveraging knowledge graphs for more accurate recommendation, we aim to conduct explicit reasoning with knowledge for decision making so that the recommendations are generated and supported by an interpretable causal inference procedure. To this end, we propose a method called Policy-Guided Path Reasoning (PGPR), which couples recommendation and interpretability by providing actual paths in a knowledge graph. Our contributions include four aspects. We first highlight the significance of incorporating knowledge graphs into recommendation to formally define and interpret the reasoning process. Second, we propose a reinforcement learning (RL) approach featured by an innovative soft reward strategy, user-conditional action pruning and a multi-hop scoring function. Third, we design a policy-guided graph search algorithm to efficiently and effectively sample reasoning paths for recommendation. Finally, we extensively evaluate our method on several large-scale real-world benchmark datasets, obtaining favorable results compared with state-of-the-art methods.","2019-07-18","2023-03-23 00:35:32","2023-05-23 01:29:37","2023-03-22","285–294","","","","","","","SIGIR'19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","; C:\Users\ambreen.hanif\Zotero\storage\7GW3SH5V\Xian et al_2019_Reinforcement Knowledge Graph Reasoning for Explainable Recommendation.pdf","notion://www.notion.so/Xian-et-al-2019-efe93f0d61d74f2d97f82e0a66c051db","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CKPT2CKD","book","2020","Tiddi, I.; Lécué, F.; Hitzler, P.","Knowledge Graphs for eXplainable Artificial Intelligence: Foundations, Applications and Challenges","","978-1-64368-081-1","","","","The latest advances in Artificial Intelligence and (deep) Machine Learning in particular revealed a major drawback of modern intelligent systems, namely the inability to explain their decisions in a way that humans can easily understand. While eXplainable AI rapidly became an active area of research in response to this need for improved understandability and trustworthiness, the field of Knowledge Representation and Reasoning (KRR) has on the other hand a long-standing tradition in managing information in a symbolic, human-understandable form. This book provides the first comprehensive collection of research contributions on the role of knowledge graphs for eXplainable AI (KG4XAI), and the papers included here present academic and industrial research focused on the theory, methods and implementations of AI systems that use structured knowledge to generate reliable explanations. Introductory material on knowledge graphs is included for those readers with only a minimal background in the field, as well as specific chapters devoted to advanced methods, applications and case-studies that use knowledge graphs as a part of knowledge-based, explainable systems (KBX-systems). The final chapters explore current challenges and future research directions in the area of knowledge graphs for eXplainable AI. The book not only provides a scholarly, state-of-the-art overview of research in this subject area, but also fosters the hybrid combination of symbolic and subsymbolic AI methods, and will be of interest to all those working in the field.","2020-05-06","2023-03-23 00:35:20","2023-05-23 01:29:36","","","314","","","","","Knowledge Graphs for eXplainable Artificial Intelligence","","","","","IOS Press","","en","","","","","Google Books","","Google-Books-ID: kfjtDwAAQBAJ","","; ","https://books.google.com.au/books?id=kfjtDwAAQBAJ; notion://www.notion.so/Tiddi-et-al-2020-3713a1eeea3348efbadc606df8f85cba","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NTGY3N8V","webpage","2018","Mohseni, Sina; Pachuilo, Andrew; Nirjhar, Ehsanul Haque; Linder, Rhema; Pena, Alyssa; Ragan, Eric D.","Analytic Provenance Datasets: A Data Repository of Human Analysis Activity and Interaction Logs","arXiv.org","","","","https://arxiv.org/abs/1801.05076v1","We present an analytic provenance data repository that can be used to study human analysis activity, thought processes, and software interaction with visual analysis tools during exploratory data analysis. We conducted a series of user studies involving exploratory data analysis scenario with textual and cyber security data. Interactions logs, think-alouds, videos and all coded data in this study are available online for research purposes. Analysis sessions are segmented in multiple sub-task steps based on user think-alouds, video and audios captured during the studies. These analytic provenance datasets can be used for research involving tools and techniques for analyzing interaction logs and analysis history. By providing high-quality coded data along with interaction logs, it is possible to compare algorithmic data processing techniques to the ground-truth records of analysis history.","2018-01-16","2023-03-23 00:18:41","2023-05-23 01:29:29","2023-03-23 00:18:41","","","","","","","Analytic Provenance Datasets","","","","","","","en","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\DTP3XD26\Mohseni et al_2018_Analytic Provenance Datasets.pdf; ","notion://www.notion.so/Mohseni-et-al-2018-4d5f598fdf4e47db86b4eb8e9f9141fd","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WVSI64NA","webpage","2018","Mohseni, Sina; Zarei, Niloofar; Ragan, Eric D.","A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems","arXiv.org","","","","https://arxiv.org/abs/1811.11839v5","The need for interpretable and accountable intelligent systems grows along with the prevalence of artificial intelligence applications used in everyday life. Explainable intelligent systems are designed to self-explain the reasoning behind system decisions and predictions, and researchers from different disciplines work together to define, design, and evaluate interpretable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of interpretable machine learning research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this paper presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of interpretable machine learning design goals and evaluation methods to show a mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.","2018-11-28","2023-03-23 00:17:43","2023-05-23 01:29:28","2023-03-23 00:17:43","","","","","","","","","","","","","","en","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\XJY9FJQZ\Mohseni et al_2018_A Multidisciplinary Survey and Framework for Design and Evaluation of.pdf; ","notion://www.notion.so/Mohseni-et-al-2018-ad187c615aec46dfa87a260d2c8da20a","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZQQ95D8U","preprint","2020","Mohseni, Sina; Block, Jeremy E.; Ragan, Eric D.","A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning","","","","10.48550/arXiv.1801.05075","http://arxiv.org/abs/1801.05075","Research in interpretable machine learning proposes different computational and human subject approaches to evaluate model saliency explanations. These approaches measure different qualities of explanations to achieve diverse goals in designing interpretable machine learning systems. In this paper, we propose a human attention benchmark for image and text domains using multi-layer human attention masks aggregated from multiple human annotators. We then present an evaluation study to evaluate model saliency explanations obtained using Grad-cam and LIME techniques. We demonstrate our benchmark's utility for quantitative evaluation of model explanations by comparing it with human subjective ratings and ground-truth single-layer segmentation masks evaluations. Our study results show that our threshold agnostic evaluation method with the human attention baseline is more effective than single-layer object segmentation masks to ground truth. Our experiments also reveal user biases in the subjective rating of model saliency explanations.","2020-06-28","2023-03-22 23:46:23","2023-05-23 01:29:26","2023-03-22 23:46:23","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1801.05075 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\ZDXQQIIY\1801.html; C:\Users\ambreen.hanif\Zotero\storage\6764D29S\Mohseni et al_2020_A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning.pdf; ","notion://www.notion.so/Mohseni-et-al-2020-b2ab83b24b3c41bcb911bfbf6177ce49","notion","","","","","","","","","","","","","","","","","","","","arXiv:1801.05075","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V94UK4ML","journalArticle","2018","Adadi, Amina; Berrada, Mohammed","Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)","IEEE Access","","2169-3536","10.1109/ACCESS.2018.2870052","","At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.","2018","2023-03-22 23:43:43","2023-05-23 01:29:25","","52138-52160","","","6","","","Peeking Inside the Black-Box","","","","","","","","","","","","IEEE Xplore","","Conference Name: IEEE Access","","C:\Users\ambreen.hanif\Zotero\storage\AEHJE8H5\Adadi_Berrada_2018_Peeking Inside the Black-Box.pdf; C:\Users\ambreen.hanif\Zotero\storage\KC3HLB4E\8466590.html; ","notion://www.notion.so/Adadi-Berrada-2018-038db0e3b2bc47eb84e73cac3630e789","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8GGM4ZCL","preprint","2017","Goodfellow, Ian","NIPS 2016 Tutorial: Generative Adversarial Networks","","","","","http://arxiv.org/abs/1701.00160","This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.","2017-04-03","2023-03-22 19:37:34","2023-05-23 01:29:23","2023-03-22 19:37:34","","","","","","","NIPS 2016 Tutorial","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1701.00160 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\VV5DLIWQ\Goodfellow - 2017 - NIPS 2016 Tutorial Generative Adversarial Network.pdf; C:\Users\ambreen.hanif\Zotero\storage\4GBMAERZ\1701.html; ","notion://www.notion.so/Goodfellow-2017-8f53588f8648487b8a1064c5a823b43f","notion","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1701.00160","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8QUYZWKG","journalArticle","2021","Ma, Xiaoxiao; Wu, Jia; Xue, Shan; Yang, Jian; Zhou, Chuan; Sheng, Quan Z.; Xiong, Hui; Akoglu, Leman","A Comprehensive Survey on Graph Anomaly Detection with Deep Learning","IEEE Transactions on Knowledge and Data Engineering","","1041-4347, 1558-2191, 2326-3865","10.1109/TKDE.2021.3118815","https://ieeexplore.ieee.org/document/9565320/","—Anomalies are rare observations ( e.g., data records or events) that deviate signiﬁcantly from the others in the sample. Over the past few decades, research on anomaly mining has received increasing interests due to the implications of these occurrences in a wide range of disciplines - for instance, security, ﬁnance, and medicine. For this reason, anomaly detection, which aims to identify these rare observations, has become one of the most vital tasks in the world and has shown its power in preventing detrimental events, such as ﬁnancial fraud, network intrusions, and social spam. The detection task is typically solved by identifying outlying data points in the feature space, which, inherently, overlooks the relational information in real-world data. At the same time, graphs have been prevalently used to represent the structural/relational information, which raises the graph anomaly detection problem - identifying anomalous graph objects ( i.e., nodes, edges and sub-graphs) in a single graph, or anomalous graphs in a set/database of graphs. Conventional anomaly detection techniques cannot tackle this problem well because of the complexity of graph data ( e.g., irregular structures, relational dependencies, node/edge types/attributes/directions/multiplicities/weights, large scale, etc.). However, thanks to the advent of deep learning in breaking these limitations, graph anomaly detection with deep learning has received a growing attention recently. In this survey, we aim to provide a systematic and comprehensive review of the contemporary deep learning techniques for graph anomaly detection. Speciﬁcally, we provide a taxonomy that follows a task-driven strategy and categorizes existing work according to the anomalous graph objects that they can detect. We especially focus on the challenges in this research area and discuss the key intuitions, technical details as well as relative strengths and weaknesses of various techniques in each category. From the survey results, we highlight 12 future research directions spanning unsolved and emerging problems introduced by graph data, anomaly detection, deep learning and real-world applications. Additionally, to provide a wealth of useful resources for future studies, we have compiled a set of open-source implementations, public datasets, and commonly-used evaluation metrics. With this survey, our goal is to create a “one-stop-shop” that provides a uniﬁed understanding of the problem categories and existing approaches, publicly available hands-on resources, and high-impact open challenges for graph anomaly detection using deep learning.","2021","2023-03-22 01:59:07","2023-05-23 01:29:19","2023-03-22 01:59:07","1-1","","","","","IEEE Trans. Knowl. Data Eng.","","","","","","","","","","","","","Semantic Scholar","","","","C:\Users\ambreen.hanif\Zotero\storage\S42FXHT7\Ma et al_2021_A Comprehensive Survey on Graph Anomaly Detection with Deep Learning.pdf; ; ","notion://www.notion.so/Ma-et-al-2021-3c5f64af28654370a3ec16cd31a4ede9; https://www.semanticscholar.org/paper/A-Comprehensive-Survey-on-Graph-Anomaly-Detection-Ma-Wu/35067cc8153b7e6270797fffaefc5c9cefdfe515","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FAD5FGEE","journalArticle","","Lakkaraju, Himabindu; Slack, Dylan; Irvine, UC; Chen, Yuxin; Tan, Chenhao","Rethinking Explainability as a Dialogue: A Practitioner’s Perspective","","","","","","As practitioners increasingly deploy machine learning models in critical domains such as healthcare, ﬁnance, and policy, it becomes vital to ensure that domain experts function eﬀectively alongside these models. Explainability is one way to bridge the gap between human decision-makers and machine learning models. However, most of the existing work on explainability focuses on one-oﬀ, static explanations like feature importances or rule-lists. These sorts of explanations may not be suﬃcient for many use cases that require dynamic, continuous discovery from stakeholders that have a range of skills and expertise. In the literature, few works ask decision-makers such as doctors, healthcare professionals, and policymakers about the utility of existing explanations and other desiderata they would like to see in an explanation going forward. In this work, we address this gap and carry out a study where we interview doctors, healthcare professionals, and policymakers about their needs and desires for explanations. Our study indicates that decision-makers would strongly prefer interactive explanations. In particular, they would prefer these interactions to take the form of natural language dialogues. Domain experts wish to treat machine learning models as “another colleague”, i.e., one who can be held accountable by asking why they made a particular decision through expressive and accessible natural language interactions. Considering these needs, we outline a set of ﬁve principles researchers should follow when designing interactive explanations as a starting place for future work. Further, we show why natural language dialogues satisfy these principles and are a desirable way to build interactive explanations. Next, we provide a design of a dialogue system for explainability, and discuss the risks, trade-oﬀs, and research opportunities of building these systems. Overall, we hope our work serves as a starting place for researchers and engineers to design interactive, natural language dialogue systems for explainability that better serve users’ needs.","","2023-03-22 00:26:25","2023-05-23 01:29:17","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\46XSRYWG\Lakkaraju et al. - Rethinking Explainability as a Dialogue A Practit.pdf; ","notion://www.notion.so/Lakkaraju-et-al-n-d-aa3e0844e91d4e6582d11cba3dc9cbb9","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IJSV9FWX","journalArticle","2023","Saeed, Waddah; Omlin, Christian","Explainable AI (XAI): A systematic meta-survey of current challenges and future opportunities","Knowledge-Based Systems","","0950-7051","10.1016/j.knosys.2023.110273","https://www.sciencedirect.com/science/article/pii/S0950705123000230","The past decade has seen significant progress in artificial intelligence (AI), which has resulted in algorithms being adopted for resolving a variety of problems. However, this success has been met by increasing model complexity and employing black-box AI models that lack transparency. In response to this need, Explainable AI (XAI) has been proposed to make AI more transparent and thus advance the adoption of AI in critical domains. Although there are several reviews of XAI topics in the literature that have identified challenges and potential research directions of XAI, these challenges and research directions are scattered. This study, hence, presents a systematic meta-survey of challenges and future research directions in XAI organized in two themes: (1) general challenges and research directions of XAI and (2) challenges and research directions of XAI based on machine learning life cycle’s phases: design, development, and deployment. We believe that our meta-survey contributes to XAI literature by providing a guide for future exploration in the XAI area.","2023-03-05","2023-03-15 02:54:13","2023-05-23 01:29:15","2023-03-15 02:54:13","110273","","","263","","Knowledge-Based Systems","Explainable AI (XAI)","","","","","","","en","","","","","ScienceDirect","","","","; C:\Users\ambreen.hanif\Zotero\storage\HGJD22VC\Saeed_Omlin_2023_Explainable AI (XAI).pdf; C:\Users\ambreen.hanif\Zotero\storage\QRK4I3IN\S0950705123000230.html","notion://www.notion.so/Saeed-Omlin-2023-0bbec7211b154a4eb917ff032b5e9551","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ICM5QRT6","journalArticle","2017","Aery, Manish; Ram, Chet","A REVIEW ON MACHINE LEARNING: TRENDS AND FUTURE PROSPECTS","","","","","","Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today's most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.","2017-11-01","2023-03-13 07:06:18","2023-05-23 01:29:13","","","","","","","","A REVIEW ON MACHINE LEARNING","","","","","","","","","","","","ResearchGate","","","","C:\Users\ambreen.hanif\Zotero\storage\96PQJLS3\Aery and Ram - 2017 - A REVIEW ON MACHINE LEARNING TRENDS AND FUTURE PR.pdf; ; ","notion://www.notion.so/Aery-Ram-2017-52d39466db5148ad89916de172b674ca; https://www.researchgate.net/publication/323377718_A_REVIEW_ON_MACHINE_LEARNING_TRENDS_AND_FUTURE_PROSPECTS","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"47H93DVN","webpage","","","[PDF] Machine learning: Trends, perspectives, and prospects | Semantic Scholar","","","","","https://www.semanticscholar.org/paper/Machine-learning%3A-Trends%2C-perspectives%2C-and-Jordan-Mitchell/d422df8bff4e677a3077635db116679d25142bfc","","","2023-03-13 07:06:40","2023-05-23 01:29:10","2023-03-13 07:06:40","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\EZJ4CH8D\d422df8bff4e677a3077635db116679d25142bfc.html; ","notion://www.notion.so/PDF-Machine-Learning-Trends-Perspectives-and-Prospects-Semantic-Scholar-n-d-b52ed6668088473eaaa8a8ce714324d7","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CL9TMKXJ","preprint","2023","Sokol, Kacper; Flach, Peter","LIMEtree: Consistent and Faithful Surrogate Explanations of Multiple Classes","","","","","http://arxiv.org/abs/2005.01427","Explainable machine learning provides tools to better understand predictive models and their decisions, but many such methods are limited to producing insights with respect to a single class. When generating explanations for several classes, reasoning over them to obtain a complete view may be difficult since they can present competing or contradictory evidence. To address this issue we introduce a novel paradigm of multi-class explanations. We outline the theory behind such techniques and propose a local surrogate model based on multi-output regression trees -- called LIMEtree -- which offers faithful and consistent explanations of multiple classes for individual predictions while being post-hoc, model-agnostic and data-universal. In addition to strong fidelity guarantees, our implementation supports (interactive) customisation of the explanatory insights and delivers a range of diverse explanation types, including counterfactual statements favoured in the literature. We evaluate our algorithm with a collection of quantitative experiments, a qualitative analysis based on explainability desiderata and a preliminary user study on an image classification task, comparing it to LIME. Our contributions demonstrate the benefits of multi-class explanations and wide-ranging advantages of our method across a diverse set scenarios.","2023-02-10","2023-03-14 04:18:26","2023-05-23 01:29:09","2023-03-14 04:18:26","","","","","","","LIMEtree","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2005.01427 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\IHQ4EU6G\2005.html; ; C:\Users\ambreen.hanif\Zotero\storage\FI3FU44E\Sokol_Flach_2023_LIMEtree.pdf","notion://www.notion.so/Sokol-Flach-2023-aa6894e0bb2c4c3c918991b0632beeab","notion","","","","","","","","","","","","","","","","","","","","arXiv:2005.01427","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"32ULWS49","journalArticle","2022","Charmet, Fabien; Tanuwidjaja, Harry Chandra; Ayoubi, Solayman; Gimenez, Pierre-François; Han, Yufei; Jmila, Houda; Blanc, Gregory; Takahashi, Takeshi; Zhang, Zonghua","Explainable artificial intelligence for cybersecurity: a literature survey","Annals of Telecommunications","","1958-9395","10.1007/s12243-022-00926-7","https://doi.org/10.1007/s12243-022-00926-7","With the extensive application of deep learning (DL) algorithms in recent years, e.g., for detecting Android malware or vulnerable source code, artificial intelligence (AI) and machine learning (ML) are increasingly becoming essential in the development of cybersecurity solutions. However, sharing the same fundamental limitation with other DL application domains, such as computer vision (CV) and natural language processing (NLP), AI-based cybersecurity solutions are incapable of justifying the results (ranging from detection and prediction to reasoning and decision-making) and making them understandable to humans. Consequently, explainable AI (XAI) has emerged as a paramount topic addressing the related challenges of making AI models explainable or interpretable to human users. It is particularly relevant in cybersecurity domain, in that XAI may allow security operators, who are overwhelmed with tens of thousands of security alerts per day (most of which are false positives), to better assess the potential threats and reduce alert fatigue. We conduct an extensive literature review on the intersection between XAI and cybersecurity. Particularly, we investigate the existing literature from two perspectives: the applications of XAI to cybersecurity (e.g., intrusion detection, malware classification), and the security of XAI (e.g., attacks on XAI pipelines, potential countermeasures). We characterize the security of XAI with several security properties that have been discussed in the literature. We also formulate open questions that are either unanswered or insufficiently addressed in the literature, and discuss future directions of research.","2022-12-01","2023-03-10 06:47:44","2023-05-23 01:29:06","2023-03-10 06:47:44","789-812","","11","77","","Ann. Telecommun.","Explainable artificial intelligence for cybersecurity","","","","","","","en","","","","","Springer Link","","","","C:\Users\ambreen.hanif\Zotero\storage\G6IIDHKV\Charmet et al_2022_Explainable artificial intelligence for cybersecurity.pdf; ","notion://www.notion.so/Charmet-et-al-2022-d929278a58814049b139ea7d26fdb191","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PM3GH9N8","report","2023","Kolajo, Taiwo; Daramola, Olawande","Human-centric and Semantics-based Explainable Event Detection: A Survey","","","","","https://www.researchsquare.com/article/rs-2639603/v1","Abstract           In recent years, there has been a surge in interest in artificial intelligent systems that can provide human-centric explanations for decisions or predictions. No matter how good and efficient a model is, users or practitioners find it difficult to trust such model if they cannot understand the model or its behaviours. Incorporating explainability that is human-centric in event detection systems is significant for building a decision-making process that is more trustworthy and sustainable. Human-centric and semantics-based explainable event detection will achieve trustworthiness, explainability, and reliability, which are currently lacking in AI systems. This paper provides a survey on the human-centric explainable AI, explainable event detection, and semantics-based explainable event detection by answering some research questions that bother on the characteristics of human-centric explanations, the state of explainable AI, methods for human-centric explanations, the essence of human-centricity in explainable event detection, research efforts in explainable event solutions, and the benefits of integrating semantics into explainable event detection. The findings from the survey show the current state of human-centric explainability, the potential of integrating semantics into explainable AI, the open problems, and the future directions which can serve as steppingstones for researchers in the explainable AI domain.","2023-03-06","2023-03-09 22:33:05","2023-05-23 01:29:03","2023-03-09 22:33:04","","","","","","","Human-centric and Semantics-based Explainable Event Detection","","","","","In Review","","en","","preprint","","","DOI.org (Crossref)","","DOI: 10.21203/rs.3.rs-2639603/v1","","C:\Users\ambreen.hanif\Zotero\storage\VH5Q7J8U\Kolajo and Daramola - 2023 - Human-centric and Semantics-based Explainable Even.pdf; ","notion://www.notion.so/Kolajo-Daramola-2023-d4e569be3c714211a8a17533de0caa5b","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FCH69G9J","webpage","2023","","Jekyll on Windows","Jekyll • Simple, blog-aware, static sites","","","","https://jekyllrb.com/docs/installation/windows/","While Windows is not an officially-supported platform, it can be used to run Jekyll with the proper tweaks.","2023-03-01","2023-03-10 00:51:03","2023-05-23 01:29:00","2023-03-10 00:51:03","","","","","","","","","","","","","","en-US","","","","","","","","","","notion://www.notion.so/Jekyll-on-Windows-2023-b9f2948d85f04972b8a215ea5321cdf3","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9PNWL8K8","journalArticle","2022","Marikyan, Davit; Papagiannidis, Savvas; Rana, Omer F.; Ranjan, Rajiv; Morgan, Graham","“Alexa, let’s talk about my productivity”: The impact of digital assistants on work productivity","Journal of Business Research","","01482963","10.1016/j.jbusres.2022.01.015","https://linkinghub.elsevier.com/retrieve/pii/S014829632200025X","Digital assistants based on artificial intelligence (AI) have been increasingly used in contexts beyond homeoriented services to support individuals in carrying out work-related tasks. Given the lack of empirical evi­ dence on this fast-developing area, this paper aims (1) to explore the factors which can lead to individuals’ satisfaction with the use of technology, and (2) to examine the impact of satisfaction on productivity and job engagement. The model was tested using 536 responses from individuals who used digital assistants for work purposes. Results showed that performance expectancy, perceived enjoyment, intelligence, social presence and trust were positively related to satisfaction with digital assistants. Satisfaction with the digital assistants was found to correlate with productivity and engagement. The findings contribute to the literature focusing on the use of AI-based technology supporting and complementing work tasks. They also offer practical recommenda­ tions as to how digital assistants could be used in the workplace.","2022-03","2023-03-07 05:20:28","2023-05-23 01:28:59","2023-03-07 05:20:28","572-584","","","142","","Journal of Business Research","“Alexa, let’s talk about my productivity”","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\ambreen.hanif\Zotero\storage\APGP3FU3\Marikyan et al. - 2022 - “Alexa, let’s talk about my productivity” The imp.pdf; ","notion://www.notion.so/Marikyan-et-al-2022-1cf4d3dea04a415ea0c2c33f7919d0c9","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AZIV4FCE","preprint","2021","Rao, Jiahua; Zheng, Shuangjia; Yang, Yuedong","Quantitative Evaluation of Explainable Graph Neural Networks for Molecular Property Prediction","","","","10.48550/arXiv.2107.04119","http://arxiv.org/abs/2107.04119","Advances in machine learning have led to graph neural network-based methods for drug discovery, yielding promising results in molecular design, chemical synthesis planning, and molecular property prediction. However, current graph neural networks (GNNs) remain of limited acceptance in drug discovery is limited due to their lack of interpretability. Although this major weakness has been mitigated by the development of explainable artificial intelligence (XAI) techniques, the ""ground truth"" assignment in most explainable tasks ultimately rests with subjective judgments by humans so that the quality of model interpretation is hard to evaluate in quantity. In this work, we first build three levels of benchmark datasets to quantitatively assess the interpretability of the state-of-the-art GNN models. Then we implemented recent XAI methods in combination with different GNN algorithms to highlight the benefits, limitations, and future opportunities for drug discovery. As a result, GradInput and IG generally provide the best model interpretability for GNNs, especially when combined with GraphNet and CMPNN. The integrated and developed XAI package is fully open-sourced and can be used by practitioners to train new models on other drug discovery tasks.","2021-07-12","2023-03-07 03:47:56","2023-05-23 01:28:57","2023-03-07 03:47:56","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2107.04119 [cs, q-bio]","","C:\Users\ambreen.hanif\Zotero\storage\DZLDVQT4\2107.html; ; C:\Users\ambreen.hanif\Zotero\storage\5GB8RDKU\Rao et al_2021_Quantitative Evaluation of Explainable Graph Neural Networks for Molecular.pdf","notion://www.notion.so/Rao-et-al-2021-f415e060c7aa4062b32727cbc17839bc","notion","","","","","","","","","","","","","","","","","","","","arXiv:2107.04119","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9YLTU4JF","webpage","","","[2107.04119] Quantitative Evaluation of Explainable Graph Neural Networks for Molecular Property Prediction","","","","","https://arxiv.org/abs/2107.04119","","","2023-03-07 03:20:48","2023-05-23 01:28:55","2023-03-07 03:20:48","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\D8CY5FWN\2107.html; ","notion://www.notion.so/2107-04119-Quantitative-Evaluation-of-Explainable-Graph-Neural-Networks-for-Molecular-Property-Pred-8d61f710e67f4bc4a5cc0a26dc54cb86","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MVXMEJM7","journalArticle","2022","Schnake, Thomas; Eberle, Oliver; Lederer, Jonas; Nakajima, Shinichi; Schütt, Kristof T.; Müller, Klaus-Robert; Montavon, Grégoire","Higher-Order Explanations of Graph Neural Networks via Relevant Walks","IEEE Transactions on Pattern Analysis and Machine Intelligence","","0162-8828, 2160-9292, 1939-3539","10.1109/TPAMI.2021.3115452","http://arxiv.org/abs/2006.03589","Graph Neural Networks (GNNs) are a popular approach for predicting graph structured data. As GNNs tightly entangle the input graph into the neural network structure, common explainable AI approaches are not applicable. To a large extent, GNNs have remained black-boxes for the user so far. In this paper, we show that GNNs can in fact be naturally explained using higher-order expansions, i.e. by identifying groups of edges that jointly contribute to the prediction. Practically, we find that such explanations can be extracted using a nested attribution scheme, where existing techniques such as layer-wise relevance propagation (LRP) can be applied at each step. The output is a collection of walks into the input graph that are relevant for the prediction. Our novel explanation method, which we denote by GNN-LRP, is applicable to a broad range of graph neural networks and lets us extract practically relevant insights on sentiment analysis of text data, structure-property relationships in quantum chemistry, and image classification.","2022-11-01","2023-03-07 03:20:14","2023-05-23 01:28:46","2023-03-07 03:20:14","7581-7596","","11","44","","IEEE Trans. Pattern Anal. Mach. Intell.","","","","","","","","","","","","","arXiv.org","","arXiv:2006.03589 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\VZAEJGSW\2006.html; ; C:\Users\ambreen.hanif\Zotero\storage\XQ9UN46J\Schnake et al_2022_Higher-Order Explanations of Graph Neural Networks via Relevant Walks.pdf","notion://www.notion.so/Schnake-et-al-2022-8903e648756b41d29bf40bd15e997291","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4FWGLNPG","webpage","","","[2006.03589v1] XAI for Graphs: Explaining Graph Neural Network Predictions by Identifying Relevant Walks","","","","","https://arxiv.org/abs/2006.03589v1","","","2023-03-07 03:19:48","2023-05-23 01:28:45","2023-03-07 03:19:48","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\38LB6LHQ\2006.html; ","notion://www.notion.so/2006-03589v1-XAI-for-Graphs-Explaining-Graph-Neural-Network-Predictions-by-Identifying-Relevant-Wa-f6bebfa1487a42b0a4580fd6e19f58c8","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WUTN924X","conferencePaper","2021","Wang, Xiang; Wu, Yingxin; Zhang, An; He, Xiangnan; Chua, Tat-Seng","Towards Multi-Grained Explainability for Graph Neural Networks","Advances in Neural Information Processing Systems","","","","https://proceedings.neurips.cc/paper/2021/hash/99bcfcd754a98ce89cb86f73acc04645-Abstract.html","When a graph neural network (GNN) made a prediction, one raises question about explainability: “Which fraction of the input graph is most inﬂuential to the model’s decision?” Producing an answer requires understanding the model’s inner workings in general and emphasizing the insights on the decision for the instance at hand. Nonetheless, most of current approaches focus only on one aspect: (1) local explainability, which explains each instance independently, thus hardly exhibits the class-wise patterns; and (2) global explainability, which systematizes the globally important patterns, but might be trivial in the local context. This dichotomy limits the ﬂexibility and effectiveness of explainers greatly. A performant paradigm towards multi-grained explainability is until-now lacking and thus a focus of our work. In this work, we exploit the pre-training and ﬁne-tuning idea to develop our explainer and generate multi-grained explanations. Speciﬁcally, the pre-training phase accounts for the contrastivity among different classes, so as to highlight the class-wise characteristics from a global view; afterwards, the ﬁne-tuning phase adapts the explanations in the local context. Experiments on both synthetic and real-world datasets show the superiority of our explainer, in terms of AUC on explaining graph classiﬁcation over the leading baselines. Our codes and datasets are available at https://github.com/Wuyxin/ReFine.","2021","2023-03-06 04:26:34","2023-05-23 01:28:43","2023-03-06 04:26:34","18446–18458","","","34","","","","","","","","Curran Associates, Inc.","","","","","","","Neural Information Processing Systems","","","","; C:\Users\ambreen.hanif\Zotero\storage\PLR2JB5M\Wang et al_2021_Towards Multi-Grained Explainability for Graph Neural Networks.pdf","notion://www.notion.so/Wang-et-al-2021-11b742393e3a44a88f6f274ff6058f0e","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WVKFRXAJ","book","2020","Yuan, Hao; Tang, Jiliang; Hu, Xia; Ji, Shuiwang","XGNN: Towards Model-Level Explanations of Graph Neural Networks","","","","","","Graphs neural networks (GNNs) learn node features by aggregating and combining neighbor information, which have achieved promising performance on many graph tasks. However, GNNs are mostly treated as black-boxes and lack human intelligible explanations. Thus, they cannot be fully trusted and used in certain application domains if GNN models cannot be explained. In this work, we propose a novel approach, known as XGNN, to interpret GNNs at the model-level. Our approach can provide high-level insights and generic understanding of how GNNs work. In particular, we propose to explain GNNs by training a graph generator so that the generated graph patterns maximize a certain prediction of the model.We formulate the graph generation as a reinforcement learning task, where for each step, the graph generator predicts how to add an edge into the current graph. The graph generator is trained via a policy gradient method based on information from the trained GNNs. In addition, we incorporate several graph rules to encourage the generated graphs to be valid. Experimental results on both synthetic and real-world datasets show that our proposed methods help understand and verify the trained GNNs. Furthermore, our experimental results indicate that the generated graphs can provide guidance on how to improve the trained GNNs.","2020-06-03","2023-03-06 04:09:05","2023-05-23 01:28:37","","","","","","","","XGNN","","","","","","","","","","","","ResearchGate","","","","; ; C:\Users\ambreen.hanif\Zotero\storage\NPR2E9JP\Yuan et al_2020_XGNN.pdf","notion://www.notion.so/Yuan-et-al-2020-e542d2a3254c4ebb8fdcf2f0c7e1282e; https://www.researchgate.net/publication/341927125_XGNN_Towards_Model-Level_Explanations_of_Graph_Neural_Networks","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VLGHF2HP","book","2022","Prado-Romero, Mario; Prenkaj, Bardh; Stilo, Giovanni; Giannotti, Fosca","A Survey on Graph Counterfactual Explanations: Definitions, Methods, Evaluation","","","","","","In recent years, Graph Neural Networks have reported outstanding performance in tasks like community detection, molecule classification and link prediction. However, the black-box nature of these models prevents their application in domains like health and finance, where understanding the models' decisions is essential. Counterfactual Explanations (CE) provide these understandings through examples. Moreover, the literature on CE is flourishing with novel explanation methods which are tailored to graph learning. In this survey, we analyse the existing Graph Counterfactual Explanation methods, by providing the reader with an organisation of the literature according to a uniform formal notation for definitions, datasets, and metrics, thus, simplifying potential comparisons w.r.t to the method advantages and disadvantages. We discussed seven methods and sixteen synthetic and real datasets providing details on the possible generation strategies. We highlight the most common evaluation strategies and formalise nine of the metrics used in the literature. We first introduce the evaluation framework GRETEL and how it is possible to extend and use it while providing a further dimension of comparison encompassing reproducibility aspects. Finally, we provide a discussion on how counterfactual explanation interplays with privacy and fairness, before delving into open challenges and future works.","2022-10-21","2023-03-06 04:08:34","2023-05-23 01:28:33","","","","","","","","A Survey on Graph Counterfactual Explanations","","","","","","","","","","","","ResearchGate","","","","; C:\Users\ambreen.hanif\Zotero\storage\TC93U9BC\Prado-Romero et al_2022_A Survey on Graph Counterfactual Explanations.pdf; ","notion://www.notion.so/Prado-Romero-et-al-2022-1f05e510c852466a93d42454c8628551; https://www.researchgate.net/publication/364658767_A_Survey_on_Graph_Counterfactual_Explanations_Definitions_Methods_Evaluation","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"38DRHSVI","conferencePaper","2021","Dai, Enyan; Wang, Suhang","Towards Self-Explainable Graph Neural Network","Proceedings of the 30th ACM International Conference on Information & Knowledge Management","978-1-4503-8446-9","","10.1145/3459637.3482306","https://doi.org/10.1145/3459637.3482306","Graph Neural Networks (GNNs), which generalize the deep neural networks to graph-structured data, have achieved great success in modeling graphs. However, as an extension of deep learning for graphs, GNNs lack explainability, which largely limits their adoption in scenarios that demand the transparency of models. Though many efforts are taken to improve the explainability of deep learning, they mainly focus on i.i.d data, which cannot be directly applied to explain the predictions of GNNs because GNNs utilize both node features and graph topology to make predictions. There are only very few work on the explainability of GNNs and they focus on post-hoc explanations. Since post-hoc explanations are not directly obtained from the GNNs, they can be biased and misrepresent the true explanations. Therefore, in this paper, we study a novel problem of self-explainable GNNs which can simultaneously give predictions and explanations. We propose a new framework which can find K-nearest labeled nodes for each unlabeled node to give explainable node classification, where nearest labeled nodes are found by interpretable similarity module in terms of both node similarity and local structure similarity. Extensive experiments on real-world and synthetic datasets demonstrate the effectiveness of the proposed framework for explainable node classification.","2021-10-30","2023-03-06 04:00:57","2023-05-23 01:28:31","2023-03-05","302–311","","","","","","","CIKM '21","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","C:\Users\ambreen.hanif\Zotero\storage\MK37V4V5\Dai_Wang_2021_Towards Self-Explainable Graph Neural Network.pdf; ","notion://www.notion.so/Dai-Wang-2021-92003ee239d74178b2765b72199297b9","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"884ZRGDK","preprint","2022","Li, Peibo; Yang, Yixing; Pagnucco, Maurice; Song, Yang","Explainability in Graph Neural Networks: An Experimental Survey","","","","10.48550/arXiv.2203.09258","http://arxiv.org/abs/2203.09258","Graph neural networks (GNNs) have been extensively developed for graph representation learning in various application domains. However, similar to all other neural networks models, GNNs suffer from the black-box problem as people cannot understand the mechanism underlying them. To solve this problem, several GNN explainability methods have been proposed to explain the decisions made by GNNs. In this survey, we give an overview of the state-of-the-art GNN explainability methods and how they are evaluated. Furthermore, we propose a new evaluation metric and conduct thorough experiments to compare GNN explainability methods on real world datasets. We also suggest future directions for GNN explainability.","2022-03-17","2023-03-06 04:00:20","2023-05-23 01:28:27","2023-03-06 04:00:20","","","","","","","Explainability in Graph Neural Networks","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2203.09258 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\GQGU3EZ4\Li et al. - 2022 - Explainability in Graph Neural Networks An Experi.pdf; C:\Users\ambreen.hanif\Zotero\storage\95VCK54S\2203.html; ","notion://www.notion.so/Li-et-al-2022-4fc5b5ab17f24b92bb928ee30f2dbbbb","notion","","","","","","","","","","","","","","","","","","","","arXiv:2203.09258","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QL7DE2MV","preprint","2022","Ouyang, Long; Wu, Jeff; Jiang, Xu; Almeida, Diogo; Wainwright, Carroll L.; Mishkin, Pamela; Zhang, Chong; Agarwal, Sandhini; Slama, Katarina; Ray, Alex; Schulman, John; Hilton, Jacob; Kelton, Fraser; Miller, Luke; Simens, Maddie; Askell, Amanda; Welinder, Peter; Christiano, Paul; Leike, Jan; Lowe, Ryan","Training language models to follow instructions with human feedback","","","","","http://arxiv.org/abs/2203.02155","Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.","2022-03-04","2023-02-13 04:25:42","2023-05-23 01:28:19","2023-02-13 04:25:42","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2203.02155 [cs] version: 1","","C:\Users\ambreen.hanif\Zotero\storage\4TLCZZRZ\Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf; C:\Users\ambreen.hanif\Zotero\storage\BWSL9RKP\2203.html; ","notion://www.notion.so/Ouyang-et-al-2022-afd04454c0364c03851013a9e1afbb23","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2203.02155","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NY7QDPAL","book","2022","Beheshti, Amin; Ghodratnama, Samira; Elahi, Mehdi; Farhood, Helia","Social Data Analytics","","978-1-00-326014-1","","","","","2022","2023-01-31 05:27:58","2023-05-23 01:28:14","","","","","","","","","","","","","CRC Press","","","","","","","","","","","","notion://www.notion.so/Beheshti-et-al-2022-77c8cd89a128407b9df8b73d49ce61c8","notion","","","","","","","","","","","","","","","","","","","","","1st","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FPM3TMWP","conferencePaper","2022","Hanif, Ambreen; Beheshti, Amin; Benatallah, Boualem; Zhang, Xuyun; Wood, Steven","Evidence Based Pipeline for Explaining Artificial Intelligence Algorithms with Interactions","9th {IEEE} International Conference on Data Science and Advanced Analytics,                {DSAA} 2022","","","","","","2022-10-13","2023-01-31 03:38:53","2023-05-23 01:28:11","","1--9","","","","","","","","","","","IEEE","Online","","","","","","","","","","","notion://www.notion.so/Hanif-et-al-2022-f53617a3ca85461ab5410d99cd3503a3","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Data Science and Advanced Analytics","","","","","","","","","","","","","","",""
"73BMLBGE","conferencePaper","2021","Hanif, Ambreen; Zhang, Xuyun; Wood, Steven","A Survey on Explainable Artificial Intelligence Techniques and Challenges","2021 IEEE 25th International Enterprise Distributed Object Computing Workshop (EDOCW)","","","10.1109/EDOCW52865.2021.00036","","In the last decade, the world has envisioned tremendous growth in technology with improved accessibility of data, cloud-computing resources, and the evolution of machine learning (ML) algorithms. The intelligent system has achieved significant performance with this growth. The state-of-the-art performance of these algorithms in various domains has increased the popularity of artificial intelligence (AI). However, alongside these achievements, the non-transparency, inscrutability and inability to expound and interpret the majority of the state-of-the-art techniques are considered an ethical issue. These flaws in AI algorithms impede the acceptance of complex ML models in a variety of fields such as medical, banking and finance, security, and education. These shortcomings have prompted many concerns about the security and safety of ML system users. These systems must be transparent, according to the current regulations and policies, in order to meet the right to explanation. Due to a lack of trust in existing ML-based systems, explainable artificial intelligence (XAI)-based methods are gaining popularity. Although neither the domain nor the methods are novel, they are gaining popularity due to their ability to unbox the black box. The explainable AI methods are of varying strengths, and they are capable of providing insights to the system. These insights can be ranging from a single feature explanation to the interpretability of sophisticated ML architecture. In this paper, we present a survey of known techniques in the field of XAI. Moreover, we suggest future research routes for developing AI systems that can be responsible. We emphasize the necessity of human knowledge-oriented systems for adopting AI in real-world applications with trust and high fidelity.","2021-10","2023-01-31 01:25:21","2023-05-23 01:28:03","","81-89","","","","","","","","","","","IEEE","","","","","","","IEEE Xplore","","ISSN: 2325-6605","","C:\Users\ambreen.hanif\Zotero\storage\BB2ZF43W\Hanif et al_2021_A Survey on Explainable Artificial Intelligence Techniques and Challenges.pdf; C:\Users\ambreen.hanif\Zotero\storage\2KFWNCWU\9626294.html; ","notion://www.notion.so/Hanif-et-al-2021-79623048ec7e499e9a5b7c5ffb4c4c4a","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2021 IEEE 25th International Enterprise Distributed Object Computing Workshop (EDOCW)","","","","","","","","","","","","","","",""
"JC9T3IIP","conferencePaper","2015","Nguyen, Anh; Yosinski, Jason; Clune, Jeff","Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images","Proceedings of the IEEE conference on computer vision and pattern recognition","","","10.48550/arXiv.1412.1897","https://arxiv.org/abs/1412.1897v4","Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call ""fooling images"" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.","2015","2023-01-30 01:26:04","2023-05-23 01:27:56","2023-01-30 01:26:04","427-436","","","","","","Deep Neural Networks are Easily Fooled","","","","","IEEE","Boston, MA, USA","en","","","","","arxiv.org","","","","C:\Users\ambreen.hanif\Zotero\storage\ZJ4F7BYM\Nguyen et al_2014_Deep Neural Networks are Easily Fooled.pdf; ","notion://www.notion.so/Nguyen-et-al-2015-c5f0660b97d74a669480711755d4e1ed","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LXHFZFQB","journalArticle","2013","Butler, Declan","When Google got flu wrong","Nature","","1476-4687","10.1038/494155a","https://www.nature.com/articles/494155a","US outbreak foxes a leading web-based method for tracking seasonal flu.","2013-02-01","2023-01-30 01:24:44","2023-05-23 01:27:55","2023-01-30 01:24:44","155-156","","7436","494","","","","","","","","","","en","","","","","www-nature-com.simsrad.net.ocs.mq.edu.au","","Number: 7436 Publisher: Nature Publishing Group","","C:\Users\ambreen.hanif\Zotero\storage\ARTTHZS7\Butler_2013_When Google got flu wrong.pdf; ; C:\Users\ambreen.hanif\Zotero\storage\MLNLIIGT\494155a.html","notion://www.notion.so/Butler-2013-8f2392fc2ddc49ca86f80737afd38740","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"55HMGC4B","webpage","","","When Google got flu wrong | Nature","","","","","https://www-nature-com.simsrad.net.ocs.mq.edu.au/articles/494155a","","","2023-01-30 01:24:32","2023-05-23 01:27:52","2023-01-30 01:24:32","","","","","","","","","","","","","","","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\UWK87SML\494155a.html","notion://www.notion.so/When-Google-Got-Flu-Wrong-Nature-n-d-bd4d2aeb15f24c7eb6e99ef56bde42e5","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5M82TJWX","journalArticle","2019","Carvalho, Diogo V.; Pereira, Eduardo M.; Cardoso, Jaime S.","Machine Learning Interpretability: A Survey on Methods and Metrics","Electronics","","2079-9292","10.3390/electronics8080832","https://www.mdpi.com/2079-9292/8/8/832","Machine learning systems are becoming increasingly ubiquitous. These systems’s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.","2019-08","2023-01-30 01:12:22","2023-05-23 01:27:49","2023-01-30 01:12:22","832","","8","8","","","Machine Learning Interpretability","","","","","","","en","http://creativecommons.org/licenses/by/3.0/","","","","www.mdpi.com","","Number: 8 Publisher: Multidisciplinary Digital Publishing Institute","","C:\Users\ambreen.hanif\Zotero\storage\FV3NNYPL\Carvalho et al_2019_Machine Learning Interpretability.pdf; ","notion://www.notion.so/Carvalho-et-al-2019-02561344be184c6fac269cd05e2427f4","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PCSPEGPT","thesis","2022","Collaris, Dennis","Interactive Visualization for Interpretable Machine Learning","","","","","https://research.tue.nl/en/publications/interactive-visualization-for-interpretable-machine-learning","","2022","2023-01-30 00:56:55","2023-05-23 01:27:47","2023-01-30 00:56:55","","","","","","","","","","","","Eindhoven University of Technology","","en","","","","","DOI.org (Datacite)","","DOI: 10.6100/51C5A54C-713B-498F-A0F3-816A1BF3BDB6","","C:\Users\ambreen.hanif\Zotero\storage\3F7VFB3A\20220708_Collaris_hf.pdf; ","notion://www.notion.so/cite-collaris_interactive_2022-6d237410b29d4fc48440dda0340e1d04","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NBU2VGJH","journalArticle","2020","Chatzimparmpas, A.; Martins, R.; Jusufi, I.; Kucher, K.; Rossi, Fabrice; Kerren, A.","The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations","Computer Graphics Forum","","0167-7055, 1467-8659","10.1111/cgf.14034","http://arxiv.org/abs/2212.11737","Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.","2020-06","2023-01-30 00:36:28","2023-05-23 01:27:38","2023-01-30 00:36:28","713-756","","3","39","","Computer Graphics Forum","","","","","","","","","","","","","arXiv.org","","arXiv:2212.11737 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\W4CXX5D6\2212.html; C:\Users\ambreen.hanif\Zotero\storage\EHFT9NIV\Chatzimparmpas et al_2020_The State of the Art in Enhancing Trust in Machine Learning Models with the Use.pdf; ","notion://www.notion.so/Chatzimparmpas-et-al-2020-c4d691c9014e4b7bbb742c20e574a571","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DGBVKHXQ","thesis","2015","Kim, Been","Interactive and interpretable machine learning models for human machine collaboration","","","","","https://dspace.mit.edu/handle/1721.1/98680","I envision a system that enables successful collaborations between humans and machine learning models by harnessing the relative strength to accomplish what neither can do alone. Machine learning techniques and humans have skills that complement each other - machine learning techniques are good at computation on data at the lowest level of granularity, whereas people are better at abstracting knowledge from their experience, and transferring the knowledge across domains. The goal of this thesis is to develop a framework for human-in-the-loop machine learning that enables people to interact effectively with machine learning models to make better decisions, without requiring in-depth knowledge about machine learning techniques. Many of us interact with machine learning systems everyday. Systems that mine data for product recommendations, for example, are ubiquitous. However these systems compute their output without end-user involvement, and there are typically no life or death consequences in the case the machine learning result is not acceptable to the user. In contrast, domains where decisions can have serious consequences (e.g., emergency response panning, medical decision-making), require the incorporation of human experts' domain knowledge. These systems also must be transparent to earn experts' trust and be adopted in their workflow. The challenge addressed in this thesis is that traditional machine learning systems are not designed to extract domain experts' knowledge from natural workflow, or to provide pathways for the human domain expert to directly interact with the algorithm to interject their knowledge or to better understand the system output. For machine learning systems to make a real-world impact in these important domains, these systems must be able to communicate with highly skilled human experts to leverage their judgment and expertise, and share useful information or patterns from the data. In this thesis, I bridge this gap by building human-in-the-loop machine learning models and systems that compute and communicate machine learning results in ways that are compatible with the human decision-making process, and that can readily incorporate human experts' domain knowledge. I start by building a machine learning model that infers human teams' planning decisions from the structured form of natural language of team meetings. I show that the model can infer a human teams' final plan with 86% accuracy on average. I then design an interpretable machine learning model then ""makes sense to humans"" by exploring and communicating patterns and structure in data to support human decision-making. Through human subject experiments, I show that this interpretable machine learning model offers statistically significant quantitative improvements in interpretability while preserving clustering performance. Finally, I design a machine learning model that supports transparent interaction with humans without requiring that a user has expert knowledge of machine learning technique. I build a human-in-the-loop machine learning system that incorporates human feedback and communicates its internal states to humans, using an intuitive medium for interaction with the machine learning model. I demonstrate the application of this model for an educational domain in which teachers cluster programming assignments to streamline the grading process.","2015","2023-01-30 00:33:26","2023-05-23 01:27:29","2023-01-30 00:33:26","","","","","","","","","","","","Massachusetts Institute of Technology","","eng","M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided URL for inquiries about permission.","Thesis","","","dspace.mit.edu","","Accepted: 2015-09-17T19:04:25Z","","C:\Users\ambreen.hanif\Zotero\storage\IMJ87T6V\Kim_2015_Interactive and interpretable machine learning models for human machine.pdf; ","notion://www.notion.so/Kim-2015-0de774bbcee0412f86285844681ee887","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z9M6YSPN","preprint","2023","Pan, Shirui; Luo, Linhao; Wang, Yufei; Chen, Chen; Wang, Jiapu; Wu, Xindong","Unifying Large Language Models and Knowledge Graphs: A Roadmap","","","","","http://arxiv.org/abs/2306.08302","Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.","2023-06-14","2023-06-19 08:10:41","2023-06-19 08:13:16","2023-06-19 08:10:41","","","","","","","Unifying Large Language Models and Knowledge Graphs","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2306.08302 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\94I43MG4\2306.html; ","notion://www.notion.so/cite-pan_unifying_2023-b3eeb62496e44a87821529c789847661","notion","","","","","","","","","","","","","","","","","","","","arXiv:2306.08302","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7GXJ69XN","conferencePaper","2016","Vartak, Manasi; Subramanyam, Harihar; Lee, Wei-En; Viswanathan, Srinidhi; Husnoo, Saadiyah; Madden, Samuel; Zaharia, Matei","ModelDB: a system for machine learning model management","Proceedings of the Workshop on Human-In-the-Loop Data Analytics","978-1-4503-4207-0","","10.1145/2939502.2939516","https://dl.acm.org/doi/10.1145/2939502.2939516","Building a machine learning model is an iterative process. A data scientist will build many tens to hundreds of models before arriving at one that meets some acceptance criteria (e.g. AUC cutoff, accuracy threshold). However, the current style of model building is ad-hoc and there is no practical way for a data scientist to manage models that are built over time. As a result, the data scientist must attempt to ""remember"" previously constructed models and insights obtained from them. This task is challenging for more than a handful of models and can hamper the process of sensemaking. Without a means to manage models, there is no easy way for a data scientist to answer questions such as ""Which models were built using an incorrect feature?"", ""Which model performed best on American customers?"" or ""How did the two top models compare?"" In this paper, we describe our ongoing work on ModelDB, a novel end-to-end system for the management of machine learning models. ModelDB clients automatically track machine learning models in their native environments (e.g. scikit-learn, spark.ml), the ModelDB backend introduces a common layer of abstractions to represent models and pipelines, and the ModelDB frontend allows visual exploration and analyses of models via a web-based interface.","2016-06-26","2023-06-19 07:34:54","2023-06-19 07:34:59","2023-06-19","1–3","","","","","","ModelDB","HILDA '16","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","62 citations (Crossref) [2023-06-19]","","C:\Users\ambreen.hanif\Zotero\storage\QJZTRX6X\Vartak et al_2016_ModelDB.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6PZDUL6U","preprint","2023","AlKhamissi, Badr; Verma, Siddharth; Yu, Ping; Jin, Zhijing; Celikyilmaz, Asli; Diab, Mona","OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models","","","","","http://arxiv.org/abs/2305.12001","In this paper, we conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations. We then evaluate all models on 57 out-of-domain tasks drawn from the SUPER-NATURALINSTRUCTIONS benchmark, covering 26 distinct reasoning skills, utilizing three prompting techniques. Through a comprehensive grid of 27 configurations and 6,156 test evaluations, we investigate the dimensions of finetuning, prompting, and scale to understand the role of explanations on different reasoning skills. Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model's performance when the model is finetuned, while positively affecting the non-finetuned counterpart. Moreover, we observe a slight yet consistent increase in classification accuracy as we incorporate explanations during prompting and finetuning, respectively. Finally, we offer insights on which skills benefit the most from incorporating explanations during finetuning and prompting, such as Numerical (+20.4%) and Analogical (+13.9%) reasoning, as well as skills that exhibit negligible or negative effects.","2023-05-19","2023-06-19 06:24:52","2023-06-19 06:24:56","2023-06-19 06:24:52","","","","","","","OPT-R","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2305.12001 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\7IFUQPYH\AlKhamissi et al_2023_OPT-R.pdf; C:\Users\ambreen.hanif\Zotero\storage\XSI8SMCL\2305.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2305.12001","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C3Z34S5T","webpage","2023","","Paper page - TryOnDiffusion: A Tale of Two UNets","","","","","https://huggingface.co/papers/2306.08276","Join the discussion on this paper page","2023-06-16","2023-06-19 03:55:30","2023-06-19 03:55:30","2023-06-19 03:55:30","","","","","","","Paper page - TryOnDiffusion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"72Q52Z96","journalArticle","2022","Rajabi, Enayat; Etminani, Kobra","Knowledge-graph-based explainable AI: A systematic review","Journal of Information Science","","0165-5515","10.1177/01655515221112844","https://doi.org/10.1177/01655515221112844","In recent years, knowledge graphs (KGs) have been widely applied in various domains for different purposes. The semantic model of KGs can represent knowledge through a hierarchical structure based on classes of entities, their properties, and their relationships. The construction of large KGs can enable the integration of heterogeneous information sources and help Artificial Intelligence (AI) systems be more explainable and interpretable. This systematic review examines a selection of recent publications to understand how KGs are currently being used in eXplainable AI systems. To achieve this goal, we design a framework and divide the use of KGs into four categories: extracting features, extracting relationships, constructing KGs, and KG reasoning. We also identify where KGs are mostly used in eXplainable AI systems (pre-model, in-model, and post-model) according to the aforementioned categories. Based on our analysis, KGs have been mainly used in pre-model XAI for feature and relation extraction. They were also utilised for inference and reasoning in post-model XAI. We found several studies that leveraged KGs to explain the XAI models in the healthcare domain.","2022-09-24","2023-06-14 05:07:01","2023-06-14 05:07:03","2023-06-14 05:07:01","01655515221112844","","","","","","Knowledge-graph-based explainable AI","","","","","","","en","","","","","SAGE Journals","","2 citations (Crossref) [2023-06-14] Publisher: SAGE Publications Ltd","","C:\Users\ambreen.hanif\Zotero\storage\IHS3U5CQ\Rajabi_Etminani_2022_Knowledge-graph-based explainable AI.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3QDANLBC","conferencePaper","2019","Mittelstadt, Brent; Russell, Chris; Wachter, Sandra","Explaining Explanations in AI","Proceedings of the Conference on Fairness, Accountability, and Transparency","978-1-4503-6125-5","","10.1145/3287560.3287574","https://dl.acm.org/doi/10.1145/3287560.3287574","Recent work on interpretability in machine learning and AI has focused on the building of simplified models that approximate the true criteria used to make decisions. These models are a useful pedagogical device for teaching trained professionals how to predict what decisions will be made by the complex system, and most importantly how the system might break. However, when considering any such model it's important to remember Box's maxim that ""All models are wrong but some are useful."" We focus on the distinction between these models and explanations in philosophy and sociology. These models can be understood as a ""do it yourself kit"" for explanations, allowing a practitioner to directly answer ""what if questions"" or generate contrastive explanations without external assistance. Although a valuable ability, giving these models as explanations appears more difficult than necessary, and other forms of explanation may not have the same trade-offs. We contrast the different schools of thought on what makes an explanation, and suggest that machine learning might benefit from viewing the problem more broadly.","2019-01-29","2023-06-13 05:16:31","2023-06-13 05:16:36","2023-06-12","279–288","","","","","","","FAT* '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","249 citations (Crossref) [2023-06-13]","","C:\Users\ambreen.hanif\Zotero\storage\F9QXAWWU\Mittelstadt et al_2019_Explaining Explanations in AI.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q98X8J3H","journalArticle","","Johri, Aditya; Katz, Andrew S.; Qadir, Junaid; Hingle, Ashish","Generative artificial intelligence and engineering education","Journal of Engineering Education","","2168-9830","10.1002/jee.20537","https://onlinelibrary.wiley.com/doi/abs/10.1002/jee.20537","","","2023-06-13 02:21:38","2023-06-13 02:21:38","2023-06-13 02:21:38","","","n/a","n/a","","","","","","","","","","en","© 2023 American Society for Engineering Education.","","","","Wiley Online Library","","_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jee.20537","","C:\Users\ambreen.hanif\Zotero\storage\TPB2EPBP\jee.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J4VG7EXK","journalArticle","","","Towards a unified model for symbolic knowledge extraction with hypercube-based methods","Intelligenza Artificiale","","","10.3233/ia-230001","https://content.iospress.com:443/download/intelligenza-artificiale/ia230001?id=intelligenza-artificiale%2Fia230001&utm_source=researcher_app&utm_medium=referral&utm_campaign=RESR_MRKT_Researcher_inbound","The XAI community is currently studying and developing symbolic knowledge-extraction (SKE) algorithms as a means to produce human-intelligible explanations for black-box machine learning predictors, so as to achieve believability in human-machine interaction. However, many extraction procedures exist in the literature, and choosing the most adequate one is increasingly cumbersome, as novel methods keep on emerging. Challenges arise from the fact that SKE algorithms are commonly defined based on theoretical assumptions that typically hinder practical applicability. This paper focuses on hypercube-based SKE methods, a quite general class of extraction techniques mostly devoted to regression-specific tasks. We first show that hypercube-based methods are flexible enough to support classification problems as well, then we propose a general model for them, and discuss how they support SKE on datasets, predictors, or learning tasks of any sort. Empirical examples are reported as well –based upon the PSyKE framework –, showing the applicability of hypercube-based methods to actual classification tasks.","","2023-06-10 08:48:22","2023-06-12 10:21:55","","","","","","","","","","","","","","","","","","","","","","0 citations (Crossref) [2023-06-12]","","","","Researcher App","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PYPBRXDI","conferencePaper","2015","Brooks, Michael; Amershi, Saleema; Lee, Bongshin; Drucker, Steven M.; Kapoor, Ashish; Simard, Patrice","FeatureInsight: Visual support for error-driven feature ideation in text classification","2015 IEEE Conference on Visual Analytics Science and Technology (VAST)","","","10.1109/VAST.2015.7347637","","Machine learning requires an effective combination of data, features, and algorithms. While many tools exist for working with machine learning data and algorithms, support for thinking of new features, or feature ideation, remains poor. In this paper, we investigate two general approaches to support feature ideation: visual summaries and sets of errors. We present FeatureInsight, an interactive visual analytics tool for building new dictionary features (semantically related groups of words) for text classification problems. FeatureInsight supports an error-driven feature ideation process and provides interactive visual summaries of sets of misclassified documents. We conducted a controlled experiment evaluating both visual summaries and sets of errors in FeatureInsight. Our results show that visual summaries significantly improve feature ideation, especially in combination with sets of errors. Users preferred visual summaries over viewing raw data, and only preferred examining sets when visual summaries were provided. We discuss extensions of both approaches to data types other than text, and point to areas for future research.","2015-10","2023-06-09 00:44:29","2023-06-09 00:41:17","","105-112","","","","","","FeatureInsight","","","","","","","","","","","","IEEE Xplore","","42 citations (Crossref) [2023-06-09]","","C:\Users\ambreen.hanif\Zotero\storage\8EFZYCGN\7347637.html; ","notion://www.notion.so/cite-brooks_featureinsight_2015-2-f9458055e3a242adbd3c688d7e28f2a9","notion","Visualization; Features; machine learning; text classification; dictionaries; interactive machine learning; visualization; Algorithm design and analysis; Classification algorithms; Dictionaries; Text categorization; Training data; Web pages","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2015 IEEE Conference on Visual Analytics Science and Technology (VAST)","","","","","","","","","","","","","","",""
"XCKGMCS3","conferencePaper","2015","Brooks, Michael; Amershi, Saleema; Lee, Bongshin; Drucker, Steven M.; Kapoor, Ashish; Simard, Patrice","FeatureInsight: Visual support for error-driven feature ideation in text classification","2015 IEEE Conference on Visual Analytics Science and Technology (VAST)","","","10.1109/VAST.2015.7347637","","Machine learning requires an effective combination of data, features, and algorithms. While many tools exist for working with machine learning data and algorithms, support for thinking of new features, or feature ideation, remains poor. In this paper, we investigate two general approaches to support feature ideation: visual summaries and sets of errors. We present FeatureInsight, an interactive visual analytics tool for building new dictionary features (semantically related groups of words) for text classification problems. FeatureInsight supports an error-driven feature ideation process and provides interactive visual summaries of sets of misclassified documents. We conducted a controlled experiment evaluating both visual summaries and sets of errors in FeatureInsight. Our results show that visual summaries significantly improve feature ideation, especially in combination with sets of errors. Users preferred visual summaries over viewing raw data, and only preferred examining sets when visual summaries were provided. We discuss extensions of both approaches to data types other than text, and point to areas for future research.","2015-10","2023-06-09 00:35:53","2023-06-09 00:32:54","","105-112","","","","","","FeatureInsight","","","","","","","","","","","","IEEE Xplore","","42 citations (Crossref) [2023-06-09]","","C:\Users\ambreen.hanif\Zotero\storage\68D57CMG\7347637.html; ","notion://www.notion.so/cite-brooks_featureinsight_2015-1-710fb071bfdd42218e7f77d142b74da3","notion","Visualization; Features; machine learning; text classification; dictionaries; interactive machine learning; visualization; Algorithm design and analysis; Classification algorithms; Dictionaries; Text categorization; Training data; Web pages","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2015 IEEE Conference on Visual Analytics Science and Technology (VAST)","","","","","","","","","","","","","","",""
"9NRZ6QVQ","journalArticle","2014","Zhang, Xiangzhou; Hu, Yong; Xie, Kang; Wang, Shouyang; Ngai, E.W.T.; Liu, Mei","A causal feature selection algorithm for stock prediction modeling","Neurocomputing","","","10.1016/j.neucom.2014.01.057","","A key issue of quantitative investment (QI) product design is how to select representative features for stock prediction. However, existing stock prediction models adopt feature selection algorithms that rely on correlation analysis. This paper is the first to apply observational data-based causal analysis to stock prediction. Causalities represent direct influences between various stock features (important for stock analysis), while correlations cannot distinguish direct influences from indirect ones. This study proposes the causal feature selection (CFS) algorithm to select more representative features for better stock prediction modeling. CFS first identifies causalities between variables and then, based on the results, generates a feature subset. Based on 13-year data from the Shanghai Stock Exchanges, comparative experiments were conducted between CFS and three well-known feature selection algorithms, namely, principal component analysis (PCA), decision trees (DT; CART), and the least absolute shrinkage and selection operator (LASSO). CFS performs best in terms of accuracy and precision in most cases when combined with each of the seven baseline models, and identifies 18 important consistent features. In conclusion, CFS has considerable potential to improve the development of QI product.","2014-10-01","2023-06-09 00:06:59","2023-06-09 00:04:07","","48–59","","","142","","Neurocomputing","","","","","","","","","","","","","ResearchGate","","63 citations (Crossref) [2023-06-09]","","C:\Users\ambreen.hanif\Zotero\storage\HNTTBAQE\Zhang et al. - 2014 - A causal feature selection algorithm for stock pre.pdf; ","https://www.researchgate.net/profile/Xiangzhou-Zhang-2/publication/263813016_A_causal_feature_selection_algorithm_for_stock_prediction_modeling/links/5b5aa33baca272a2d66cd7a6/A-causal-feature-selection-algorithm-for-stock-prediction-modeling.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9HPBTGD2","preprint","2021","Bolukbasi, Tolga; Pearce, Adam; Yuan, Ann; Coenen, Andy; Reif, Emily; Viégas, Fernanda; Wattenberg, Martin","An Interpretability Illusion for BERT","","","","10.48550/arXiv.2104.07143","http://arxiv.org/abs/2104.07143","We describe an ""interpretability illusion"" that arises when analyzing the BERT model. Activations of individual neurons in the network may spuriously appear to encode a single, simple concept, when in fact they are encoding something far more complex. The same effect holds for linear combinations of activations. We trace the source of this illusion to geometric properties of BERT's embedding space as well as the fact that common text corpora represent only narrow slices of possible English sentences. We provide a taxonomy of model-learned concepts and discuss methodological implications for interpretability research, especially the importance of testing hypotheses on multiple data sets.","2021-04-14","2023-06-08 05:22:19","2023-06-08 05:22:19","2023-06-08 05:22:19","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2104.07143 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\A77NGYV4\2104.html; C:\Users\ambreen.hanif\Zotero\storage\YEUEKRH5\Bolukbasi et al_2021_An Interpretability Illusion for BERT.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2104.07143","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6Q9Z8LE6","preprint","2018","Weld, Daniel S.; Bansal, Gagan","The Challenge of Crafting Intelligible Intelligence","","","","10.48550/arXiv.1803.04263","http://arxiv.org/abs/1803.04263","Since Artificial Intelligence (AI) software uses techniques like deep lookahead search and stochastic optimization of huge neural networks to fit mammoth datasets, it often results in complex behavior that is difficult for people to understand. Yet organizations are deploying AI algorithms in many mission-critical settings. To trust their behavior, we must make AI intelligible, either by using inherently interpretable models or by developing new methods for explaining and controlling otherwise overwhelmingly complex decisions using local approximation, vocabulary alignment, and interactive explanation. This paper argues that intelligibility is essential, surveys recent work on building such systems, and highlights key directions for research.","2018-10-15","2023-06-08 04:08:48","2023-06-08 04:08:48","2023-06-08 04:08:48","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1803.04263 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\V8U9L7L5\1803.html; C:\Users\ambreen.hanif\Zotero\storage\EZ4ZJQ64\Weld_Bansal_2018_The Challenge of Crafting Intelligible Intelligence.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1803.04263","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GL95CRKG","preprint","2018","Bojchevski, Aleksandar; Shchur, Oleksandr; Zügner, Daniel; Günnemann, Stephan","NetGAN: Generating Graphs via Random Walks","","","","","http://arxiv.org/abs/1803.00816","We propose NetGAN - the first implicit generative model for graphs able to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over the input graph. The proposed model is based on a stochastic neural network that generates discrete output samples and is trained using the Wasserstein GAN objective. NetGAN is able to produce graphs that exhibit well-known network patterns without explicitly specifying them in the model definition. At the same time, our model exhibits strong generalization properties, as highlighted by its competitive link prediction performance, despite not being trained specifically for this task. Being the first approach to combine both of these desirable properties, NetGAN opens exciting avenues for further research.","2018-06-01","2023-06-08 04:03:38","2023-06-08 04:03:38","2023-06-08 04:03:38","","","","","","","NetGAN","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1803.00816 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\QSJZWGVC\1803.html; C:\Users\ambreen.hanif\Zotero\storage\G5YGMQM2\Bojchevski et al_2018_NetGAN.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1803.00816","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U8FNZHJ3","journalArticle","","","Explaining AI in Finance: Past, Present, Prospects. (arXiv:2306.02773v1 [q-fin.ST])","arXiv Quantitative Finance","","","arXiv:2306.02773v1","https://arxiv.org/abs/2306.02773?utm_source=researcher_app&utm_medium=referral&utm_campaign=RESR_MRKT_Researcher_inbound","This paper explores the journey of AI in finance, with a particular focus on the crucial role and potential of Explainable AI (XAI). We trace AI's evolution from early statistical methods to sophisticated machine learning, highlighting XAI's role in popular financial applications. The paper underscores the superior interpretability of methods like Shapley values compared to traditional linear regression in complex financial scenarios. It emphasizes the necessity of further XAI research, given forthcoming EU regulations. The paper demonstrates, through simulations, that XAI enhances trust in AI systems, fostering more responsible decision-making within finance.","","2023-06-06 20:48:55","2023-06-07 04:32:35","","","","","","","","","","","","","","","","","","","","","","","","","","Researcher App","⚠️ Invalid DOI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7K9EQZVA","journalArticle","","","Ethics in AI through the Developer's View: A Grounded Theory Literature Review. (arXiv:2206.09514v2 [cs.SE] UPDATED)","arXiv Computer Science","","","arXiv:2206.09514v2","https://arxiv.org/abs/2206.09514?utm_source=researcher_app&utm_medium=referral&utm_campaign=RESR_MRKT_Researcher_inbound","The term ethics is widely used, explored, and debated in the context of developing Artificial Intelligence (AI) based software systems. In recent years, numerous incidents have raised the profile of ethical issues in AI development and led to public concerns about the proliferation of AI technology in our everyday lives. But what do we know about the views and experiences of those who develop these systems: the AI developers? We conducted a grounded theory literature review (GTLR) of 38 primary empirical studies that included AI developers' views on ethics in AI and analysed them to derive five categories - developer awareness, perception, need, challenge, and approach. These are underpinned by multiple codes and concepts that we explain with evidence from the included studies. We present a taxonomy of ethics in AI from developers' viewpoints to assist AI developers in identifying and understanding the different aspects of AI ethics. The taxonomy provides a landscape view of the key aspects that concern AI developers when it comes to ethics in AI. We also share an agenda for future research studies and recommendations for developers, managers, and organisations to help in their efforts to better consider and implement ethics in AI.","","2023-06-06 10:07:48","2023-06-06 10:24:01","","","","","","","","","","","","","","","","","","","","","","","","","","Researcher App","⚠️ Invalid DOI","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7Z9J6SPT","preprint","2019","Ancona, Marco; Öztireli, Cengiz; Gross, Markus","Explaining Deep Neural Networks with a Polynomial Time Algorithm for Shapley Values Approximation","","","","","http://arxiv.org/abs/1903.10992","The problem of explaining the behavior of deep neural networks has recently gained a lot of attention. While several attribution methods have been proposed, most come without strong theoretical foundations, which raises questions about their reliability. On the other hand, the literature on cooperative game theory suggests Shapley values as a unique way of assigning relevance scores such that certain desirable properties are satisfied. Unfortunately, the exact evaluation of Shapley values is prohibitively expensive, exponential in the number of input features. In this work, by leveraging recent results on uncertainty propagation, we propose a novel, polynomial-time approximation of Shapley values in deep neural networks. We show that our method produces significantly better approximations of Shapley values than existing state-of-the-art attribution methods.","2019-06-21","2023-06-06 04:00:20","2023-06-06 04:00:30","2023-06-06 04:00:20","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1903.10992 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\TRL25JXL\Ancona et al_2019_Explaining Deep Neural Networks with a Polynomial Time Algorithm for Shapley.pdf; C:\Users\ambreen.hanif\Zotero\storage\KXAVT6BB\1903.html","","","","","","","","","","","","","","","","","","","","","","arXiv:1903.10992","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HDIH6JJF","webpage","","","[1803.04263] The Challenge of Crafting Intelligible Intelligence","","","","","https://arxiv.org/abs/1803.04263","","","2023-06-05 23:55:02","2023-06-05 23:55:02","2023-06-05 23:55:02","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\8V9HVEZQ\1803.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YD6K34E8","journalArticle","","Grafberger, Stefan; Munich, TU; Stoyanovich, Julia; Schelter, Sebastian","Lightweight Inspection of Data Preprocessing in Native Machine Learning Pipelines","","","","","","Machine Learning (ML) is increasingly used to automate impactful decisions, and the risks arising from this wide-spread use are garnering attention from policy makers, scientists, and the media. ML applications are often very brittle with respect to their input data, which leads to concerns about their reliability, accountability, and fairness. In this paper we discuss such hard-to-identify data issues and describe mlinspect, a library that enables lightweight lineage-based inspection of ML preprocessing pipelines.","","2023-06-01 07:10:54","2023-06-01 19:07:53","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\HBEQ34L2\Grafberger et al. - Lightweight Inspection of Data Preprocessing in Na.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F9JYFYI3","journalArticle","","Schelter, Sebastian; Grafberger, Stefan; Guha, Shubha; Sprangers, Olivier; Karlaš, Bojan; Zhang, Ce","Screening Native ML Pipelines with “ArgusEyes”","","","","","","","","2023-06-01 07:11:32","2023-06-01 19:07:52","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\B7CHPRQR\Schelter et al. - Screening Native ML Pipelines with “ArgusEyes”.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H4LM76C3","preprint","2022","Karlaš, Bojan; Dao, David; Interlandi, Matteo; Li, Bo; Schelter, Sebastian; Wu, Wentao; Zhang, Ce","Data Debugging with Shapley Importance over End-to-End Machine Learning Pipelines","","","","","http://arxiv.org/abs/2204.11131","Developing modern machine learning (ML) applications is data-centric, of which one fundamental challenge is to understand the influence of data quality to ML training -- ""Which training examples are 'guilty' in making the trained ML model predictions inaccurate or unfair?"" Modeling data influence for ML training has attracted intensive interest over the last decade, and one popular framework is to compute the Shapley value of each training example with respect to utilities such as validation accuracy and fairness of the trained ML model. Unfortunately, despite recent intensive interest and research, existing methods only consider a single ML model ""in isolation"" and do not consider an end-to-end ML pipeline that consists of data transformations, feature extractors, and ML training. We present DataScope (ease.ml/datascope), the first system that efficiently computes Shapley values of training examples over an end-to-end ML pipeline, and illustrate its applications in data debugging for ML training. To this end, we first develop a novel algorithmic framework that computes Shapley value over a specific family of ML pipelines that we call canonical pipelines: a positive relational algebra query followed by a K-nearest-neighbor (KNN) classifier. We show that, for many subfamilies of canonical pipelines, computing Shapley value is in PTIME, contrasting the exponential complexity of computing Shapley value in general. We then put this to practice -- given an sklearn pipeline, we approximate it with a canonical pipeline to use as a proxy. We conduct extensive experiments illustrating different use cases and utilities. Our results show that DataScope is up to four orders of magnitude faster over state-of-the-art Monte Carlo-based methods, while being comparably, and often even more, effective in data debugging.","2022-04-26","2023-06-01 07:12:57","2023-06-01 07:12:58","2023-06-01 07:12:57","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2204.11131 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\AVKUWESK\Karlaš et al. - 2022 - Data Debugging with Shapley Importance over End-to.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2204.11131","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FRGVGXE4","computerProgram","2022","","amsterdata/freamon","","","","","https://github.com/amsterdata/freamon","Freamon enables data scientists to automatically reconstruct and query the intermediate data from ML pipelines to reduce the level of expertise and manual effort required to debug this data.","2022-11-11","2023-06-01 07:11:25","2023-06-01 07:11:25","2023-06-01 07:11:25","","","","","","","","","","","","amsterdata","","","GPL-3.0","","","","GitHub","","original-date: 2022-05-21T16:34:14Z","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Jupyter Notebook","","","","","","","","",""
"Q2MUHUKN","journalArticle","2022","Grafberger, Stefan; Groth, Paul; Stoyanovich, Julia; Schelter, Sebastian","Data distribution debugging in machine learning pipelines","The VLDB Journal","","1066-8888, 0949-877X","10.1007/s00778-021-00726-w","https://link.springer.com/10.1007/s00778-021-00726-w","Machine learning (ML) is increasingly used to automate impactful decisions, and the risks arising from this widespread use are garnering attention from policy makers, scientists, and the media. ML applications are often brittle with respect to their input data, which leads to concerns about their correctness, reliability, and fairness. In this paper, we describe mlinspect, a library that helps diagnose and mitigate technical bias that may arise during preprocessing steps in an ML pipeline. We refer to these problems collectively as data distribution bugs. The key idea is to extract a directed acyclic graph representation of the dataﬂow from a preprocessing pipeline and to use this representation to automatically instrument the code with predeﬁned inspections. These inspections are based on a lightweight annotation propagation approach to propagate metadata such as lineage information from operator to operator. In contrast to existing work, mlinspect operates on declarative abstractions of popular data science libraries like estimator/transformer pipelines and does not require manual code instrumentation. We discuss the design and implementation of the mlinspect library and give a comprehensive end-to-end example that illustrates its functionality.","2022-09","2023-06-01 07:10:23","2023-06-01 07:10:56","2023-06-01 07:10:23","1103-1126","","5","31","","The VLDB Journal","","","","","","","","en","","","","","DOI.org (Crossref)","","5 citations (Crossref) [2023-06-01]","","C:\Users\ambreen.hanif\Zotero\storage\8B68LHZ8\Grafberger et al. - 2022 - Data distribution debugging in machine learning pi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CHEN6T66","computerProgram","2022","","ArgusEyes Demonstration","","","","","https://github.com/amsterdata/arguseyes-demo","","2022-12-02","2023-06-01 07:10:17","2023-06-01 07:10:17","2023-06-01 07:10:10","","","","","","","","","","","","amsterdata","","","GPL-3.0","","","","GitHub","","original-date: 2022-12-02T08:51:16Z","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Jupyter Notebook","","","","","","","","",""
"LZSVL8IJ","conferencePaper","2021","Grafberger, Stefan; Guha, Shubha; Stoyanovich, Julia; Schelter, Sebastian","MLINSPECT: A Data Distribution Debugger for Machine Learning Pipelines","Proceedings of the 2021 International Conference on Management of Data","978-1-4503-8343-1","","10.1145/3448016.3452759","https://dl.acm.org/doi/10.1145/3448016.3452759","Machine Learning (ML) is increasingly used to automate impactful decisions, and the risks arising from this wide-spread use are garnering attention from policymakers, scientists, and the media. ML applications are often very brittle with respect to their input data, which leads to concerns about their reliability, accountability, and fairness. While bias detection cannot be fully automated, computational tools can help pinpoint particular types of data issues. We recently proposed mlinspect, a library that enables lightweight lineage-based inspection of ML preprocessing pipelines. In this demonstration, we show how mlinspect can be used to detect data distribution bugs in a representative pipeline. In contrast to existing work, mlinspect operates on declarative abstractions of popular data science libraries like estimator/transformer pipelines, can handle both relational and matrix data, and does not require manual code instrumentation. The library is publicly available at https://github.com/stefan-grafberger/mlinspect.","2021-06-09","2023-06-01 07:09:05","2023-06-01 07:10:09","2023-06-01 07:09:05","2736-2739","","","","","","MLINSPECT","","","","","ACM","Virtual Event China","en","","","","","DOI.org (Crossref)","","3 citations (Crossref) [2023-06-01]","","C:\Users\ambreen.hanif\Zotero\storage\YJX5I5BS\Grafberger et al. - 2021 - MLINSPECT A Data Distribution Debugger for Machin.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","SIGMOD/PODS '21: International Conference on Management of Data","","","","","","","","","","","","","","",""
"RHIFXU2R","conferencePaper","2023","Grafberger, Stefan; Groth, Paul; Schelter, Sebastian","Provenance Tracking for End-to-End Machine Learning Pipelines","Companion Proceedings of the ACM Web Conference 2023","978-1-4503-9419-2","","10.1145/3543873.3587557","https://dl.acm.org/doi/10.1145/3543873.3587557","","2023-04-30","2023-06-01 06:51:00","2023-06-01 06:51:04","2023-05-31","1512","","","","","","","WWW '23 Companion","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","0 citations (Crossref) [2023-06-01]","","C:\Users\ambreen.hanif\Zotero\storage\X8J9WNZC\Grafberger et al_2023_Provenance Tracking for End-to-End Machine Learning Pipelines.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HVUZDZAF","preprint","2019","Schelter, Sebastian; He, Yuxuan; Khilnani, Jatin; Stoyanovich, Julia","FairPrep: Promoting Data to a First-Class Citizen in Studies on Fairness-Enhancing Interventions","","","","10.48550/arXiv.1911.12587","http://arxiv.org/abs/1911.12587","The importance of incorporating ethics and legal compliance into machine-assisted decision-making is broadly recognized. Further, several lines of recent work have argued that critical opportunities for improving data quality and representativeness, controlling for bias, and allowing humans to oversee and impact computational processes are missed if we do not consider the lifecycle stages upstream from model training and deployment. Yet, very little has been done to date to provide system-level support to data scientists who wish to develop and deploy responsible machine learning methods. We aim to fill this gap and present FairPrep, a design and evaluation framework for fairness-enhancing interventions. FairPrep is based on a developer-centered design, and helps data scientists follow best practices in software engineering and machine learning. As part of our contribution, we identify shortcomings in existing empirical studies for analyzing fairness-enhancing interventions. We then show how FairPrep can be used to measure the impact of sound best practices, such as hyperparameter tuning and feature scaling. In particular, our results suggest that the high variability of the outcomes of fairness-enhancing interventions observed in previous studies is often an artifact of a lack of hyperparameter tuning. Further, we show that the choice of a data cleaning method can impact the effectiveness of fairness-enhancing interventions.","2019-11-28","2023-06-01 06:49:46","2023-06-01 06:49:46","2023-06-01 06:49:42","","","","","","","FairPrep","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1911.12587 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\IIDD4GC4\1911.html; C:\Users\ambreen.hanif\Zotero\storage\8364GI2I\Schelter et al_2019_FairPrep.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1911.12587","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BPWIB96X","journalArticle","2011","Moreau, Luc; Clifford, Ben; Freire, Juliana; Futrelle, Joe; Gil, Yolanda; Groth, Paul; Kwasnikowska, Natalia; Miles, Simon; Missier, Paolo; Myers, Jim; Plale, Beth; Simmhan, Yogesh; Stephan, Eric; den Bussche, Jan Van","The Open Provenance Model core specification (v1.1)","Future Generation Computer Systems","","0167-739X","10.1016/j.future.2010.07.005","https://www.sciencedirect.com/science/article/pii/S0167739X10001275","The Open Provenance Model is a model of provenance that is designed to meet the following requirements: (1) Allow provenance information to be exchanged between systems, by means of a compatibility layer based on a shared provenance model. (2) Allow developers to build and share tools that operate on such a provenance model. (3) Define provenance in a precise, technology-agnostic manner. (4) Support a digital representation of provenance for any “thing”, whether produced by computer systems or not. (5) Allow multiple levels of description to coexist. (6) Define a core set of rules that identify the valid inferences that can be made on provenance representation. This document contains the specification of the Open Provenance Model (v1.1) resulting from a community effort to achieve inter-operability in the Provenance Challenge series.","2011-06-01","2023-06-01 05:23:42","2023-06-01 05:23:45","2023-06-01 05:23:42","743-756","","6","27","","Future Generation Computer Systems","","","","","","","","en","","","","","ScienceDirect","","452 citations (Crossref) [2023-06-01]","","C:\Users\ambreen.hanif\Zotero\storage\R7T54A89\Moreau et al_2011_The Open Provenance Model core specification (v1.pdf; C:\Users\ambreen.hanif\Zotero\storage\IGK38WCQ\S0167739X10001275.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2EQELUF9","preprint","2023","Bell, Andrew; Bynum, Lucius; Drushchak, Nazarii; Herasymova, Tetiana; Rosenblatt, Lucas; Stoyanovich, Julia","The Possibility of Fairness: Revisiting the Impossibility Theorem in Practice","","","","","http://arxiv.org/abs/2302.06347","The ``impossibility theorem'' -- which is considered foundational in algorithmic fairness literature -- asserts that there must be trade-offs between common notions of fairness and performance when fitting statistical models, except in two special cases: when the prevalence of the outcome being predicted is equal across groups, or when a perfectly accurate predictor is used. However, theory does not always translate to practice. In this work, we challenge the implications of the impossibility theorem in practical settings. First, we show analytically that, by slightly relaxing the impossibility theorem (to accommodate a \textit{practitioner's} perspective of fairness), it becomes possible to identify a large set of models that satisfy seemingly incompatible fairness constraints. Second, we demonstrate the existence of these models through extensive experiments on five real-world datasets. We conclude by offering tools and guidance for practitioners to understand when -- and to what degree -- fairness along multiple criteria can be achieved. For example, if one allows only a small margin-of-error between metrics, there exists a large set of models simultaneously satisfying \emph{False Negative Rate Parity}, \emph{False Positive Rate Parity}, and \emph{Positive Predictive Value Parity}, even when there is a moderate prevalence difference between groups. This work has an important implication for the community: achieving fairness along multiple metrics for multiple groups (and their intersections) is much more possible than was previously believed.","2023-02-13","2023-05-31 06:36:45","2023-05-31 06:37:40","2023-05-31 06:35:27","","","","","","","The Possibility of Fairness","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2302.06347 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\HIK94JIW\2302.html; C:\Users\ambreen.hanif\Zotero\storage\I36W9HUY\Bell et al_2023_The Possibility of Fairness.pdf; ","notion://www.notion.so/Bell-et-al-2023-1eaa852877404732a1792fef92d29153","notion","","","","","","","","","","","","","","","","","","","","arXiv:2302.06347","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EV9BZSJJ","journalArticle","2023","Vasconcelos, Helena; Jörke, Matthew; Grunde-McLaughlin, Madeleine; Gerstenberg, Tobias; Bernstein, Michael S.; Krishna, Ranjay","Explanations Can Reduce Overreliance on AI Systems During Decision-Making","Proceedings of the ACM on Human-Computer Interaction","","2573-0142","10.1145/3579605","https://dl.acm.org/doi/10.1145/3579605","Prior work has identified a resilient phenomenon that threatens the performance of human-AI decision-making teams: overreliance, when people agree with an AI, even when it is incorrect. Surprisingly, overreliance does not reduce when the AI produces explanations for its predictions, compared to only providing predictions. Some have argued that overreliance results from cognitive biases or uncalibrated trust, attributing overreliance to an inevitability of human cognition. By contrast, our paper argues that people strategically choose whether or not to engage with an AI explanation, demonstrating empirically that there are scenarios where AI explanations reduce overreliance. To achieve this, we formalize this strategic choice in a cost-benefit framework, where the costs and benefits of engaging with the task are weighed against the costs and benefits of relying on the AI. We manipulate the costs and benefits in a maze task, where participants collaborate with a simulated AI to find the exit of a maze. Through 5 studies (N = 731), we find that costs such as task difficulty (Study 1), explanation difficulty (Study 2, 3), and benefits such as monetary compensation (Study 4) affect overreliance. Finally, Study 5 adapts the Cognitive Effort Discounting paradigm to quantify the utility of different explanations, providing further support for our framework. Our results suggest that some of the null effects found in literature could be due in part to the explanation not sufficiently reducing the costs of verifying the AI's prediction.","2023-04-14","2023-05-31 06:30:18","2023-05-31 06:30:57","2023-05-31 06:30:17","1-38","","CSCW1","7","","Proc. ACM Hum.-Comput. Interact.","","","","","","","","en","","","","","DOI.org (Crossref)","","0 citations (Crossref) [2023-05-31]","","; C:\Users\ambreen.hanif\Zotero\storage\26KE5T74\Vasconcelos et al_2023_Explanations Can Reduce Overreliance on AI Systems During Decision-Making.pdf","notion://www.notion.so/Vasconcelos-et-al-2023-84212e8d3b744ead966951ff68eb2f8d","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NEZ687BK","journalArticle","1988","Lowry, Stella; Macpherson, Gordon","A blot on the profession","British Medical Journal (Clinical research ed.)","","0267-0623","","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2545288/","","1988-03-05","2023-05-31 05:51:57","2023-05-31 05:51:57","2023-05-31 05:51:52","657-658","","6623","296","","Br Med J (Clin Res Ed)","","","","","","","","","","","","","PubMed Central","","PMID: 3128356 PMCID: PMC2545288","","C:\Users\ambreen.hanif\Zotero\storage\HXB9UAYT\Lowry_Macpherson_1988_A blot on the profession.pdf; ","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2545288/","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MYATENA3","webpage","","","A blot on the profession - PMC","","","","","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2545288/","","","2023-05-31 05:51:17","2023-05-31 05:51:17","2023-05-31 05:51:17","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\GJPGPCDG\PMC2545288.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AIP5BTP2","blogPost","2023","casati, fabio","Annoying frequent perspectives on responsible AI","Medium","","","","https://medium.com/@sphoebs/annoying-frequent-perspectives-on-responsible-ai-55ce82545b9d","(a subject nobody cared about until basically yesterday, as shown below, but that now includes many experts)","2023-05-30","2023-05-31 05:51:07","2023-05-31 05:51:07","2023-05-31 05:51:07","","","","","","","","","","","","","","en","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\V2UEY36S\annoying-frequent-perspectives-on-responsible-ai-55ce82545b9d.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CKC2JGXB","computerProgram","","","","","","","","","","","2023-05-29 05:07:15","2023-05-30 05:44:49","","","","","","","","","","","","","","","","","","","","","","","","","notion://www.notion.so/n-d-6863ea814f7e4f6d8ccbf7543727075f","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZVSLADGG","journalArticle","","Sullivan, Brendan W","A Guided Journey Into the World of Abstract Mathematics and the Writing of Proofs","","","","","","","","2023-05-29 02:19:16","2023-05-30 05:44:48","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\9385LKYQ\Sullivan - A Guided Journey Into the World of Abstract Mathem.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H9ZEQQ4I","journalArticle","","Sullivan, Brendan W","A Guided Journey Into the World of Abstract Mathematics and the Writing of Proofs","","","","","","","","2023-05-29 02:19:58","2023-05-30 05:44:44","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\KYAIR3PK\bws_book.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AXIK5KTW","journalArticle","","Sullivan, Brendan W","A Guided Journey Into the World of Abstract Mathematics and the Writing of Proofs","","","","","","","","2023-05-29 02:19:34","2023-05-30 05:44:43","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\ERBNZWR5\Sullivan - A Guided Journey Into the World of Abstract Mathem.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I8BL6V8U","preprint","2023","Cunegatti, Elia; Bucur, Doina; Iacca, Giovanni","Peeking inside Sparse Neural Networks using Multi-Partite Graph Representations","","","","10.48550/arXiv.2305.16886","http://arxiv.org/abs/2305.16886","Modern Deep Neural Networks (DNNs) have achieved very high performance at the expense of computational resources. To decrease the computational burden, several techniques have proposed to extract, from a given DNN, efficient subnetworks which are able to preserve performance while reducing the number of network parameters. The literature provides a broad set of techniques to discover such subnetworks, but few works have studied the peculiar topologies of such pruned architectures. In this paper, we propose a novel \emph{unrolled input-aware} bipartite Graph Encoding (GE) that is able to generate, for each layer in an either sparse or dense neural network, its corresponding graph representation based on its relation with the input data. We also extend it into a multipartite GE, to capture the relation between layers. Then, we leverage on topological properties to study the difference between the existing pruning algorithms and algorithm categories, as well as the relation between topologies and performance.","2023-05-26","2023-05-30 00:03:34","2023-05-30 00:03:34","2023-05-30 00:03:34","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2305.16886 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\6S8854QD\2305.html; C:\Users\ambreen.hanif\Zotero\storage\QIKQ6KRJ\Cunegatti et al_2023_Peeking inside Sparse Neural Networks using Multi-Partite Graph Representations.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2305.16886","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BW8F2J9G","webpage","","","[2305.16886] Peeking inside Sparse Neural Networks using Multi-Partite Graph Representations","","","","","https://arxiv.org/abs/2305.16886","","","2023-05-30 00:03:30","2023-05-30 00:03:30","2023-05-30 00:03:30","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\MMFDYD8T\2305.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2S665W2R","journalArticle","2020","Tamburri, Damian A.","Sustainable MLOps - Trends and Challenges","Proceedings - 2020 22nd International Symposium on Symbolic and Numeric Algorithms for Scientific Computing, SYNASC 2020","","978-1-7281-7629-1","10.1109/SYNASC51798.2020.00015","http://www.scopus.com/inward/record.url?scp=85102347494&partnerID=8YFLogxK","Even simply through a GoogleTrends search it becomes clear that Machine-Learning Operations-or MLOps, for short-are climbing in interest from both a scientific and practical perspective. On the one hand, software components and middleware are proliferating to support all manners of MLOps, from AutoML (i.e., software which enables developers with limited machine-learning expertise to train high-quality models specific to their domain or data) to feature-specific ML engineering, e.g., Explainability and Interpretability. On the other hand, the more these platforms penetrate the day-to-day activities of software operations, the more the risk for AI Software becoming unsustainable from a social, technical, or organisational perspective. This paper offers a concise definition of MLOps and AI Software Sustainability and outlines key challenges in its pursuit.","2020-09","2023-05-29 12:54:24","2023-05-29 12:54:32","2023-05-29 12:54:24","17-23","","","","","","","","","","","","","","","","","","Eindhoven University of Technology research portal","","25 citations (Crossref) [2023-05-29]","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5J2N4IMY","conferencePaper","2021","Parulian, Nikolaus Nova; McPhillips, Timothy M.; Ludäscher, Bertram","A Model and System for Querying Provenance from Data Cleaning Workflows","Provenance and Annotation of Data and Processes","978-3-030-80960-7","","10.1007/978-3-030-80960-7_11","","Data cleaning is an essential component of data preparation in machine learning and other data science workflows, and is widely recognized as the most time-consuming and error-prone part when working with real-world data. How data was prepared and cleaned has a significant impact on the reliability and trustworthiness of results of any subsequent analysis. Transparent data cleaning not only requires that provenance (i.e., operation history and value changes) be captured, but also that those changes are easy to explore and evaluate: The data scientists who prepare the data, as well as others who want to reuse the cleaned data for their studies, need to be able to easily explore and query its data cleaning history. We have developed a domain-specific provenance model for data cleaning that supports the kind of provenance questions that data scientists need to answer when inspecting and debugging data preparation histories. The design of the model was driven by the need (i) to answer relevant, user-oriented provenance questions, and (ii) to do so in an effective and efficient manner. The model is a refinement of an earlier provenance model and has been implemented as a companion tool to OpenRefine, a popular, open source tool for data cleaning.","2021","2023-05-29 12:49:44","2023-05-29 12:49:51","","183-197","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","2 citations (Crossref) [2023-05-29]","","C:\Users\ambreen.hanif\Zotero\storage\E4K36XLJ\Parulian et al_2021_A Model and System for Querying Provenance from Data Cleaning Workflows.pdf","","","","Glavic, Boris; Braganholo, Vanessa; Koop, David","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BAGKUGEW","journalArticle","2023","Pina, Débora; Chapman, Adriane; De Oliveira, Daniel; Mattoso, Marta","Deep Learning Provenance Data Integration: a Practical Approach","Companion Proceedings of the ACM Web Conference 2023","","","10.1145/3543873.3587561","https://dl.acm.org/doi/10.1145/3543873.3587561","A Deep Learning (DL) life cycle involves several data transformations, such as performing data pre-processing, defining datasets to train and test a deep neural network (DNN), and training and evaluating the DL model. Choosing a final model requires DL model selection, which involves analyzing data from several training configurations (e.g. hyperparameters and DNN architectures). Tracing training data back to pre-processing operations can provide insights into the model selection step. Provenance is a natural solution to represent data derivation of the whole DL life cycle. However, there are challenges in providing an integration of the provenance of these different steps. There are a few approaches to capturing and integrating provenance data from the DL life cycle, but they require that the same provenance capture solution is used along all the steps, which can limit interoperability and flexibility when choosing the DL environment. Therefore, in this work, we present a prototype for provenance data integration using different capture solutions. We show use cases where the integrated provenance from pre-processing and training steps can show how data pre-processing decisions influenced the model selection. Experiments were performed using real-world datasets to train a DNN and provided evidence of the integration between the considered steps, answering queries such as how the data used to train a model that achieved a specific result was processed.","2023-04-30","2023-05-29 12:33:42","2023-05-29 12:33:53","2023-05-29 12:33:41","1542-1550","","","","","","Deep Learning Provenance Data Integration","","","","","","","en","","","","","Semantic Scholar","","0 citations (Crossref) [2023-05-29] Conference Name: WWW '23: The ACM Web Conference 2023 ISBN: 9781450394192 Place: Austin TX USA Publisher: ACM","","","https://www.semanticscholar.org/paper/Deep-Learning-Provenance-Data-Integration%3A-a-Pina-Chapman/131de27f87e9d5ad75d75b21fd67e51af691e7ed","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7262Y2VK","conferencePaper","2021","Pina, Débora; Kunstmann, Liliane; De Oliveira, Daniel; Valduriez, Patrick; Mattoso, Marta","Provenance Supporting Hyperparameter Analysis in Deep Neural Networks","","978-3-030-80959-1 978-3-030-80960-7","","10.1007/978-3-030-80960-7_2","https://link.springer.com/10.1007/978-3-030-80960-7_2",". The duration of the life cycle in deep neural networks (DNN) depends on the data conﬁguration decisions that lead to success in obtaining models. Analyzing hyperparameters along the evolution of the network’s execution allows for adapting the data. Provenance data derivation traces help the parameter ﬁne-tuning by providing a global data picture with clear dependencies. Provenance can also contribute to the interpretation of models resulting from the DNN life cycle. However, there are challenges in collecting hyperparameters and in modeling the relationships between the data involved in the DNN life cycle to build a provenance database. Current approaches adopt diﬀerent notions of provenance in their representation and require the execution of the DNN under a speciﬁc software framework, which limits interoperability and ﬂexibility when choosing the DNN execution environment. This work presents a provenance data-based approach to address these challenges, proposing a collection mechanism with ﬂexibility in the choice and representation of data to be analyzed. Experiments of the approach, using a convolutional neural network focused on image recognition, provide evidence of the ﬂexibility, the eﬃciency of data collection, the analysis and the validation of network data.","2021","2023-05-29 12:33:27","2023-05-29 12:33:41","2023-05-29 12:33:26","20-38","","","12839","","","","","","","","Springer International Publishing","Cham","en","","","","","Semantic Scholar","","0 citations (Crossref) [2023-05-29] Book Title: Provenance and Annotation of Data and Processes Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-80960-7_2","","C:\Users\ambreen.hanif\Zotero\storage\A3BM44S9\Pina et al_2021_Provenance Supporting Hyperparameter Analysis in Deep Neural Networks.pdf; ","https://www.semanticscholar.org/paper/Provenance-Supporting-Hyperparameter-Analysis-in-Pina-Kunstmann/7efc1fadfec6e37f826cd06786a8027a1f5318c3","","","Glavic, Boris; Braganholo, Vanessa; Koop, David","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EN2RYFGQ","journalArticle","2022","Pina, Débora; Kunstmann, Liliane; Bevilaqua, Felipe; Siqueira, Isabela; Lyra, Alan; Oliveira, Daniel de; Mattoso, Marta","Capturing Provenance from Deep Learning Applications Using Keras-Prov and Colab: a Practical Approach","Journal of Information and Data Management","","2178-7107","10.5753/jidm.2022.2544","https://sol.sbc.org.br/journals/index.php/jidm/article/view/2544","Due to the exploratory nature of DNNs, DL specialists often need to modify the input dataset, change a filter when preprocessing input data, or fine-tune the models’ hyperparameters, while analyzing the evolution of the training. However, the specialist may lose track of what hyperparameter configurations have been used and tuned if these data are not properly registered. Thus, these configurations must be tracked and made available for the user’s analysis. One way of doing this is to use provenance data derivation traces to help the hyperparameter’s fine-tuning by providing a global data picture with clear dependencies. Current provenance solutions present provenance data disconnected from W3C PROV recommendation, which is difficult to reproduce and compare to other provenance data. To help with these challenges, we present Keras-Prov, an extension to the Keras deep learning library to collect provenance data compliant with PROV. To show the flexibility of Keras-Prov, we extend a previous Keras-Prov demonstration paper with larger experiments using GPUs with the help of Google Colab. Despite the challenges of running a DBMS with virtual environments, DL analysis with provenance has added trust and persistence in databases and PROV serializations. Experiments show Keras-Prov data analysis, during training execution, to support hyperparameter fine-tuning decisions, favoring the comparison, and reproducibility of such DL experiments. Keras-Prov is open source and can be downloaded from https://github.com/dbpina/keras-prov.","2022-12-19","2023-05-29 12:27:37","2023-05-29 12:27:44","2023-05-29 12:27:37","","","5","13","","","Capturing Provenance from Deep Learning Applications Using Keras-Prov and Colab","","","","","","","en","Copyright (c) 2022 Journal of Information and Data Management","","","","sol.sbc.org.br","","0 citations (Crossref) [2023-05-29] Number: 5","","C:\Users\ambreen.hanif\Zotero\storage\LS5B2I2J\Pina et al_2022_Capturing Provenance from Deep Learning Applications Using Keras-Prov and Colab.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7NB6Y5T9","conferencePaper","2017","Schelter, Sebastian; Böse, Joos-Hendrik; Kirschnick, Johannes; Klein, Thoralf; Seufert, Stephan","Automatically Tracking Metadata and Provenance of Machine Learning Experiments","","","","","","We present a lightweight system to extract, store and manage metadata and provenance information of common artifacts in machine learning (ML) experiments: datasets, models, predictions, evaluations and training runs. Our system accelerates users in their ML workﬂow, and provides a basis for comparability and repeatability of ML experiments. We achieve this by tracking the lineage of produced artifacts and automatically extracting metadata such as hyperparameters of models, schemas of datasets or layouts of deep neural networks. Our system provides a general declarative representation of said ML artifacts, is integrated with popular frameworks such as MXNet, SparkML and scikit-learn, and meets the demands of various production use cases at Amazon.","2017","2023-05-29 12:12:58","2023-05-29 12:19:48","","27-29","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\8ZR2UBMH\Schelter et al. - Automatically Tracking Metadata and Provenance of .pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NeurIPS","","","","","","","","","","","","","","",""
"EHPAI594","preprint","2022","Nakao, Yuri; Strappelli, Lorenzo; Stumpf, Simone; Naseer, Aisha; Regoli, Daniele; Del Gamba, Giulia","Towards Responsible AI: A Design Space Exploration of Human-Centered Artificial Intelligence User Interfaces to Investigate Fairness","","","","10.48550/arXiv.2206.00474","http://arxiv.org/abs/2206.00474","With Artificial intelligence (AI) to aid or automate decision-making advancing rapidly, a particular concern is its fairness. In order to create reliable, safe and trustworthy systems through human-centred artificial intelligence (HCAI) design, recent efforts have produced user interfaces (UIs) for AI experts to investigate the fairness of AI models. In this work, we provide a design space exploration that supports not only data scientists but also domain experts to investigate AI fairness. Using loan applications as an example, we held a series of workshops with loan officers and data scientists to elicit their requirements. We instantiated these requirements into FairHIL, a UI to support human-in-the-loop fairness investigations, and describe how this UI could be generalized to other use cases. We evaluated FairHIL through a think-aloud user study. Our work contributes better designs to investigate an AI model's fairness-and move closer towards responsible AI.","2022-06-01","2023-05-29 12:13:46","2023-05-29 12:15:13","2023-05-29 12:13:46","","","","","","","Towards Responsible AI","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2206.00474 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\CLYRWUC2\2206.html; C:\Users\ambreen.hanif\Zotero\storage\2FMA86LY\Nakao et al_2022_Towards Responsible AI.pdf; ","notion://www.notion.so/Nakao-et-al-2022-90e9fa4e9e2544ad8c8941f9f7354aa3","notion","","","","","","","","","","","","","","","","","","","","arXiv:2206.00474","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6KP8CEHF","conferencePaper","2020","Namaki, Mohammad Hossein; Floratou, Avrilia; Psallidas, Fotis; Krishnan, Subru; Agrawal, Ashvin; Wu, Yinghui; Zhu, Yiwen; Weimer, Markus","Vamsa: Automated Provenance Tracking in Data Science Scripts","Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining","978-1-4503-7998-4","","10.1145/3394486.3403205","https://dl.acm.org/doi/10.1145/3394486.3403205","There has recently been a lot of ongoing research in the areas of fairness, bias and explainability of machine learning (ML) models due to the self-evident or regulatory requirements of various ML applications. We make the following observation: All of these approaches require a robust understanding of the relationship between ML models and the data used to train them. In this work, we introduce the ML provenance tracking problem: the fundamental idea is to automatically track which columns in a dataset have been used to derive the features/labels of an ML model. We discuss the challenges in capturing such information in the context of Python, the most common language used by data scientists. We then present Vamsa, a modular system that extracts provenance from Python scripts without requiring any changes to the users' code. Using 26K real data science scripts, we verify the effectiveness of Vamsa in terms of coverage, and performance. We also evaluate Vamsa's accuracy on a smaller subset of manually labeled data. Our analysis shows that Vamsa's precision and recall range from 90.4% to 99.1% and its latency is in the order of milliseconds for average size scripts. Drawing from our experience in deploying ML models in production, we also present an example in which Vamsa helps automatically identify models that are affected by data corruption issues.","2020-08-20","2023-05-29 07:16:29","2023-05-29 07:16:43","2023-05-29","1542–1551","","","","","","Vamsa","KDD '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","15 citations (Crossref) [2023-05-29]","","C:\Users\ambreen.hanif\Zotero\storage\WN9RA3BM\Namaki et al_2020_Vamsa.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"68FK6W8V","conferencePaper","2017","Ma, Shiqing; Aafer, Yousra; Xu, Zhaogui; Lee, Wen-Chuan; Zhai, Juan; Liu, Yingqi; Zhang, Xiangyu","LAMP: data provenance for graph based machine learning algorithms through derivative computation","Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering","978-1-4503-5105-8","","10.1145/3106237.3106291","https://dl.acm.org/doi/10.1145/3106237.3106291","Data provenance tracking determines the set of inputs related to a given output. It enables quality control and problem diagnosis in data engineering. Most existing techniques work by tracking program dependencies. They cannot quantitatively assess the importance of related inputs, which is critical to machine learning algorithms, in which an output tends to depend on a huge set of inputs while only some of them are of importance. In this paper, we propose LAMP, a provenance computation system for machine learning algorithms. Inspired by automatic differentiation (AD), LAMP quantifies the importance of an input for an output by computing the partial derivative. LAMP separates the original data processing and the more expensive derivative computation to different processes to achieve cost-effectiveness. In addition, it allows quantifying importance for inputs related to discrete behavior, such as control flow selection. The evaluation on a set of real world programs and data sets illustrates that LAMP produces more precise and succinct provenance than program dependence based techniques, with much less overhead. Our case studies demonstrate the potential of LAMP in problem diagnosis in data engineering.","2017-08-21","2023-05-29 07:11:51","2023-05-29 07:11:59","2023-05-29 07:11:51","786-797","","","","","","LAMP","","","","","ACM","Paderborn Germany","en","","","","","DOI.org (Crossref)","","15 citations (Crossref) [2023-05-29]","","C:\Users\ambreen.hanif\Zotero\storage\D56XEN5X\Ma et al. - 2017 - LAMP data provenance for graph based machine lear.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ESEC/FSE'17: Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering","","","","","","","","","","","","","","",""
"ARN687YG","conferencePaper","2020","Ma, Lin; Ding, Bailu; Das, Sudipto; Swaminathan, Adith","Active Learning for ML Enhanced Database Systems","Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data","978-1-4503-6735-6","","10.1145/3318464.3389768","https://dl.acm.org/doi/10.1145/3318464.3389768","Recent research has shown promising results by using machine learning (ML) techniques to improve the performance of database systems, e.g., in query optimization or index recommendation. However, in many production deployments, the ML models' performance degrades significantly when the test data diverges from the data used to train these models. In this paper, we address this performance degradation by using B-instances to collect additional data during deployment. We propose an active data collection platform, ADCP, that employs active learning (AL) to gather relevant data cost-effectively. We develop a novel AL technique, Holistic Active Learner (HAL), that robustly combines multiple noisy signals for data gathering in the context of database applications. HAL applies to various ML tasks, budget sizes, cost types, and budgeting interfaces for database applications. We evaluate ADCP on both industry-standard benchmarks and real customer workloads. Our evaluation shows that, compared with other baselines, our technique improves ML models' prediction performance by up to 2x with the same cost budget. In particular, on production workloads, our technique reduces the prediction error of ML models by 75% using about 100 additionally collected queries.","2020-05-31","2023-05-29 07:10:56","2023-05-29 07:11:02","2023-05-29","175–191","","","","","","","SIGMOD '20","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","29 citations (Crossref) [2023-05-29]","","C:\Users\ambreen.hanif\Zotero\storage\WS3VD3WF\Ma et al_2020_Active Learning for ML Enhanced Database Systems.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9JN8Z7BP","webpage","","","(13) (PDF) Combining P-Plan and the REPRODUCE-ME Ontology to Achieve Semantic Enrichment of Scientific Experiments Using Interactive Notebooks: ESWC 2018 Satellite Events, Heraklion, Crete, Greece, June 3-7, 2018, Revised Selected Papers","","","","","https://www.researchgate.net/publication/326758287_Combining_P-Plan_and_the_REPRODUCE-ME_Ontology_to_Achieve_Semantic_Enrichment_of_Scientific_Experiments_Using_Interactive_Notebooks_ESWC_2018_Satellite_Events_Heraklion_Crete_Greece_June_3-7_2018_Revi","","","2023-05-29 07:08:46","2023-05-29 07:08:46","2023-05-29 07:08:46","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\AWZEML28\326758287_Combining_P-Plan_and_the_REPRODUCE-ME_Ontology_to_Achieve_Semantic_Enrichment_of_Scie.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3D4CUVAM","webpage","","","Samuel, S.; König-Ries, B.: ProvBook: Provenance-based Semantic Enrichment of Interactive Notebooks for Reproducibility. In: ISWC (P&D/Industry/BlueSky). 2018. - Google Search","","","","","https://www.google.com/search?q=Samuel%2C+S.%3B+K%C3%B6nig-Ries%2C+B.%3A+ProvBook%3A+Provenance-based+Semantic+Enrichment+of+Interactive+Notebooks+for+Reproducibility.+In%3A+ISWC+(P%26D%2FIndustry%2FBlueSky).+2018.&rlz=1C1GCEB_enAU939AU939&oq=Samuel%2C+S.%3B+K%C3%B6nig-Ries%2C+B.%3A+ProvBook%3A+Provenance-based+Semantic+Enrichment+of+Interactive+Notebooks+for+Reproducibility.+In%3A+ISWC+(P%26D%2FIndustry%2FBlueSky).+2018.&aqs=chrome..69i57.397j0j4&sourceid=chrome&ie=UTF-8","","","2023-05-29 06:37:40","2023-05-29 06:37:40","2023-05-29 06:37:40","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\ZELMCQBP\search.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M6EGGS4Q","webpage","","","[PDF] Understanding experiments and research practices for reproducibility: an exploratory study | Semantic Scholar","","","","","https://www.semanticscholar.org/paper/Understanding-experiments-and-research-practices-an-Samuel-K%C3%B6nig%E2%80%90Ries/61ede3c2db7ceec8b4518bc21fe687e502dcfaf5","","","2023-05-29 06:37:21","2023-05-29 06:37:21","2023-05-29 06:37:21","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\PGNHN3U9\61ede3c2db7ceec8b4518bc21fe687e502dcfaf5.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LX9AKXUN","journalArticle","2021","Samuel, Sheeba; König-Ries, Birgitta","Understanding experiments and research practices for reproducibility: an exploratory study","PeerJ","","2167-8359","10.7717/peerj.11140","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8067906/","Scientific experiments and research practices vary across disciplines. The research practices followed by scientists in each domain play an essential role in the understandability and reproducibility of results. The “Reproducibility Crisis”, where researchers find difficulty in reproducing published results, is currently faced by several disciplines. To understand the underlying problem in the context of the reproducibility crisis, it is important to first know the different research practices followed in their domain and the factors that hinder reproducibility. We performed an exploratory study by conducting a survey addressed to researchers representing a range of disciplines to understand scientific experiments and research practices for reproducibility. The survey findings identify a reproducibility crisis and a strong need for sharing data, code, methods, steps, and negative and positive results. Insufficient metadata, lack of publicly available data, and incomplete information in study methods are considered to be the main reasons for poor reproducibility. The survey results also address a wide number of research questions on the reproducibility of scientific results. Based on the results of our explorative study and supported by the existing published literature, we offer general recommendations that could help the scientific community to understand, reproduce, and reuse experimental data and results in the research data lifecycle.","2021-04-21","2023-05-29 06:36:56","2023-05-29 06:37:04","2023-05-29 06:36:50","e11140","","","9","","PeerJ","Understanding experiments and research practices for reproducibility","","","","","","","","","","","","PubMed Central","","11 citations (Crossref) [2023-05-29] PMID: 33976964 PMCID: PMC8067906","","; C:\Users\ambreen.hanif\Zotero\storage\XMP28RQ9\Samuel_König-Ries_2021_Understanding experiments and research practices for reproducibility.pdf","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8067906/","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ULRV6F3G","preprint","2020","Samuel, Sheeba; Löffler, Frank; König-Ries, Birgitta","Machine Learning Pipelines: Provenance, Reproducibility and FAIR Data Principles","","","","10.48550/arXiv.2006.12117","http://arxiv.org/abs/2006.12117","Machine learning (ML) is an increasingly important scientific tool supporting decision making and knowledge generation in numerous fields. With this, it also becomes more and more important that the results of ML experiments are reproducible. Unfortunately, that often is not the case. Rather, ML, similar to many other disciplines, faces a reproducibility crisis. In this paper, we describe our goals and initial steps in supporting the end-to-end reproducibility of ML pipelines. We investigate which factors beyond the availability of source code and datasets influence reproducibility of ML experiments. We propose ways to apply FAIR data practices to ML workflows. We present our preliminary results on the role of our tool, ProvBook, in capturing and comparing provenance of ML experiments and their reproducibility using Jupyter Notebooks.","2020-06-22","2023-05-29 06:36:00","2023-05-29 06:36:00","2023-05-29 06:36:00","","","","","","","Machine Learning Pipelines","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2006.12117 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\DW9YGN5P\2006.html; C:\Users\ambreen.hanif\Zotero\storage\IY4Y36CS\Samuel et al_2020_Machine Learning Pipelines.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2006.12117","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DJ24UBNG","journalArticle","2017","Herschel, Melanie; Diestelkämper, Ralf; Ben Lahmar, Houssem","A survey on provenance: What for? What form? What from?","The VLDB Journal — The International Journal on Very Large Data Bases","","1066-8888","10.1007/s00778-017-0486-1","https://dl.acm.org/doi/10.1007/s00778-017-0486-1","Provenance refers to any information describing the production process of an end product, which can be anything from a piece of digital data to a physical object. While this survey focuses on the former type of end product, this definition still leaves room for many different interpretations of and approaches to provenance. These are typically motivated by different application domains for provenance (e.g., accountability, reproducibility, process debugging) and varying technical requirements such as runtime, scalability, or privacy. As a result, we observe a wide variety of provenance types and provenance-generating methods. This survey provides an overview of the research field of provenance, focusing on what provenance is used for (what for?), what types of provenance have been defined and captured for the different applications (what form?), and which resources and system requirements impact the choice of deploying a particular provenance solution (what from?). For each of these three key questions, we provide a classification and review the state of the art for each class. We conclude with a summary and possible future research challenges.","2017-12-01","2023-05-29 06:02:57","2023-05-29 06:03:01","2023-05-29 06:02:57","881–906","","6","26","","The VLDB Journal","A survey on provenance","","","","","","","","","","","","ACM Digital Library","","139 citations (Crossref) [2023-05-29]","","C:\Users\ambreen.hanif\Zotero\storage\87CAYYA9\Herschel et al_2017_A survey on provenance.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3EKX9PCI","preprint","2016","Missier, Paolo","The lifecycle of provenance metadata and its associated challenges and opportunities","","","","10.48550/arXiv.1605.01229","http://arxiv.org/abs/1605.01229","This chapter outlines some of the challenges and opportunities associated with adopting provenance principles and standards in a variety of disciplines, including data publication and reuse, and information sciences.","2016-05-04","2023-05-29 05:56:00","2023-05-29 05:56:00","2023-05-29 05:55:48","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1605.01229 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\PR36MCVH\1605.html; C:\Users\ambreen.hanif\Zotero\storage\XQXPSUUW\Missier_2016_The lifecycle of provenance metadata and its associated challenges and.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1605.01229","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IVCNH2ED","conferencePaper","2013","Missier, Paolo; Belhajjame, Khalid; Cheney, James","The W3C PROV family of specifications for modelling provenance metadata","Proceedings of the 16th International Conference on Extending Database Technology","978-1-4503-1597-5","","10.1145/2452376.2452478","https://dl.acm.org/doi/10.1145/2452376.2452478","Provenance, a form of structured metadata designed to record the origin or source of information, can be instrumental in deciding whether information is to be trusted, how it can be integrated with other diverse information sources, and how to establish attribution of information to authors throughout its history. The PROV set of specifications, produced by the World Wide Web Consortium (W3C), is designed to promote the publication of provenance information on the Web, and offers a basis for interoperability across diverse provenance management systems. The PROV provenance model is deliberately generic and domain-agnostic, but extension mechanisms are available and can be exploited for modelling specific domains. This tutorial provides an account of these specifications. Starting from intuitive and informal examples that present idiomatic provenance patterns, it progressively introduces the relational model of provenance along with the constraints model for validation of provenance documents, and concludes with example applications that show the extension points in use.","2013-03-18","2023-05-29 05:46:49","2023-05-29 05:47:05","2023-05-28","773–776","","","","","","","EDBT '13","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","101 citations (Crossref) [2023-05-29]","","C:\Users\ambreen.hanif\Zotero\storage\26JKS698\Missier et al_2013_The W3C PROV family of specifications for modelling provenance metadata.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CA45SZ3L","webpage","2013","Lebo, Tim; Sahoo, Satya; McGuinness, Deborah; Belhajjame, Khalid; Cheney, James; Corsar, David; Garijo, Daniel; Soiland-Reyes, Stian; Zednik, Stephan; Zhao, Jun","PROV-O: The PROV ontology","","","","","https://www.w3.org/TR/prov-o/","","2013-04-30","2023-05-29 05:29:26","2023-05-29 05:41:42","","","","","","","","","","","","","","","English","","","","","","","Type: Other","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BV476HNT","webpage","2013","Moreau, Luc; Missier, Paolo; Belhajjame, Khalid; BFar, Reza; Cheney, James; Coppens, Sam; Cresswell, Stephen; Gil, Yolanda; Groth, Paul; Klyne, Graham; Lebo, Timothy; McCusker, Jim; Miles, Simon; Myers, James; Sahoo, Satya; Tilmes, Curt","PROV-DM: The PROV data model","","","","","http://www.w3.org/TR/prov-dm/","","2013","2023-05-29 05:29:23","2023-05-29 05:38:51","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IN6H878V","journalArticle","2023","Ali, Sajid; Abuhmed, Tamer; El-Sappagh, Shaker; Muhammad, Khan; Alonso-Moral, Jose M.; Confalonieri, Roberto; Guidotti, Riccardo; Ser, Javier Del; Díaz-Rodríguez, Natalia; Herrera, Francisco","Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence","Information Fusion","","1566-2535","10.1016/j.inffus.2023.101805","https://www.sciencedirect.com/science/article/pii/S1566253523001148","Artificial intelligence (AI) is currently being utilized in a wide range of sophisticated applications, but the outcomes of many AI models are challenging to comprehend and trust due to their black-box nature. Usually, it is essential to understand the reasoning behind an AI model’s decision-making. Thus, the need for eXplainable AI (XAI) methods for improving trust in AI models has arisen. XAI has become a popular research subject within the AI field in recent years. Existing survey papers have tackled the concepts of XAI, its general terms, and post-hoc explainability methods but there have not been any reviews that have looked at the assessment methods, available tools, XAI datasets, and so on. Therefore, in this comprehensive study, we provide readers with an overview of the current research and trends in this rapidly emerging area with a case study example. The review starts by explaining the background of XAI, common definitions, and summarizing recently proposed techniques in XAI for supervised machine learning. The review divides XAI techniques into four axes using a hierarchical categorization system: (i) data explainability, (ii) model explainability, (iii) post-hoc explainability, and (iv) assessment of explanations. We also introduce available evaluation metrics as well as open-source packages and datasets with future research directions. Then, the significance of explainability in terms of legal demands, user viewpoints, and application orientation is outlined, termed as XAI concerns. This paper advocates for tailoring explanation content to specific user types. An examination of XAI techniques and evaluation was conducted by looking at 410 critical articles, published between January 2016 and October 2022, in reputed journals and using a wide range of research databases as a source of information. The article is aimed at XAI researchers who are interested in making their AI models more trustworthy, as well as towards researchers from other disciplines who are looking for effective XAI methods to complete tasks with confidence while communicating meaning from data.","2023-04-18","2023-05-09 04:01:28","2023-05-29 02:31:57","2023-05-09 04:01:28","101805","","","","","Information Fusion","Explainable Artificial Intelligence (XAI)","","","","","","","en","","","","","ScienceDirect","","0 citations (Crossref) [2023-05-09]","","C:\Users\ambreen.hanif\Zotero\storage\2HMLS6UB\Ali et al_2023_Explainable Artificial Intelligence (XAI).pdf; C:\Users\ambreen.hanif\Zotero\storage\L88RFHTM\S1566253523001148.html","","survey; first-pass","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GQUFNWQ7","journalArticle","2023","Li, Bo; Qi, Peng; Liu, Bo; Di, Shuai; Liu, Jingen; Pei, Jiquan; Yi, Jinfeng; Zhou, Bowen","Trustworthy AI: From Principles to Practices","ACM Computing Surveys","","0360-0300, 1557-7341","10.1145/3555803","https://dl.acm.org/doi/10.1145/3555803","The rapid development of Artificial Intelligence (AI) technology has enabled the deployment of various systems based on it. However, many current AI systems are found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection. These shortcomings degrade user experience and erode people’s trust in all AI systems. In this review, we provide AI practitioners with a comprehensive guide for building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, and accountability. To unify currently available but fragmented approaches toward trustworthy AI, we organize them in a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to system development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items for practitioners and societal stakeholders (e.g., researchers, engineers, and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges for the future development of trustworthy AI systems, where we identify the need for a paradigm shift toward comprehensively trustworthy AI systems.","2023-09-30","2023-04-03 02:46:19","2023-05-29 02:30:58","2023-04-03 02:46:19","1-46","","9","55","","ACM Comput. Surv.","Trustworthy AI","","","","","","","en","","","","","DOI.org (Crossref)","","6 citations (Crossref) [2023-04-03]","","C:\Users\ambreen.hanif\Zotero\storage\MCEVSIDB\Li et al_2023_Trustworthy AI.pdf; ","notion://www.notion.so/cite-li_trustworthy_2023-42cd177f62d8471b9feef7a42f40073e","firstpass","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X47SZE5U","preprint","2022","Nakao, Yuri; Strappelli, Lorenzo; Stumpf, Simone; Naseer, Aisha; Regoli, Daniele; Del Gamba, Giulia","Towards Responsible AI: A Design Space Exploration of Human-Centered Artificial Intelligence User Interfaces to Investigate Fairness","","","","10.48550/arXiv.2206.00474","http://arxiv.org/abs/2206.00474","With Artificial intelligence (AI) to aid or automate decision-making advancing rapidly, a particular concern is its fairness. In order to create reliable, safe and trustworthy systems through human-centred artificial intelligence (HCAI) design, recent efforts have produced user interfaces (UIs) for AI experts to investigate the fairness of AI models. In this work, we provide a design space exploration that supports not only data scientists but also domain experts to investigate AI fairness. Using loan applications as an example, we held a series of workshops with loan officers and data scientists to elicit their requirements. We instantiated these requirements into FairHIL, a UI to support human-in-the-loop fairness investigations, and describe how this UI could be generalized to other use cases. We evaluated FairHIL through a think-aloud user study. Our work contributes better designs to investigate an AI model's fairness-and move closer towards responsible AI.","2022-06-01","2023-05-25 02:11:26","2023-05-25 03:28:45","2023-05-25 02:11:22","","","","","","","Towards Responsible AI","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2206.00474 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\85R5JQAW\2206.html; C:\Users\ambreen.hanif\Zotero\storage\2FBFKSE9\Nakao et al_2022_Towards Responsible AI.pdf; ","notion://www.notion.so/cite-nakao_towards_2022-1-2bd41655a52f434c960340a5a92a471b","notion","","","","","","","","","","","","","","","","","","","","arXiv:2206.00474","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MIC4SC99","preprint","2023","Chen, Chen; Fu, Jie; Lyu, Lingjuan","A Pathway Towards Responsible AI Generated Content","","","","10.48550/arXiv.2303.01325","http://arxiv.org/abs/2303.01325","AI Generated Content (AIGC) has received tremendous attention within the past few years, with content ranging from image, text, to audio, video, etc. Meanwhile, AIGC has become a double-edged sword and recently received much criticism regarding its responsible usage. In this vision paper, we focus on three main concerns that may hinder the healthy development and deployment of AIGC in practice, including risks from privacy, bias, toxicity, misinformation, and intellectual property (IP). By documenting known and potential risks, as well as any possible misuse scenarios of AIGC, the aim is to draw attention to potential risks and misuse, help society to eliminate obstacles, and promote the more ethical and secure deployment of AIGC. Additionally, we provide insights into the promising directions for tackling these risks while constructing generative models, enabling AIGC to be used responsibly to benefit society.","2023-03-17","2023-05-25 02:11:07","2023-05-25 02:12:19","2023-05-25 02:11:07","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2303.01325 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\TKDRXWTU\2303.html; C:\Users\ambreen.hanif\Zotero\storage\Q5BQFBHB\Chen et al_2023_A Pathway Towards Responsible AI Generated Content.pdf; ","notion://www.notion.so/Chen-et-al-2023-718ac25f3578434c9dd2ead3de655e73","notion","","","","","","","","","","","","","","","","","","","","arXiv:2303.01325","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DLDT9YIA","conferencePaper","2019","Hind, Michael; Wei, Dennis; Campbell, Murray; Codella, Noel C. F.; Dhurandhar, Amit; Mojsilović, Aleksandra; Natesan Ramamurthy, Karthikeyan; Varshney, Kush R.","TED: Teaching AI to Explain its Decisions","Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society","978-1-4503-6324-2","","10.1145/3306618.3314273","https://dl.acm.org/doi/10.1145/3306618.3314273","Artificial intelligence systems are being increasingly deployed due to their potential to increase the efficiency, scale, consistency, fairness, and accuracy of decisions. However, as many of these systems are opaque in their operation, there is a growing demand for such systems to provide explanations for their decisions. Conventional approaches to this problem attempt to expose or discover the inner workings of a machine learning model with the hope that the resulting explanations will be meaningful to the consumer. In contrast, this paper suggests a new approach to this problem. It introduces a simple, practical framework, called Teaching Explanations for Decisions (TED), that provides meaningful explanations that match the mental model of the consumer. We illustrate the generality and effectiveness of this approach with two different examples, resulting in highly accurate explanations with no loss of prediction accuracy for these two examples.","2019-01-27","2023-05-25 02:06:29","2023-05-25 02:06:29","2023-05-24","123–129","","","","","","TED","AIES '19","","","","Association for Computing Machinery","New York, NY, USA","","","","","","ACM Digital Library","","","","C:\Users\ambreen.hanif\Zotero\storage\M9PFXSBK\Hind et al_2019_TED.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z67KCHXL","webpage","","","TED | Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society","","","","","https://dl-acm-org.simsrad.net.ocs.mq.edu.au/doi/10.1145/3306618.3314273","","","2023-05-25 02:05:45","2023-05-25 02:06:06","2023-05-25 02:05:45","","","","","","","","","","","","","","","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\GHGCBHTA\3306618.html","notion://www.notion.so/TED-Proceedings-of-the-2019-AAAI-ACM-Conference-on-AI-Ethics-and-Society-n-d-9ddf1f0d3a7b4a90b30d5e1be0eb3f6f","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VRWBHWYY","journalArticle","2018","Xie, Tian; Grossman, Jeffrey C.","Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties","Physical Review Letters","","0031-9007, 1079-7114","10.1103/PhysRevLett.120.145301","http://arxiv.org/abs/1710.10324","The use of machine learning methods for accelerating the design of crystalline materials usually requires manually constructed feature vectors or complex transformation of atom coordinates to input the crystal structure, which either constrains the model to certain crystal types or makes it difficult to provide chemical insights. Here, we develop a crystal graph convolutional neural networks framework to directly learn material properties from the connection of atoms in the crystal, providing a universal and interpretable representation of crystalline materials. Our method provides a highly accurate prediction of density functional theory calculated properties for eight different properties of crystals with various structure types and compositions after being trained with $10^4$ data points. Further, our framework is interpretable because one can extract the contributions from local chemical environments to global properties. Using an example of perovskites, we show how this information can be utilized to discover empirical rules for materials design.","2018-04-06","2023-05-25 02:05:28","2023-05-25 02:05:33","2023-05-25 02:05:28","145301","","14","120","","Phys. Rev. Lett.","","","","","","","","","","","","","arXiv.org","","787 citations (Crossref) [2023-05-25] arXiv:1710.10324 [cond-mat]","","C:\Users\ambreen.hanif\Zotero\storage\MXTWVZF3\1710.html; C:\Users\ambreen.hanif\Zotero\storage\LQY42559\Xie_Grossman_2018_Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"55Z6AIYK","journalArticle","2018","Xie, Tian; Grossman, Jeffrey C.","Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties","Physical Review Letters","","0031-9007, 1079-7114","10.1103/PhysRevLett.120.145301","http://arxiv.org/abs/1710.10324","The use of machine learning methods for accelerating the design of crystalline materials usually requires manually constructed feature vectors or complex transformation of atom coordinates to input the crystal structure, which either constrains the model to certain crystal types or makes it difficult to provide chemical insights. Here, we develop a crystal graph convolutional neural networks framework to directly learn material properties from the connection of atoms in the crystal, providing a universal and interpretable representation of crystalline materials. Our method provides a highly accurate prediction of density functional theory calculated properties for eight different properties of crystals with various structure types and compositions after being trained with $10^4$ data points. Further, our framework is interpretable because one can extract the contributions from local chemical environments to global properties. Using an example of perovskites, we show how this information can be utilized to discover empirical rules for materials design.","2018-04-06","2023-05-25 01:55:18","2023-05-25 01:56:34","2023-05-25 01:55:18","145301","","14","120","","Phys. Rev. Lett.","","","","","","","","","","","","","arXiv.org","","787 citations (Crossref) [2023-05-25] arXiv:1710.10324 [cond-mat]","","C:\Users\ambreen.hanif\Zotero\storage\SCJHHI9V\1710.html; ; C:\Users\ambreen.hanif\Zotero\storage\NKNECTQL\Xie_Grossman_2018_Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable.pdf","notion://www.notion.so/Xie-Grossman-2018-8c20d374cf48460d90e7e39088251f4a","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VAWMDMGS","preprint","2018","Veličković, Petar; Cucurull, Guillem; Casanova, Arantxa; Romero, Adriana; Liò, Pietro; Bengio, Yoshua","Graph Attention Networks","","","","10.48550/arXiv.1710.10903","http://arxiv.org/abs/1710.10903","We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).","2018-02-04","2023-05-25 01:55:08","2023-05-25 01:56:33","2023-05-25 01:55:08","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1710.10903 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\HIKA9US2\1710.html; ; C:\Users\ambreen.hanif\Zotero\storage\9GB65JQD\Veličković et al_2018_Graph Attention Networks.pdf","notion://www.notion.so/Veli-kovi-et-al-2018-6ddd9737884e4337abace46bdf8d3496","notion","","","","","","","","","","","","","","","","","","","","arXiv:1710.10903","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YAQKNTIK","preprint","2018","Neil, Daniel; Briody, Joss; Lacoste, Alix; Sim, Aaron; Creed, Paidi; Saffari, Amir","Interpretable Graph Convolutional Neural Networks for Inference on Noisy Knowledge Graphs","","","","10.48550/arXiv.1812.00279","http://arxiv.org/abs/1812.00279","In this work, we provide a new formulation for Graph Convolutional Neural Networks (GCNNs) for link prediction on graph data that addresses common challenges for biomedical knowledge graphs (KGs). We introduce a regularized attention mechanism to GCNNs that not only improves performance on clean datasets, but also favorably accommodates noise in KGs, a pervasive issue in real-world applications. Further, we explore new visualization methods for interpretable modelling and to illustrate how the learned representation can be exploited to automate dataset denoising. The results are demonstrated on a synthetic dataset, the common benchmark dataset FB15k-237, and a large biomedical knowledge graph derived from a combination of noisy and clean data sources. Using these improvements, we visualize a learned model's representation of the disease cystic fibrosis and demonstrate how to interrogate a neural network to show the potential of PPARG as a candidate therapeutic target for rheumatoid arthritis.","2018-12-01","2023-05-25 01:55:06","2023-05-25 01:56:20","2023-05-25 01:55:06","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1812.00279 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\M5YBV3HQ\1812.html; C:\Users\ambreen.hanif\Zotero\storage\HARLZHSP\Neil et al_2018_Interpretable Graph Convolutional Neural Networks for Inference on Noisy.pdf; ","notion://www.notion.so/Neil-et-al-2018-a2103924ff57450ba6f319ddaf3bc5fc","notion","","","","","","","","","","","","","","","","","","","","arXiv:1812.00279","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"73BSCDKX","conferencePaper","2023","Grafberger, Stefan; Groth, Paul; Schelter, Sebastian","Provenance Tracking for End-to-End Machine Learning Pipelines","Companion Proceedings of the ACM Web Conference 2023","978-1-4503-9419-2","","10.1145/3543873.3587557","https://dl.acm.org/doi/10.1145/3543873.3587557","","2023-04-30","2023-05-25 01:46:21","2023-05-25 01:46:45","2023-05-25 01:46:20","1512-1512","","","","","","","","","","","ACM","Austin TX USA","en","","","","","DOI.org (Crossref)","","0 citations (Crossref) [2023-05-25]","","","notion://www.notion.so/Grafberger-et-al-2023-f50ebdb35c094124b50b3a9b63b2408e","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","WWW '23: The ACM Web Conference 2023","","","","","","","","","","","","","","",""
"4PJVMSAY","thesis","2023","Jackson, Jon-Nicklaus Z.","Exploring Training Provenance for Clues of Data Poisoning in Machine Learning","","","","","http://deepblue.lib.umich.edu/handle/2027.42/176346","Machine Learning today plays a vital role in a wide range of critical applications. To ensure ML models are consistently able to produce correct output, such models must be retrained as new input samples become available to avoid performance degradation as a result of data drift. During retraining, such models are left vulnerable to possible injection of poisonous samples via data poisoning attacks, where adversaries manipulate the training data to have the ML model misbehave to serve adversarial goals. Inspired by previous works that leverage data provenance for detecting and filtering out poisonous data from the training set, we adapt the definition of provenance for what we define as training provenance, the history of training metrics captured over training time-frame. We then build a framework that allows us to capture both key performance metrics for the overall dataset and per-class metrics for each label at every epoch as training provenance. Through exploratory analysis of captured training provenance we aim to find clues that standout to serve as strong signals for making a call on training data poisoning. We evaluate our proposed framework on two benchmark image classification datasets: MNIST and CIFAR-10. For MNIST, we observed promising signal(s) for establishing a per-epoch poisoning detection threshold based on captured metrics at the overall dataset level. For CIFAR-10, captured overall dataset metrics as training provenance for clean and poisoned training data were not as effective as compared to our observations for MNIST experiments. As for captured per-class metrics, we discovered that these metrics provided little insight as a result of the nondeterministic nature of machine learning. Overall, we observe that this is a promising direction that invites further exploration with more poisoning attacks and diverse datasets.","2023-04-30","2023-05-25 01:45:23","2023-05-25 01:45:51","2023-05-25 01:45:23","","","","","","","","","","","","","","English","","Thesis","","","deepblue.lib.umich.edu","","Accepted: 2023-05-02T14:27:55Z DOI: 10.7302/7196","","C:\Users\ambreen.hanif\Zotero\storage\LATSQY7X\Jackson_2023_Exploring Training Provenance for Clues of Data Poisoning in Machine Learning.pdf; ","notion://www.notion.so/Jackson-2023-9b2d940e6d9f46bebb510ecf353b94d4","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B5AZYXAY","book","2023","Mustafa, Tarek Al; König-Ries, Birgitta; Samuel, Sheeba","MLProvCodeGen: A Tool for Provenance Data Input and Capture of Customizable Machine Learning Scripts","","978-3-88579-725-8","","","http://dl.gi.de/handle/20.500.12116/40382","Over the last decade Machine learning (ML) has dramatically changed the application ofand research in computer science. It becomes increasingly complicated to assure the transparency and reproducibility of advanced ML systems from raw data to deployment. In this paper, we describe an approach to supply users with an interface to specify a variety of parameters that together provide complete provenance information and automatically generate executable ML code from this information. We introduce MLProvCodeGen (Machine Learning Provenance Code Generator), a JupyterLab extension to generate custom code for ML experiments from user-defined metadata. ML workflows can be generated with different data settings, model parameters, methods, and trainingparameters and reproduce results in Jupyter Notebooks. We evaluated our approach with two ML applications, image and multiclass classification, and conducted a user evaluation.","2023","2023-05-25 01:45:11","2023-05-25 01:45:43","2023-05-25 01:45:11","","","","","","","MLProvCodeGen","","","","","Gesellschaft für Informatik e.V.","","en","","","","","dl.gi.de","","Accepted: 2023-02-23T14:00:19Z DOI: 10.18420/BTW2023-72","","C:\Users\ambreen.hanif\Zotero\storage\CDY7Z8ZR\Mustafa et al_2023_MLProvCodeGen.pdf; ","notion://www.notion.so/Mustafa-et-al-2023-1e3ed7b507cc4b1d8859423214c7cbf9","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SD6BTSPN","journalArticle","2023","Schlegel, Marius; Sattler, Kai-Uwe","Management of Machine Learning Lifecycle Artifacts: A Survey","ACM SIGMOD Record","","0163-5808","10.1145/3582302.3582306","https://dl.acm.org/doi/10.1145/3582302.3582306","The explorative and iterative nature of developing and operating ML applications leads to a variety of artifacts, such as datasets, features, models, hyperparameters, metrics, software, configurations, and logs. In order to enable comparability, reproducibility, and traceability of these artifacts across the ML lifecycle steps and iterations, systems and tools have been developed to support their collection, storage, and management. It is often not obvious what precise functional scope such systems offer so that the comparison and the estimation of synergy effects between candidates are quite challenging. In this paper, we aim to give an overview of systems and platforms which support the management of ML lifecycle artifacts. Based on a systematic literature review, we derive assessment criteria and apply them to a representative selection of more than 60 systems and platforms.","2023-01-25","2023-05-25 01:45:20","2023-05-25 01:45:35","2023-05-25 01:45:14","18–35","","4","51","","SIGMOD Rec.","Management of Machine Learning Lifecycle Artifacts","","","","","","","","","","","","ACM Digital Library","","1 citations (Crossref) [2023-05-25]","","; C:\Users\ambreen.hanif\Zotero\storage\B8LHCYZN\Schlegel_Sattler_2023_Management of Machine Learning Lifecycle Artifacts.pdf","notion://www.notion.so/Schlegel-Sattler-2023-8a1527c97695472cac24d4e54f345ffb","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TCM373LI","journalArticle","2019","Jobin, Anna; Ienca, Marcello; Vayena, Effy","The global landscape of AI ethics guidelines","Nature Machine Intelligence","","2522-5839","10.1038/s42256-019-0088-2","https://www.nature.com/articles/s42256-019-0088-2","In the past five years, private companies, research institutions and public sector organizations have issued principles and guidelines for ethical artificial intelligence (AI). However, despite an apparent agreement that AI should be ‘ethical’, there is debate about both what constitutes ‘ethical AI’ and which ethical requirements, technical standards and best practices are needed for its realization. To investigate whether a global agreement on these questions is emerging, we mapped and analysed the current corpus of principles and guidelines on ethical AI. Our results reveal a global convergence emerging around five ethical principles (transparency, justice and fairness, non-maleficence, responsibility and privacy), with substantive divergence in relation to how these principles are interpreted, why they are deemed important, what issue, domain or actors they pertain to, and how they should be implemented. Our findings highlight the importance of integrating guideline-development efforts with substantive ethical analysis and adequate implementation strategies.","2019-09","2023-05-25 01:25:57","2023-05-25 01:27:21","2023-05-25 01:25:57","389-399","","9","1","","Nat Mach Intell","","","","","","","","en","2019 Springer Nature Limited","","","","www-nature-com.simsrad.net.ocs.mq.edu.au","","1035 citations (Crossref) [2023-05-25] Number: 9 Publisher: Nature Publishing Group","","C:\Users\ambreen.hanif\Zotero\storage\6D7Q9Z2K\Jobin et al_2019_The global landscape of AI ethics guidelines.pdf; ","notion://www.notion.so/Jobin-et-al-2019-496794dea6c44288b53cd9e5fa9cf80a","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RV3DR3ZI","preprint","2021","Kokhlikyan, Narine; Miglani, Vivek; Alsallakh, Bilal; Martin, Miguel; Reblitz-Richardson, Orion","Investigating sanity checks for saliency maps with image and text classification","","","","10.48550/arXiv.2106.07475","http://arxiv.org/abs/2106.07475","Saliency maps have shown to be both useful and misleading for explaining model predictions especially in the context of images. In this paper, we perform sanity checks for text modality and show that the conclusions made for image do not directly transfer to text. We also analyze the effects of the input multiplier in certain saliency maps using similarity scores, max-sensitivity and infidelity evaluation metrics. Our observations reveal that the input multiplier carries input's structural patterns in explanation maps, thus leading to similar results regardless of the choice of model parameters. We also show that the smoothness of a Neural Network (NN) function can affect the quality of saliency-based explanations. Our investigations reveal that replacing ReLUs with Softplus and MaxPool with smoother variants such as LogSumExp (LSE) can lead to explanations that are more reliable based on the infidelity evaluation metric.","2021-06-08","2023-05-25 01:23:37","2023-05-25 01:25:14","2023-05-25 01:23:37","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2106.07475 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\2M3KKRDE\2106.html; C:\Users\ambreen.hanif\Zotero\storage\A4BAW6S4\Kokhlikyan et al_2021_Investigating sanity checks for saliency maps with image and text classification.pdf; ","notion://www.notion.so/Kokhlikyan-et-al-2021-3538967be7684ddfba4319966758660b","notion","","","","","","","","","","","","","","","","","","","","arXiv:2106.07475","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CNDUZ9FB","conferencePaper","2017","Kim, Jinkyu; Canny, John","Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention","2017 IEEE International Conference on Computer Vision (ICCV)","","","10.1109/ICCV.2017.320","","Deep neural perception and control networks are likely to be a key component of self-driving vehicles. These models need to be explainable - they should provide easy-tointerpret rationales for their behavior - so that passengers, insurance companies, law enforcement, developers etc., can understand what triggered a particular behavior. Here we explore the use of visual explanations. These explanations take the form of real-time highlighted regions of an image that causally influence the network’s output (steering control). Our approach is two-stage. In the first stage, we use a visual attention model to train a convolution network endto- end from images to steering angle. The attention model highlights image regions that potentially influence the network’s output. Some of these are true influences, but some are spurious. We then apply a causal filtering step to determine which input regions actually influence the output. This produces more succinct visual explanations and more accurately exposes the network’s behavior. We demonstrate the effectiveness of our model on three datasets totaling 16 hours of driving. We first show that training with attention does not degrade the performance of the end-to-end network. Then we show that the network causally cues on a variety of features that are used by humans while driving.","2017-10","2023-05-25 01:19:24","2023-05-25 01:19:39","","2961-2969","","","","","","","","","","","","","","","","","","IEEE Xplore","","149 citations (Crossref) [2023-05-25] ISSN: 2380-7504","","C:\Users\ambreen.hanif\Zotero\storage\S5H6VM9Z\8237582.html; C:\Users\ambreen.hanif\Zotero\storage\4ZJ6ZPCH\Kim_Canny_2017_Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2017 IEEE International Conference on Computer Vision (ICCV)","","","","","","","","","","","","","","",""
"24SYCJW6","conferencePaper","2019","Reiter, Ehud","Natural Language Generation Challenges for Explainable AI","Proceedings of the 1st Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence (NL4XAI 2019)","","","10.18653/v1/W19-8402","https://aclanthology.org/W19-8402","","2019","2023-05-25 01:04:04","2023-05-25 01:04:12","2023-05-25 01:04:03","3–7","","","","","","","","","","","Association for Computational Linguistics","","","","","","","ACLWeb","","10 citations (Crossref) [2023-05-25]","","C:\Users\ambreen.hanif\Zotero\storage\WRPD7ETA\Reiter_2019_Natural Language Generation Challenges for Explainable AI.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","INLG 2019","","","","","","","","","","","","","","",""
"H7CTYNAB","journalArticle","2018","Biecek, Przemyslaw","DALEX: Explainers for Complex Predictive Models in R","Journal of Machine Learning Research","","1533-7928","","http://jmlr.org/papers/v19/18-416.html","Predictive modeling is invaded by elastic, yet complex methods such as neural networks or ensembles (model stacking, boosting or bagging). Such methods are usually described by a large number of parameters or hyper parameters - a price that one needs to pay for elasticity. The very number of parameters makes models hard to understand. This paper describes a consistent collection of explainers for predictive models, a.k.a. black boxes. Each explainer is a technique for exploration of a black box model. Presented approaches are model-agnostic, what means that they extract useful information from any predictive method irrespective of its internal structure. Each explainer is linked with a specific aspect of a model. Some are useful in decomposing predictions, some serve better in understanding performance, while others are useful in understanding importance and conditional responses of a particular variable. Every explainer presented here works for a single model or for a collection of models. In the latter case, models can be compared against each other. Such comparison helps to find strengths and weaknesses of different models and gives additional tools for model validation. Presented explainers are implemented in the DALEX package for R. They are based on a uniform standardized grammar of model exploration which may be easily extended.","2018","2023-05-24 13:22:20","2023-05-24 23:22:47","2023-05-24 13:22:20","1-5","","84","19","","","DALEX","","","","","","","","","","","","jmlr.org","","","","C:\Users\ambreen.hanif\Zotero\storage\92VIMY9K\Biecek - 2018 - DALEX Explainers for Complex Predictive Models in.pdf; ","notion://www.notion.so/Biecek-2018-8326f941b25a40b7ad6a5f866b9b6011","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4IVFQQ7G","journalArticle","2019","Alber, Maximilian; Lapuschkin, Sebastian; Seegerer, Philipp; Hägele, Miriam; Schütt, Kristof T.; Montavon, Grégoire; Samek, Wojciech; Müller, Klaus-Robert; Dähne, Sven; Kindermans, Pieter-Jan","iNNvestigate Neural Networks!","Journal of Machine Learning Research","","1533-7928","","http://jmlr.org/papers/v20/18-540.html","In recent years, deep neural networks have revolutionized many application domains of machine learning and are key components of many critical decision or predictive processes. Therefore, it is crucial that domain specialists can understand and analyze actions and predictions, even of the most complex neural network architectures. Despite these arguments neural networks are often treated as black boxes. In the attempt to alleviate this shortcoming many analysis methods were proposed, yet the lack of reference implementations often makes a systematic comparison between the methods a major effort. The presented library innvestigate addresses this by providing a common interface and out-of-the-box implementation for many analysis methods, including the reference implementation for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the versatility of innvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures.","2019","2023-05-24 12:52:11","2023-05-24 23:22:46","2023-05-24 12:52:11","1-8","","93","20","","","","","","","","","","","","","","","jmlr.org","","","","C:\Users\ambreen.hanif\Zotero\storage\54FPHMBF\Alber et al. - 2019 - iNNvestigate Neural Networks!.pdf; ; C:\Users\ambreen.hanif\Zotero\storage\JIYLRBLD\innvestigate.html","notion://www.notion.so/Alber-et-al-2019-0c15ce5959a74f708127e87c076d4bb3","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VWYYI7GH","conferencePaper","2021","Castle, Steffen; Schwarzenberg, Robert; Pourvali, Mohsen","Detecting Covariate Drift with Explanations","Natural Language Processing and Chinese Computing","978-3-030-88483-3","","10.1007/978-3-030-88483-3_24","","Detecting when there is a domain drift between training and inference data is important for any model evaluated on data collected in real time. Many current data drift detection methods only utilize input features to detect domain drift. While effective, these methods disregard the model’s evaluation of the data, which may be a significant source of information about the data domain. We propose to use information from the model in the form of explanations, specifically gradient times input, in order to utilize this information. Following the framework of Rabanser et al. [11], we combine these explanations with two-sample tests in order to detect a shift in distribution between training and evaluation data. Promising initial experiments show that explanations provide useful information for detecting shift, which potentially improves upon the current state-of-the-art.","2021","2023-05-24 14:08:54","2023-05-24 23:22:46","","317-322","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","0 citations (Crossref) [2023-05-25]","","","notion://www.notion.so/Castle-et-al-2021-d39e1064a22c41b3b936c302a418bb01","notion","XAI; Data drift; Two-sample tests","Wang, Lu; Feng, Yansong; Hong, Yu; He, Ruifang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"47T7TAUH","conferencePaper","2021","Castle, Steffen; Schwarzenberg, Robert; Pourvali, Mohsen","Detecting Covariate Drift with Explanations","Natural Language Processing and Chinese Computing","978-3-030-88483-3","","10.1007/978-3-030-88483-3_24","","Detecting when there is a domain drift between training and inference data is important for any model evaluated on data collected in real time. Many current data drift detection methods only utilize input features to detect domain drift. While effective, these methods disregard the model’s evaluation of the data, which may be a significant source of information about the data domain. We propose to use information from the model in the form of explanations, specifically gradient times input, in order to utilize this information. Following the framework of Rabanser et al. [11], we combine these explanations with two-sample tests in order to detect a shift in distribution between training and evaluation data. Promising initial experiments show that explanations provide useful information for detecting shift, which potentially improves upon the current state-of-the-art.","2021","2023-05-24 14:13:17","2023-05-24 23:22:46","","317-322","","","","","","","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","0 citations (Crossref) [2023-05-25]","","C:\Users\ambreen.hanif\Zotero\storage\K5A7VBXL\Castle et al. - 2021 - Detecting Covariate Drift with Explanations.pdf; ","notion://www.notion.so/Castle-et-al-2021-23fa536ec79c4549a8e4bff3333b3046","notion","XAI; Data drift; Two-sample tests","Wang, Lu; Feng, Yansong; Hong, Yu; He, Ruifang","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MR8B428S","journalArticle","2017","Montavon, Grégoire; Lapuschkin, Sebastian; Binder, Alexander; Samek, Wojciech; Müller, Klaus-Robert","Explaining nonlinear classification decisions with deep Taylor decomposition","Pattern Recognition","","0031-3203","10.1016/j.patcog.2016.11.008","https://www.sciencedirect.com/science/article/pii/S0031320316303582","Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems such as image recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method called deep Taylor decomposition efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.","2017-05-01","2023-05-24 12:58:10","2023-05-24 23:22:43","2023-05-24 12:58:10","211-222","","","65","","Pattern Recognition","","","","","","","","en","","","","","ScienceDirect","","612 citations (Crossref) [2023-05-25]","","; C:\Users\ambreen.hanif\Zotero\storage\Y67KAMDF\Montavon et al. - 2017 - Explaining nonlinear classification decisions with.pdf; C:\Users\ambreen.hanif\Zotero\storage\R9VB2KZ5\S0031320316303582.html","notion://www.notion.so/Montavon-et-al-2017-ff2a98c2b8f540928388506752afe553","notion","Deep neural networks; Heatmapping; Image recognition; Relevance propagation; Taylor decomposition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I8GYSFKI","conferencePaper","2019","Fields, Tonya; Hsieh, George; Chenou, Jules","Mitigating Drift in Time Series Data with Noise Augmentation","2019 International Conference on Computational Science and Computational Intelligence (CSCI)","","","10.1109/CSCI49370.2019.00046","","Machine leaning (ML) models must be accurate to produce quality AI solutions. There must be high accuracy in the data and with the model that is built using the data. Online machine learning algorithms fits naturally with use cases that involves time series data. In online environments the data distribution can change over time producing what is known as concept drift. Real-life, real-time, machine learning algorithms operating in dynamic environments must be able to detect any drift or changes in the data distribution and adapt and update the ML model in the face of data that changes over time. In this paper we present the work of a simulated drift added to time series ML models. We simulate drift on Multiplayer perceptron (MLP), Long Short Term Memory (LSTM), Convolution Neural Networks (CNN) and Gated Recurrent Unit (GRU). Results show ML models with flavors of recurrent neural network (RNN) are less sensitive to drift compared to other models. By adding noise to the training set, we can recover accuracy of the model in the face of drift.","2019-12","2023-05-24 14:16:58","2023-05-24 23:22:43","","227-230","","","","","","","","","","","","","","","","","","IEEE Xplore","","13 citations (Crossref) [2023-05-25]","","C:\Users\ambreen.hanif\Zotero\storage\X3557RHT\9071001.html; ","notion://www.notion.so/Fields-et-al-2019-975145ebf8a645e1aea6f2a1e1a78f1d","notion","Data models; Predictive models; Adaptation models; Atmospheric modeling; data drift, machine learning, time series; Machine learning algorithms; Time series analysis; Training","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 International Conference on Computational Science and Computational Intelligence (CSCI)","","","","","","","","","","","","","","",""
"VAQV2GR3","conferencePaper","2019","Elsahar, Hady; Gallé, Matthias","To Annotate or Not? Predicting Performance Drop under Domain Shift","Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)","","","10.18653/v1/D19-1222","https://aclanthology.org/D19-1222","Performance drop due to domain-shift is an endemic problem for NLP models in production. This problem creates an urge to continuously annotate evaluation datasets to measure the expected drop in the model performance which can be prohibitively expensive and slow. In this paper, we study the problem of predicting the performance drop of modern NLP models under domain-shift, in the absence of any target domain labels. We investigate three families of methods (\mathcalH-divergence, reverse classification accuracy and confidence measures), show how they can be used to predict the performance drop and study their robustness to adversarial domain-shifts. Our results on sentiment classification and sequence labelling show that our method is able to predict performance drops with an error rate as low as 2.15% and 0.89% for sentiment analysis and POS tagging respectively.","2019-11","2023-05-24 14:20:59","2023-05-24 23:22:43","2023-05-24 14:20:59","2163–2173","","","","","","To Annotate or Not?","","","","","Association for Computational Linguistics","Hong Kong, China","","","","","","ACLWeb","","24 citations (Crossref) [2023-05-25]","","C:\Users\ambreen.hanif\Zotero\storage\TK5UPCEL\Elsahar and Gallé - 2019 - To Annotate or Not Predicting Performance Drop un.pdf; ","notion://www.notion.so/Elsahar-Gall-2019-2a6552d0f55b47b480a20c5911b8be73","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","EMNLP-IJCNLP 2019","","","","","","","","","","","","","","",""
"QHV59LIT","journalArticle","","Baniecki, Hubert; Kretowicz, Wojciech; Piatyszek, Piotr; Wisniewski, Jakub; Biecek, Przemyslaw","dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python","","","","","","In modern machine learning, we observe the phenomenon of opaqueness debt, which manifests itself by an increased risk of discrimination, lack of reproducibility, and deﬂated performance due to data drift. An increasing amount of available data and computing power results in the growing complexity of black-box predictive models. To manage these issues, good MLOps practice asks for better validation of model performance and fairness, higher explainability, and continuous monitoring. The necessity for deeper model transparency comes from both scientiﬁc and social domains and is also caused by emerging laws and regulations on artiﬁcial intelligence. To facilitate the responsible development of machine learning models, we introduce dalex, a Python package which implements a model-agnostic interface for interactive explainability and fairness. It adopts the design crafted through the development of various tools for explainable machine learning; thus, it aims at the uniﬁcation of existing solutions. This library’s source code and documentation are available under open license at https://python.drwhy.ai.","","2023-05-24 13:22:26","2023-05-24 23:22:42","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\ADPAZK99\Baniecki et al. - dalex Responsible Machine Learning with Interacti.pdf; ","notion://www.notion.so/Baniecki-et-al-n-d-a2ca46901e344278a83f3c5f0a0fe457","notion","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T5WCRGG3","book","","Burzykowski, Przemyslaw Biecek and Tomasz","12 Local-diagnostics Plots | Explanatory Model Analysis","","","","","https://ema.drwhy.ai/localDiagnostics.html","This book introduces unified language for exploration, explanation and examination of predictive machine learning models.","","2023-05-24 14:21:15","2023-05-24 14:23:30","2023-05-24 14:21:15","","","","","","","","","","","","","","","","","","","ema.drwhy.ai","","","","; C:\Users\ambreen.hanif\Zotero\storage\BFPR367M\localDiagnostics.html","notion://www.notion.so/Burzykowski-n-d-74db64f9e31d46029cd0857a869e53f2","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9EJEIT9J","webpage","","","2023_AmbreenHAN_Seminar_1","","","","","https://www.overleaf.com/project/643f39c9ab6e2ed0cd7c708b","An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.","","2023-05-24 14:20:20","2023-05-24 14:23:25","2023-05-24 14:20:20","","","","","","","","","","","","","","en","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\QPR34HB7\643f39c9ab6e2ed0cd7c708b.html","notion://www.notion.so/2023_AmbreenHAN_Seminar_1-n-d-5f5b07d17c764330bcd9a1c0bb27f3dd","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2AMQ7YDI","preprint","2019","Rabanser, Stephan; Günnemann, Stephan; Lipton, Zachary C.","Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift","","","","10.48550/arXiv.1810.11953","http://arxiv.org/abs/1810.11953","We might hope that when faced with unexpected inputs, well-designed software systems would fire off warnings. Machine learning (ML) systems, however, which depend strongly on properties of their inputs (e.g. the i.i.d. assumption), tend to fail silently. This paper explores the problem of building ML systems that fail loudly, investigating methods for detecting dataset shift, identifying exemplars that most typify the shift, and quantifying shift malignancy. We focus on several datasets and various perturbations to both covariates and label distributions with varying magnitudes and fractions of data affected. Interestingly, we show that across the dataset shifts that we explore, a two-sample-testing-based approach, using pre-trained classifiers for dimensionality reduction, performs best. Moreover, we demonstrate that domain-discriminating approaches tend to be helpful for characterizing shifts qualitatively and determining if they are harmful.","2019-10-28","2023-05-24 14:20:15","2023-05-24 14:23:22","2023-05-24 14:20:15","","","","","","","Failing Loudly","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1810.11953 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\6LHEYIKR\Rabanser et al. - 2019 - Failing Loudly An Empirical Study of Methods for .pdf; C:\Users\ambreen.hanif\Zotero\storage\ET8WMDRZ\1810.html; ","notion://www.notion.so/Rabanser-et-al-2019-19dbde619eee4cf89dbb7a08718b1525","notion","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1810.11953","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7Y5NU6X5","preprint","2018","Lipton, Zachary C.; Wang, Yu-Xiang; Smola, Alex","Detecting and Correcting for Label Shift with Black Box Predictors","","","","10.48550/arXiv.1802.03916","http://arxiv.org/abs/1802.03916","Faced with distribution shift between training and test set, we wish to detect and quantify the shift, and to correct our classifiers without test set labels. Motivated by medical diagnosis, where diseases (targets) cause symptoms (observations), we focus on label shift, where the label marginal $p(y)$ changes but the conditional $p(x| y)$ does not. We propose Black Box Shift Estimation (BBSE) to estimate the test distribution $p(y)$. BBSE exploits arbitrary black box predictors to reduce dimensionality prior to shift correction. While better predictors give tighter estimates, BBSE works even when predictors are biased, inaccurate, or uncalibrated, so long as their confusion matrices are invertible. We prove BBSE's consistency, bound its error, and introduce a statistical test that uses BBSE to detect shift. We also leverage BBSE to correct classifiers. Experiments demonstrate accurate estimates and improved prediction, even on high-dimensional datasets of natural images.","2018-07-26","2023-05-24 14:16:54","2023-05-24 14:23:18","2023-05-24 14:16:53","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1802.03916 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\9RVPC6EA\Lipton et al. - 2018 - Detecting and Correcting for Label Shift with Blac.pdf; C:\Users\ambreen.hanif\Zotero\storage\AFQ3SJIK\1802.html; ","notion://www.notion.so/Lipton-et-al-2018-11c81bdc4da14cae84b110fae93d55e6","notion","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1802.03916","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A7UEM9T2","journalArticle","2023","Bodria, Francesco; Giannotti, Fosca; Guidotti, Riccardo; Naretto, Francesca; Pedreschi, Dino; Rinzivillo, Salvatore","Benchmarking and survey of explanation methods for black box models","Data Mining and Knowledge Discovery","","","10.1007/s10618-023-00933-9","","The rise of sophisticated black-box machine learning models in Artificial Intelligence systems has prompted the need for explanation methods that reveal how these models work in an understandable way to users and decision makers. Unsurprisingly, the state-of-the-art exhibits currently a plethora of explainers providing many different types of explanations. With the aim of providing a compass for researchers and practitioners, this paper proposes a categorization of explanation methods from the perspective of the type of explanation they return, also considering the different input data formats. The paper accounts for the most representative explainers to date, also discussing similarities and discrepancies of returned explanations through their visual appearance. A companion website to the paper is provided as a continuous update to new explainers as they appear. Moreover, a subset of the most robust and widely adopted explainers, are benchmarked with respect to a repertoire of quantitative metrics.","2023-06-03","2023-07-18 02:00:53","2023-07-18 02:00:56","","1-60","","","","","Data Mining and Knowledge Discovery","","","","","","","","","","","","","ResearchGate","","0 citations (Crossref) [2023-07-18]","","C:\Users\ambreen.hanif\Zotero\storage\RJP9K9QV\Bodria et al_2023_Benchmarking and survey of explanation methods for black box models.pdf; ","https://www.researchgate.net/publication/371279014_Benchmarking_and_survey_of_explanation_methods_for_black_box_models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VHXC8V66","webpage","","","(PDF) Benchmarking and survey of explanation methods for black box models","","","","","https://www.researchgate.net/publication/371279014_Benchmarking_and_survey_of_explanation_methods_for_black_box_models","","","2023-07-18 01:49:55","2023-07-18 01:49:55","2023-07-18 01:49:55","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\BGXADDQ3\371279014_Benchmarking_and_survey_of_explanation_methods_for_black_box_models.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XCWXHNZA","journalArticle","2022","Guidotti, Riccardo; Monreale, Anna; Ruggieri, Salvatore; Naretto, Francesca; Turini, Franco; Pedreschi, Dino; Giannotti, Fosca","Stable and actionable explanations of black-box models through factual and counterfactual rules","Data Mining and Knowledge Discovery","","","10.1007/s10618-022-00878-5","","Recent years have witnessed the rise of accurate but obscure classification models that hide the logic of their internal decision processes. Explaining the decision taken by a black-box classifier on a specific input instance is therefore of striking interest. We propose a local rule-based model-agnostic explanation method providing stable and actionable explanations. An explanation consists of a factual logic rule, stating the reasons for the black-box decision, and a set of actionable counterfactual logic rules, proactively suggesting the changes in the instance that lead to a different outcome. Explanations are computed from a decision tree that mimics the behavior of the black-box locally to the instance to explain. The decision tree is obtained through a bagging-like approach that favors stability and fidelity: first, an ensemble of decision trees is learned from neighborhoods of the instance under investigation; then, the ensemble is merged into a single decision tree. Neighbor instances are synthetically generated through a genetic algorithm whose fitness function is driven by the black-box behavior. Experiments show that the proposed method advances the state-of-the-art towards a comprehensive approach that successfully covers stability and actionability of factual and counterfactual explanations.","2022-11-14","2023-07-18 01:49:41","2023-07-18 01:49:45","","1-38","","","","","Data Mining and Knowledge Discovery","","","","","","","","","","","","","ResearchGate","","0 citations (Crossref) [2023-07-18]","","C:\Users\ambreen.hanif\Zotero\storage\5MNLD9TJ\Guidotti et al. - 2022 - Stable and actionable explanations of black-box mo.pdf; ","https://www.researchgate.net/publication/365371519_Stable_and_actionable_explanations_of_black-box_models_through_factual_and_counterfactual_rules","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NNZDFYHJ","book","2023","Beckh, Katharina; Müller, Sebastian; Jakobs, Matthias; Toborek, Vanessa; Tan, Hanxiao; Fischer, Raphael; Welke, Pascal; Houben, Sebastian; von Rueden, Laura","Harnessing Prior Knowledge for Explainable Machine Learning: An Overview","","","","","","","2023-02-01","2023-07-18 01:49:26","2023-07-18 01:49:34","","","450","","","","","Harnessing Prior Knowledge for Explainable Machine Learning","","","","","","","","","","","","ResearchGate","","Pages: 463 DOI: 10.1109/SaTML54575.2023.00038","","C:\Users\ambreen.hanif\Zotero\storage\CA2JWIPJ\Beckh et al. - 2023 - Harnessing Prior Knowledge for Explainable Machine.pdf; ","https://www.researchgate.net/publication/371243326_Harnessing_Prior_Knowledge_for_Explainable_Machine_Learning_An_Overview","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BAKIC2XA","document","2013","Lebo, Tim; Sahoo, Satya; McGuinness, Deborah; Belhajjame, Khalid; Cheney, James; Corsar, David; Garijo, Daniel; Soiland-Reyes, Stian; Zednik, Stephan; Zhao, Jun","PROV-O: The PROV ontology: W3C recommendation 30 april 2013","","","","","https://www.w3.org/TR/prov-o/","","2013-04-30","2023-07-14 00:28:23","2023-07-14 00:28:23","","","","","","","","","","","","","","","English","","","","","","","Citation Key: w3c_provenance-o-13 Type: Other","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TLGWF4PJ","document","2013","Moreau, Luc; Missier, Paolo; Belhajjame, Khalid; BFar, Reza; Cheney, James; Coppens, Sam; Cresswell, Stephen; Gil, Yolanda; Groth, Paul; Klyne, Graham; Lebo, Timothy; McCusker, Jim; Miles, Simon; Myers, James; Sahoo, Satya; Tilmes, Curt","PROV-DM: The PROV data model","","","","","http://www.w3.org/TR/prov-dm/","","2013","2023-07-14 00:28:15","2023-07-14 00:28:15","","","","","","","","","","","","","","","","","","","","","","Citation Key: w3c-provenance-dm-13","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VK7SIEQX","preprint","2019","Guidotti, Riccardo; Ruggieri, Salvatore","On The Stability of Interpretable Models","","","","10.48550/arXiv.1810.09352","http://arxiv.org/abs/1810.09352","Interpretable classification models are built with the purpose of providing a comprehensible description of the decision logic to an external oversight agent. When considered in isolation, a decision tree, a set of classification rules, or a linear model, are widely recognized as human-interpretable. However, such models are generated as part of a larger analytical process. Bias in data collection and preparation, or in model's construction may severely affect the accountability of the design process. We conduct an experimental study of the stability of interpretable models with respect to feature selection, instance selection, and model selection. Our conclusions should raise awareness and attention of the scientific community on the need of a stability impact assessment of interpretable models.","2019-03-15","2023-07-13 23:38:17","2023-07-13 23:38:17","2023-07-13 23:38:17","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1810.09352 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\DZG45Q7P\1810.html; C:\Users\ambreen.hanif\Zotero\storage\HFTZMDHX\Guidotti_Ruggieri_2019_On The Stability of Interpretable Models.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1810.09352","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PSNGBT28","preprint","2022","Flora, Montgomery; Potvin, Corey; McGovern, Amy; Handler, Shawn","Comparing Explanation Methods for Traditional Machine Learning Models Part 2: Quantifying Model Explainability Faithfulness and Improvements with Dimensionality Reduction","","","","10.48550/arXiv.2211.10378","http://arxiv.org/abs/2211.10378","Machine learning (ML) models are becoming increasingly common in the atmospheric science community with a wide range of applications. To enable users to understand what an ML model has learned, ML explainability has become a field of active research. In Part I of this two-part study, we described several explainability methods and demonstrated that feature rankings from different methods can substantially disagree with each other. It is unclear, though, whether the disagreement is overinflated due to some methods being less faithful in assigning importance. Herein, ""faithfulness"" or ""fidelity"" refer to the correspondence between the assigned feature importance and the contribution of the feature to model performance. In the present study, we evaluate the faithfulness of feature ranking methods using multiple methods. Given the sensitivity of explanation methods to feature correlations, we also quantify how much explainability faithfulness improves after correlated features are limited. Before dimensionality reduction, the feature relevance methods [e.g., SHAP, LIME, ALE variance, and logistic regression (LR) coefficients] were generally more faithful than the permutation importance methods due to the negative impact of correlated features. Once correlated features were reduced, traditional permutation importance became the most faithful method. In addition, the ranking uncertainty (i.e., the spread in rank assigned to a feature by the different ranking methods) was reduced by a factor of 2-10, and excluding less faithful feature ranking methods reduces it further. This study is one of the first to quantify the improvement in explainability from limiting correlated features and knowing the relative fidelity of different explainability methods.","2022-11-18","2023-07-13 23:35:41","2023-07-13 23:35:41","2023-07-13 23:35:41","","","","","","","Comparing Explanation Methods for Traditional Machine Learning Models Part 2","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2211.10378 [physics, stat]","","C:\Users\ambreen.hanif\Zotero\storage\CJNEWWYF\2211.html; C:\Users\ambreen.hanif\Zotero\storage\3PXUUQZ8\Flora et al_2022_Comparing Explanation Methods for Traditional Machine Learning Models Part 2.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2211.10378","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TVKP5EGG","preprint","2022","Flora, Montgomery; Potvin, Corey; McGovern, Amy; Handler, Shawn","Comparing Explanation Methods for Traditional Machine Learning Models Part 1: An Overview of Current Methods and Quantifying Their Disagreement","","","","10.48550/arXiv.2211.08943","http://arxiv.org/abs/2211.08943","With increasing interest in explaining machine learning (ML) models, the first part of this two-part study synthesizes recent research on methods for explaining global and local aspects of ML models. This study distinguishes explainability from interpretability, local from global explainability, and feature importance versus feature relevance. We demonstrate and visualize different explanation methods, how to interpret them, and provide a complete Python package (scikit-explain) to allow future researchers to explore these products. We also highlight the frequent disagreement between explanation methods for feature rankings and feature effects and provide practical advice for dealing with these disagreements. We used ML models developed for severe weather prediction and sub-freezing road surface temperature prediction to generalize the behavior of the different explanation methods. For feature rankings, there is substantially more agreement on the set of top features (e.g., on average, two methods agree on 6 of the top 10 features) than on specific rankings (on average, two methods only agree on the ranks of 2-3 features in the set of top 10 features). On the other hand, two feature effect curves from different methods are in high agreement as long as the phase space is well sampled. Finally, a lesser-known method, tree interpreter, was found comparable to SHAP for feature effects, and with the widespread use of random forests in geosciences and computational ease of tree interpreter, we recommend it be explored in future research.","2022-11-16","2023-07-13 23:35:12","2023-07-13 23:35:12","2023-07-13 23:35:12","","","","","","","Comparing Explanation Methods for Traditional Machine Learning Models Part 1","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2211.08943 [physics, stat]","","C:\Users\ambreen.hanif\Zotero\storage\4CBUTLRM\2211.html; C:\Users\ambreen.hanif\Zotero\storage\Z7I7RJ5I\Flora et al_2022_Comparing Explanation Methods for Traditional Machine Learning Models Part 1.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2211.08943","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5YQ6IXJ3","preprint","2021","Watson, Matthew; Hasan, Bashar Awwad Shiekh; Moubayed, Noura Al","Agree to Disagree: When Deep Learning Models With Identical Architectures Produce Distinct Explanations","","","","10.48550/arXiv.2105.06791","http://arxiv.org/abs/2105.06791","Deep Learning of neural networks has progressively become more prominent in healthcare with models reaching, or even surpassing, expert accuracy levels. However, these success stories are tainted by concerning reports on the lack of model transparency and bias against some medical conditions or patients' sub-groups. Explainable methods are considered the gateway to alleviate many of these concerns. In this study we demonstrate that the generated explanations are volatile to changes in model training that are perpendicular to the classification task and model structure. This raises further questions about trust in deep learning models for healthcare. Mainly, whether the models capture underlying causal links in the data or just rely on spurious correlations that are made visible via explanation methods. We demonstrate that the output of explainability methods on deep neural networks can vary significantly by changes of hyper-parameters, such as the random seed or how the training set is shuffled. We introduce a measure of explanation consistency which we use to highlight the identified problems on the MIMIC-CXR dataset. We find explanations of identical models but with different training setups have a low consistency: $\approx$ 33% on average. On the contrary, kernel methods are robust against any orthogonal changes, with explanation consistency at 94%. We conclude that current trends in model explanation are not sufficient to mitigate the risks of deploying models in real life healthcare applications.","2021-10-30","2023-07-13 23:34:08","2023-07-13 23:34:08","2023-07-13 23:34:07","","","","","","","Agree to Disagree","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2105.06791 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\NFZ5BLXC\2105.html; C:\Users\ambreen.hanif\Zotero\storage\F9282LFH\Watson et al_2021_Agree to Disagree.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2105.06791","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T23H5LI5","conferencePaper","2023","Leventi-Peetz, Anastasia-M.; Weber, Kai","Rashomon Effect and Consistency in Explainable Artificial Intelligence (XAI)","Proceedings of the Future Technologies Conference (FTC) 2022, Volume 1","978-3-031-18461-1","","10.1007/978-3-031-18461-1_52","","The consistency of the explainability of artificial intelligence (XAI), especially with regard to the Rashomon effect, is in the focus of the here presented work. Rashomon effect has been named the phenomenon of receiving different machine learning (ML) explanations when employing different models to describe the same data. On the basis of concrete examples, cases of Rashomon effect will be visually demonstrated and discussed to underline the difficulty to practically produce definite and unambiguous machine learning explanations and predictions. Artificial intelligence (AI) presently undergoes a so-called replication and reproducibility crisis which hinders models and techniques from being properly assessed for robustness, fairness, and safety. Studying the Rashomon effect is important for understanding the causes of the unintended variability of results which originate from-* within the models and the XAI methods themselves.","2023","2023-07-13 23:32:53","2023-07-13 23:32:56","","796-808","","","","","","","Lecture Notes in Networks and Systems","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","0 citations (Crossref) [2023-07-14]","","C:\Users\ambreen.hanif\Zotero\storage\DXSC9CG6\Leventi-Peetz and Weber - 2023 - Rashomon Effect and Consistency in Explainable Art.pdf","","","","Arai, Kohei","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XXMTJ9RN","preprint","2023","Cortacero, Kévin; McKenzie, Brienne; Müller, Sabina; Khazen, Roxana; Lafouresse, Fanny; Corsaut, Gaëlle; Van Acker, Nathalie; Frenois, François-Xavier; Lamant, Laurence; Meyer, Nicolas; Vergier, Béatrice; Wilson, Dennis G.; Luga, Hervé; Staufer, Oskar; Dustin, Michael L.; Valitutti, Salvatore; Cussat-Blanc, Sylvain","Kartezio: Evolutionary Design of Explainable Pipelines for Biomedical Image Analysis","","","","10.48550/arXiv.2302.14762","http://arxiv.org/abs/2302.14762","An unresolved issue in contemporary biomedicine is the overwhelming number and diversity of complex images that require annotation, analysis and interpretation. Recent advances in Deep Learning have revolutionized the field of computer vision, creating algorithms that compete with human experts in image segmentation tasks. Crucially however, these frameworks require large human-annotated datasets for training and the resulting models are difficult to interpret. In this study, we introduce Kartezio, a modular Cartesian Genetic Programming based computational strategy that generates transparent and easily interpretable image processing pipelines by iteratively assembling and parameterizing computer vision functions. The pipelines thus generated exhibit comparable precision to state-of-the-art Deep Learning approaches on instance segmentation tasks, while requiring drastically smaller training datasets, a feature which confers tremendous flexibility, speed, and functionality to this approach. We also deployed Kartezio to solve semantic and instance segmentation problems in four real-world Use Cases, and showcase its utility in imaging contexts ranging from high-resolution microscopy to clinical pathology. By successfully implementing Kartezio on a portfolio of images ranging from subcellular structures to tumoral tissue, we demonstrated the flexibility, robustness and practical utility of this fully explicable evolutionary designer for semantic and instance segmentation.","2023-02-28","2023-07-13 23:27:41","2023-07-13 23:27:41","2023-07-13 23:27:41","","","","","","","Kartezio","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2302.14762 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\4HXWT4TT\2302.html; C:\Users\ambreen.hanif\Zotero\storage\XKD2MJ5H\Cortacero et al_2023_Kartezio.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2302.14762","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DTM8TRQN","preprint","2023","Müller, Sebastian; Toborek, Vanessa; Beckh, Katharina; Jakobs, Matthias; Bauckhage, Christian; Welke, Pascal","An Empirical Evaluation of the Rashomon Effect in Explainable Machine Learning","","","","10.48550/arXiv.2306.15786","http://arxiv.org/abs/2306.15786","The Rashomon Effect describes the following phenomenon: for a given dataset there may exist many models with equally good performance but with different solution strategies. The Rashomon Effect has implications for Explainable Machine Learning, especially for the comparability of explanations. We provide a unified view on three different comparison scenarios and conduct a quantitative evaluation across different datasets, models, attribution methods, and metrics. We find that hyperparameter-tuning plays a role and that metric selection matters. Our results provide empirical support for previously anecdotal evidence and exhibit challenges for both scientists and practitioners.","2023-06-29","2023-07-13 23:23:56","2023-07-13 23:24:19","2023-07-13 23:23:56","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2306.15786 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\GH4Q7W97\2306.html; C:\Users\ambreen.hanif\Zotero\storage\FBI2JTHC\Müller et al_2023_An Empirical Evaluation of the Rashomon Effect in Explainable Machine Learning.pdf; ","notion://www.notion.so/cite-muller_empirical_2023-fcc92232ecc94452978db4ce6d49de86","","","","","","","","","","","","","","","","","","","","","arXiv:2306.15786","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V5Q2SPLF","preprint","2017","Zhang, Quanshi; Cao, Ruiming; Zhang, Shengming; Redmonds, Mark; Wu, Ying Nian; Zhu, Song-Chun","Interactively Transferring CNN Patterns for Part Localization","","","","10.48550/arXiv.1708.01783","http://arxiv.org/abs/1708.01783","In the scenario of one/multi-shot learning, conventional end-to-end learning strategies without sufficient supervision are usually not powerful enough to learn correct patterns from noisy signals. Thus, given a CNN pre-trained for object classification, this paper proposes a method that first summarizes the knowledge hidden inside the CNN into a dictionary of latent activation patterns, and then builds a new model for part localization by manually assembling latent patterns related to the target part via human interactions. We use very few (e.g., three) annotations of a semantic object part to retrieve certain latent patterns from conv-layers to represent the target part. We then visualize these latent patterns and ask users to further remove incorrect patterns, in order to refine part representation. With the guidance of human interactions, our method exhibited superior performance of part localization in experiments.","2017-11-21","2023-07-13 01:30:08","2023-07-13 01:30:48","2023-07-13 01:30:08","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1708.01783 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\PHKA3WED\1708.html; ; C:\Users\ambreen.hanif\Zotero\storage\TY3SHGQJ\Zhang et al_2017_Interactively Transferring CNN Patterns for Part Localization.pdf","notion://www.notion.so/cite-zhang_interactively_2017-1-48c411ad506a4c8bb1018c7dd5934efa","notion","","","","","","","","","","","","","","","","","","","","arXiv:1708.01783","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CZ7IULEH","webpage","","","7.1. Visualizing Categorical Distributions — Computational and Inferential Thinking","","","","","https://inferentialthinking.com/chapters/07/1/Visualizing_Categorical_Distributions.html","","","2023-07-12 03:31:41","2023-07-12 03:31:47","2023-07-12 03:31:41","","","","","","","","","","","","","","","","","","","","","","","","notion://www.notion.so/cite-noauthor_71_nodate-3d2be7695eff42c1ad4fc50b2122010a","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LUT3Z3JI","book","2019","Y, Geeta; Nair, Nimisha; Satpathy, Pallavi; Christopher, Jabez","Covariate Shift: A Review and Analysis on Classifiers","","","","","","","2019-10-01","2023-07-05 03:08:25","2023-07-05 03:08:25","","","1","","","","","Covariate Shift","","","","","","","","","","","","ResearchGate","","Pages: 6 DOI: 10.1109/GCAT47503.2019.8978471","","C:\Users\ambreen.hanif\Zotero\storage\GBN8KBPJ\Y et al. - 2019 - Covariate Shift A Review and Analysis on Classifi.pdf; ","https://www.researchgate.net/publication/339021786_Covariate_Shift_A_Review_and_Analysis_on_Classifiers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F7QWMAU9","conferencePaper","2019","Gonen, Hila; Goldberg, Yoav","Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them","Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)","","","10.18653/v1/N19-1061","https://aclanthology.org/N19-1061","Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","2019-06","2023-07-03 00:31:49","2023-07-03 00:31:52","2023-07-03 00:31:49","609–614","","","","","","Lipstick on a Pig","","","","","Association for Computational Linguistics","Minneapolis, Minnesota","","","","","","ACLWeb","","30 citations (Crossref) [2023-07-03]","","C:\Users\ambreen.hanif\Zotero\storage\DV8UE4YX\Gonen_Goldberg_2019_Lipstick on a Pig.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","NAACL-HLT 2019","","","","","","","","","","","","","","",""
"9TDENZJV","conferencePaper","2016","Bolukbasi, Tolga; Chang, Kai-Wei; Zou, James; Saligrama, Venkatesh; Kalai, Adam","Man is to computer programmer as woman is to homemaker? debiasing word embeddings","Proceedings of the 30th International Conference on Neural Information Processing Systems","978-1-5108-3881-9","","","","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.","2016-12-05","2023-07-03 00:29:48","2023-07-03 00:29:48","2023-07-02","4356–4364","","","","","","Man is to computer programmer as woman is to homemaker?","NIPS'16","","","","Curran Associates Inc.","Red Hook, NY, USA","","","","","","ACM Digital Library","","","","C:\Users\ambreen.hanif\Zotero\storage\J49Y25VM\Bolukbasi et al_2016_Man is to computer programmer as woman is to homemaker.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QSKC997S","conferencePaper","2022","Aleisa, Monirah Ali; Beloff, Natalia; White, Martin","AIRM: A New AI Recruiting Model for the Saudi Arabia Labor Market","Intelligent Systems and Applications","978-3-030-82199-9","","10.1007/978-3-030-82199-9_8","","One of Saudi Arabia’s vision 2030 goals is to keep the unemployment rate at the lowest level to empower the economy. Research has shown that a rise in unemployment has a negative effect on any countries gross domestic product. Artificial Intelligence is the fastest developing technology these days. It has served in many specialties. Recently, Artificial Intelligence technology has shined in the field of recruiting. Researchers are working to invest its capabilities with many applications that help speed up the recruiting process. However, having an open labor market without a coherent data center makes it hard to monitor, integrate, analyze, and build an evaluation matrix that helps reach the best match of job candidate to job vacancy. A recruiter’s job is to assess a candidate’s data to build metrics that can make them choose a suitable candidate. Job seekers build themselves metrics to compare job offers to choose the best opportunity for their preferred choice. This paper address how Artificial Intelligence techniques can be effectively exploited to improve the current Saudi labor market. It aims to decrease the gap between recruiters and job seekers. This paper analyzes the current Saudi labor market, it then outlines an approach that proposes: 1) a new data storage technology approach, and 2) a new Artificial Intelligence architecture, with three layers to extract relevant information from data of both recruiters and job seekers by exploiting machine learning, in particular clustering algorithms, to group data points, natural language processing to convert text to numerical representations, and recurrent neural networks to produce matching keywords, and equations to generate a similarity score. We have completed the current Saudi labor market analysis, and a proposal for the Artificial Intelligence and data storage components is articulated in this paper. The proposed approach and technology will empower the Saudi government’s immediate and strategic decisions by having a comprehensive insight into the labor market.","2022","2023-07-03 00:25:17","2023-07-03 00:25:27","","105-124","","","","","","AIRM","Lecture Notes in Networks and Systems","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","0 citations (Crossref) [2023-07-03]","","C:\Users\ambreen.hanif\Zotero\storage\J4ZIUDH9\Aleisa et al_2022_AIRM.pdf","","","","Arai, Kohei","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RN9MAYUV","webpage","","","AIRM: A New AI Recruiting Model for the Saudi Arabia Labor Market | SpringerLink","","","","","https://link-springer-com.simsrad.net.ocs.mq.edu.au/chapter/10.1007/978-3-030-82199-9_8","","","2023-07-03 00:25:05","2023-07-03 00:25:15","2023-07-03 00:25:05","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\SV4K9LS4\978-3-030-82199-9_8.html; ","notion://www.notion.so/cite-noauthor_airm_nodate-50e996fb5fc24911bf09d1e8aaefdbd0","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7XEB9G8L","journalArticle","2023","Tritscher, Julian; Krause, Anna; Hotho, Andreas","Feature relevance XAI in anomaly detection: Reviewing approaches and challenges","Frontiers in Artificial Intelligence","","2624-8212","10.3389/frai.2023.1099521","https://www.frontiersin.org/articles/10.3389/frai.2023.1099521","With complexity of artificial intelligence systems increasing continuously in past years, studies to explain these complex systems have grown in popularity. While much work has focused on explaining artificial intelligence systems in popular domains such as classification and regression, explanations in the area of anomaly detection have only recently received increasing attention from researchers. In particular, explaining singular model decisions of a complex anomaly detector by highlighting which inputs were responsible for a decision, commonly referred to as local post-hoc feature relevance, has lately been studied by several authors. In this paper, we systematically structure these works based on their access to training data and the anomaly detection model, and provide a detailed overview of their operation in the anomaly detection domain. We demonstrate their performance and highlight their limitations in multiple experimental showcases, discussing current challenges and opportunities for future work in feature relevance XAI for anomaly detection.","2023","2023-06-30 22:49:32","2023-06-30 22:50:14","2023-06-30 22:49:32","","","","6","","","Feature relevance XAI in anomaly detection","","","","","","","","","","","","Frontiers","","","","; C:\Users\ambreen.hanif\Zotero\storage\42TN6YWX\Tritscher et al_2023_Feature relevance XAI in anomaly detection.pdf","notion://www.notion.so/cite-tritscher_feature_2023-1-a84d78ba168e4983ab8e89540218b559","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"94JG7L49","conferencePaper","2021","Nagaraj Rao, Varun; Zhen, Xingjian; Hovsepian, Karen; Shen, Mingwei","A First Look: Towards Explainable TextVQA Models via Visual and Textual Explanations","Proceedings of the Third Workshop on Multimodal Artificial Intelligence","","","10.18653/v1/2021.maiworkshop-1.4","https://aclanthology.org/2021.maiworkshop-1.4","Explainable deep learning models are advantageous in many situations. Prior work mostly provide unimodal explanations through post-hoc approaches not part of the original system design. Explanation mechanisms also ignore useful textual information present in images. In this paper, we propose MTXNet, an end-to-end trainable multimodal architecture to generate multimodal explanations, which focuses on the text in the image. We curate a novel dataset TextVQA-X, containing ground truth visual and multi-reference textual explanations that can be leveraged during both training and evaluation. We then quantitatively show that training with multimodal explanations complements model performance and surpasses unimodal baselines by up to 7% in CIDEr scores and 2% in IoU. More importantly, we demonstrate that the multimodal explanations are consistent with human interpretations, help justify the models' decision, and provide useful insights to help diagnose an incorrect prediction. Finally, we describe a real-world e-commerce application for using the generated multimodal explanations.","2021-06","2023-06-30 08:06:06","2023-06-30 08:06:14","2023-06-30 08:06:06","19–29","","","","","","A First Look","","","","","Association for Computational Linguistics","Mexico City, Mexico","","","","","","ACLWeb","","1 citations (Crossref) [2023-06-30]","","C:\Users\ambreen.hanif\Zotero\storage\R2X5QUUW\Nagaraj Rao et al_2021_A First Look.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","maiworkshop 2021","","","","","","","","","","","","","","",""
"4BV9DDKS","webpage","","","A First Look: Towards Explainable TextVQA Models via Visual and Textual Explanations - ACL Anthology","","","","","https://aclanthology.org/2021.maiworkshop-1.4/","","","2023-06-30 08:05:48","2023-06-30 08:05:48","2023-06-30 08:05:48","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\M9Q3H34J\2021.maiworkshop-1.4.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AGR4RW87","preprint","2017","Miller, Tim; Howe, Piers; Sonenberg, Liz","Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences","","","","10.48550/arXiv.1712.00547","http://arxiv.org/abs/1712.00547","In his seminal book `The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity' [2004, Sams Indianapolis, IN, USA], Alan Cooper argues that a major reason why software is often poorly designed (from a user perspective) is that programmers are in charge of design decisions, rather than interaction designers. As a result, programmers design software for themselves, rather than for their target audience, a phenomenon he refers to as the `inmates running the asylum'. This paper argues that explainable AI risks a similar fate. While the re-emergence of explainable AI is positive, this paper argues most of us as AI researchers are building explanatory agents for ourselves, rather than for the intended users. But explainable AI is more likely to succeed if researchers and practitioners understand, adopt, implement, and improve models from the vast and valuable bodies of research in philosophy, psychology, and cognitive science, and if evaluation of these models is focused more on people than on technology. From a light scan of literature, we demonstrate that there is considerable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.","2017-12-04","2023-06-30 07:18:51","2023-06-30 07:25:40","2023-06-30 07:18:49","","","","","","","Explainable AI","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1712.00547 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\L9K5EIUM\1712.html; C:\Users\ambreen.hanif\Zotero\storage\6BDEP2W2\Miller et al_2017_Explainable AI.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1712.00547","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WGQEGZBT","journalArticle","2019","Rai, Arun","Explainable AI: from black box to glass box","Journal of the Academy of Marketing Science","","","10.1007/s11747-019-00710-5","https://doi.org/10.1007/s11747-019-00710-5","","2019","2021-09-30 01:44:53","2023-06-30 07:10:47","2021-09-30","","","","","","","","","","","","","","","","","","","","","249 citations (Crossref) [2022-12-20] QID: Q102633571","","C:\Users\ambreen.hanif\Zotero\storage\MI8Z4ECL\full-text.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UYRNZCVK","journalArticle","2017","Sagir, Abdu; Sathasivam, Saratha","A novel adaptive neuro fuzzy inference system based classification model for heart disease prediction","Pertanika Journal of Science \& Technology","","","","","Adaptive Neuro Fuzzy Inference System (ANFIS) is among the most efficient classification and prediction modelling techniques used to develop accurate relationship between input and output parameters in different processes. This paper reports the design and evaluation of the classification performances of two discrete Adaptive Neuro Fuzzy Inference System models, ANFIS Matlab’s built-in model (ANFIS_ LSGD) and a newly ANFIS model with Levenberg-Marquardt algorithm (ANFIS_LSLM). Major steps were performed, which included classification using grid partitioning method, the ANFIS trained with least square estimates and backpropagation gradient descent method, as well as the ANFIS trained with Levenberg-Marquardt algorithm using finite difference technique for computation of a Jacobian matrix. The proposed ANFIS_LSLM model predicts the degree of patient’s heart disease with better, reliable and more accurate results. This is due to its new feature of index membership function that determines the unique membership functions in an ANFIS structure, which indexes them into a row-wise vector. In addition, an attempt was also done to specify the effectiveness of the model’s performance measuring accuracy, sensitivity and specificity. A comparison of the two models in terms of training and testing with the Statlog-Cleveland Heart Disease dataset have also been done.","2017-01-01","2023-04-12 00:51:00","2023-06-30 07:08:29","","43-56","","","25","","Pertanika Journal of Science \& Technology","","","","","","","","","","","","","ResearchGate","","","","; ; C:\Users\ambreen.hanif\Zotero\storage\IB6CBYZJ\Sagir_Sathasivam_2017_A novel adaptive neuro fuzzy inference system based classification model for.pdf","notion://www.notion.so/cite-sagir_novel_2017-f5dbc451a4e14f6f90c699b2e19db9de; https://www.researchgate.net/publication/313745773_A_novel_adaptive_neuro_fuzzy_inference_system_based_classification_model_for_heart_disease_prediction","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"69Y4Z4QI","journalArticle","2016","Zhou, Bolei; Khosla, Aditya; Lapedriza, Agata; Oliva, Aude; Torralba, Antonio","Learning Deep Features for Discriminative Localization","{Proceedings of the IEEE conference on computer vision and pattern recognition}","","","10.48550/arXiv.1512.04150","https://arxiv.org/abs/1512.04150v1","In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them","2016","2022-11-14 01:32:14","2023-06-30 07:07:06","2022-11-14 01:32:14","2921-2929","","","","","","","","","","","","","en","","","","","arxiv.org","","5947 citations (Semantic Scholar/arXiv) [2022-12-20]","","; C:\Users\ambreen.hanif\Zotero\storage\PTFNN7GH\1512.html; C:\Users\ambreen.hanif\Zotero\storage\9HISR5WG\Zhou et al_2015_Learning Deep Features for Discriminative Localization.pdf","notion://www.notion.so/cite-zhou_learning_2016-41119719786f4b8e933fba59d8ffd656","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KK5SB9LI","preprint","2021","Molnar, Christoph; König, Gunnar; Bischl, Bernd; Casalicchio, Giuseppe","Model-agnostic Feature Importance and Effects with Dependent Features -- A Conditional Subgroup Approach","","","","10.48550/arXiv.2006.04628","http://arxiv.org/abs/2006.04628","The interpretation of feature importance in machine learning models is challenging when features are dependent. Permutation feature importance (PFI) ignores such dependencies, which can cause misleading interpretations due to extrapolation. A possible remedy is more advanced conditional PFI approaches that enable the assessment of feature importance conditional on all other features. Due to this shift in perspective and in order to enable correct interpretations, it is therefore important that the conditioning is transparent and humanly comprehensible. In this paper, we propose a new sampling mechanism for the conditional distribution based on permutations in conditional subgroups. As these subgroups are constructed using decision trees (transformation trees), the conditioning becomes inherently interpretable. This not only provides a simple and effective estimator of conditional PFI, but also local PFI estimates within the subgroups. In addition, we apply the conditional subgroups approach to partial dependence plots (PDP), a popular method for describing feature effects that can also suffer from extrapolation when features are dependent and interactions are present in the model. We show that PFI and PDP based on conditional subgroups often outperform methods such as conditional PFI based on knockoffs, or accumulated local effect plots. Furthermore, our approach allows for a more fine-grained interpretation of feature effects and importance within the conditional subgroups.","2021-06-21","2023-06-30 06:40:39","2023-06-30 06:40:39","2023-06-30 06:40:39","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2006.04628 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\JRLT39WB\2006.html; C:\Users\ambreen.hanif\Zotero\storage\KJM87VQI\Molnar et al_2021_Model-agnostic Feature Importance and Effects with Dependent Features -- A.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2006.04628","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IREBLPUK","preprint","2021","Carletti, Mattia; Terzi, Matteo; Susto, Gian Antonio","Interpretable Anomaly Detection with DIFFI: Depth-based Isolation Forest Feature Importance","","","","10.48550/arXiv.2007.11117","http://arxiv.org/abs/2007.11117","Anomaly Detection is an unsupervised learning task aimed at detecting anomalous behaviours with respect to historical data. In particular, multivariate Anomaly Detection has an important role in many applications thanks to the capability of summarizing the status of a complex system or observed phenomenon with a single indicator (typically called `Anomaly Score') and thanks to the unsupervised nature of the task that does not require human tagging. The Isolation Forest is one of the most commonly adopted algorithms in the field of Anomaly Detection, due to its proven effectiveness and low computational complexity. A major problem affecting Isolation Forest is represented by the lack of interpretability, an effect of the inherent randomness governing the splits performed by the Isolation Trees, the building blocks of the Isolation Forest. In this paper we propose effective, yet computationally inexpensive, methods to define feature importance scores at both global and local level for the Isolation Forest. Moreover, we define a procedure to perform unsupervised feature selection for Anomaly Detection problems based on our interpretability method; such procedure also serves the purpose of tackling the challenging task of feature importance evaluation in unsupervised anomaly detection. We assess the performance on several synthetic and real-world datasets, including comparisons against state-of-the-art interpretability techniques, and make the code publicly available to enhance reproducibility and foster research in the field.","2021-07-13","2023-06-30 06:11:52","2023-06-30 06:11:52","2023-06-30 06:11:52","","","","","","","Interpretable Anomaly Detection with DIFFI","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2007.11117 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\LAHGK75K\2007.html; C:\Users\ambreen.hanif\Zotero\storage\LX8UMY6J\Carletti et al_2021_Interpretable Anomaly Detection with DIFFI.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2007.11117","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QTCX7YJE","journalArticle","2023","Tritscher, Julian; Krause, Anna; Hotho, Andreas","Feature relevance XAI in anomaly detection: Reviewing approaches and challenges","Frontiers in Artificial Intelligence","","2624-8212","10.3389/frai.2023.1099521","https://www.frontiersin.org/articles/10.3389/frai.2023.1099521","With complexity of artificial intelligence systems increasing continuously in past years, studies to explain these complex systems have grown in popularity. While much work has focused on explaining artificial intelligence systems in popular domains such as classification and regression, explanations in the area of anomaly detection have only recently received increasing attention from researchers. In particular, explaining singular model decisions of a complex anomaly detector by highlighting which inputs were responsible for a decision, commonly referred to as local post-hoc feature relevance, has lately been studied by several authors. In this paper, we systematically structure these works based on their access to training data and the anomaly detection model, and provide a detailed overview of their operation in the anomaly detection domain. We demonstrate their performance and highlight their limitations in multiple experimental showcases, discussing current challenges and opportunities for future work in feature relevance XAI for anomaly detection.","2023","2023-06-30 06:06:16","2023-06-30 06:06:21","2023-06-30 06:06:16","","","","6","","","Feature relevance XAI in anomaly detection","","","","","","","","","","","","Frontiers","","","","C:\Users\ambreen.hanif\Zotero\storage\L3AX7WT5\Tritscher et al_2023_Feature relevance XAI in anomaly detection.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LWUGY7ZU","preprint","2018","Yuan, Xiaoyong; He, Pan; Zhu, Qile; Li, Xiaolin","Adversarial Examples: Attacks and Defenses for Deep Learning","","","","10.48550/arXiv.1712.07107","http://arxiv.org/abs/1712.07107","With rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks have been recently found vulnerable to well-designed input samples, called adversarial examples. Adversarial examples are imperceptible to human but can easily fool deep neural networks in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying deep neural networks in safety-critical environments. Therefore, attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples for deep neural networks, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples and explore the challenges and the potential solutions.","2018-07-06","2023-06-30 06:03:30","2023-06-30 06:03:30","2023-06-30 06:03:29","","","","","","","Adversarial Examples","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1712.07107 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\NUM53B7U\1712.html; C:\Users\ambreen.hanif\Zotero\storage\NWJAWN2V\Yuan et al_2018_Adversarial Examples.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1712.07107","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WXE3S8CP","webpage","","","[1712.07107] Adversarial Examples: Attacks and Defenses for Deep Learning","","","","","https://arxiv.org/abs/1712.07107","","","2023-06-30 06:03:09","2023-06-30 06:03:16","2023-06-30 06:03:09","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\EJS2J3RU\1712.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6EZGGWXW","journalArticle","2017","Das, Abhishek; Agrawal, Harsh; Zitnick, Larry; Parikh, Devi; Batra, Dhruv","Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?","Computer Vision and Image Understanding","","1077-3142","10.1016/j.cviu.2017.10.001","https://www.sciencedirect.com/science/article/pii/S1077314217301649","We conduct large-scale studies on ‘human attention’ in Visual Question Answering (VQA) to understand where humans choose to look to answer questions about images. We design and test multiple game-inspired novel attention-annotation interfaces that require the subject to sharpen regions of a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human ATtention) dataset. We evaluate attention maps generated by state-of-the-art VQA models against human attention both qualitatively (via visualizations) and quantitatively (via rank-order correlation). Our experiments show that current attention models in VQA do not seem to be looking at the same regions as humans. Finally, we train VQA models with explicit attention supervision, and find that it improves VQA performance.","2017-10-01","2023-06-30 05:54:59","2023-06-30 05:55:06","2023-06-30 05:54:56","90-100","","","163","","Computer Vision and Image Understanding","Human Attention in Visual Question Answering","Language in Vision","","","","","","en","","","","","ScienceDirect","","127 citations (Crossref) [2023-06-30]","","C:\Users\ambreen.hanif\Zotero\storage\YQ72P6IH\Das et al_2017_Human Attention in Visual Question Answering.pdf; C:\Users\ambreen.hanif\Zotero\storage\9UGYWSZ7\S1077314217301649.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6PGAWPY5","conferencePaper","2021","Lin, Kuo-Yi; Liu, Yuguang; Li, Li; Dou, Runliang","A Review of Explainable Artificial Intelligence","Advances in Production Management Systems. Artificial Intelligence for Sustainable and Resilient Production Systems","978-3-030-85910-7","","10.1007/978-3-030-85910-7_61","","Artificial intelligence developed rapidly, while people are increasingly concerned about internal structure in machine learning models. Starting from the definition of interpretability and historical process of interpretability model, this paper summarizes and analyzes the existing interpretability methods according to the two dimensions of model type and model time based on the objectives of interpretability model and different categories. With the help of the existing interpretable methods, this paper summarizes and analyzes its application value to the society analyzes the reasons why its application is hindered. This paper concretely analyzes and summarizes the applications in industrial fields, including model debugging, feature engineering and data collection. This paper aims to summarizes the shortcomings of the existing interpretability model, and proposes some suggestions based on them. Starting from the nature of interpretability model, this paper analyzes and summarizes the disadvantages of the existing model evaluation index, and puts forward the quantitative evaluation index of the model from the definition of interpretability. Finally, this paper summarizes the above and looks forward to the development direction of interpretability models.","2021","2023-06-30 05:14:29","2023-06-30 05:14:32","","574-584","","","","","","","IFIP Advances in Information and Communication Technology","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","0 citations (Crossref) [2023-06-30]","","C:\Users\ambreen.hanif\Zotero\storage\VMPHWFQV\Lin et al_2021_A Review of Explainable Artificial Intelligence.pdf","","","","Dolgui, Alexandre; Bernard, Alain; Lemoine, David; von Cieminski, Gregor; Romero, David","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QA4UJYYQ","preprint","2020","Fort, Stanislav; Hu, Huiyi; Lakshminarayanan, Balaji","Deep Ensembles: A Loss Landscape Perspective","","","","10.48550/arXiv.1912.02757","http://arxiv.org/abs/1912.02757","Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis.","2020-06-24","2023-06-30 04:46:10","2023-06-30 04:46:10","2023-06-30 04:46:10","","","","","","","Deep Ensembles","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1912.02757 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\26NKSADR\1912.html; C:\Users\ambreen.hanif\Zotero\storage\5X6S5Q2V\Fort et al_2020_Deep Ensembles.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1912.02757","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MKC9QMKU","preprint","2016","Gal, Yarin; Ghahramani, Zoubin","Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning","","","","","http://arxiv.org/abs/1506.02142","Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.","2016-10-04","2023-06-30 04:34:49","2023-06-30 04:34:49","2023-06-30 04:34:49","","","","","","","Dropout as a Bayesian Approximation","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1506.02142 [cs, stat] version: 6","","C:\Users\ambreen.hanif\Zotero\storage\AFGEAACN\1506.html; C:\Users\ambreen.hanif\Zotero\storage\NLCQCXXJ\Gal_Ghahramani_2016_Dropout as a Bayesian Approximation.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1506.02142","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RL39567M","webpage","","","Papers with Code - Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning","","","","","https://paperswithcode.com/paper/dropout-as-a-bayesian-approximation","Implemented in 23 code libraries.","","2023-06-30 04:30:05","2023-06-30 04:30:05","2023-06-30 04:30:05","","","","","","","Papers with Code - Dropout as a Bayesian Approximation","","","","","","","en","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\5JQTNHU6\dropout-as-a-bayesian-approximation.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JY8DCLKL","journalArticle","2019","Amir, Ofra; Doshi-Velez, Finale; Sarne, David","Summarizing agent strategies","Autonomous Agents and Multi-Agent Systems","","1387-2532, 1573-7454","10.1007/s10458-019-09418-w","http://link.springer.com/10.1007/s10458-019-09418-w","Intelligent agents and AI-based systems are becoming increasingly prevalent. They support people in different ways, such as providing users with advice, working with them to achieve goals or acting on users’ behalf. One key capability missing in such systems is the ability to present their users with an effective summary of their strategy and expected behaviors under different conditions and scenarios. This capability, which we see as complementary to those currently under development in the context of “interpretable machine learning” and “explainable AI”, is critical in various settings. In particular, it is likely to play a key role when a user needs to collaborate with an agent, when having to choose between different available agents to act on her behalf, or when requested to determine the level of autonomy to be granted to an agent or approve its strategy. In this paper, we pose the challenge of developing capabilities for strategy summarization, which is not addressed by current theories and methods in the ﬁeld. We propose a conceptual framework for strategy summarization, which we envision as a collaborative process that involves both agents and people. Last, we suggest possible testbeds that could be used to evaluate progress in research on strategy summarization.","2019-09","2023-06-29 06:07:11","2023-06-29 06:07:19","2023-06-29 06:07:11","628-644","","5","33","","Auton Agent Multi-Agent Syst","","","","","","","","en","","","","","DOI.org (Crossref)","","13 citations (Crossref) [2023-06-29]","","C:\Users\ambreen.hanif\Zotero\storage\W82WSKAW\Amir et al. - 2019 - Summarizing agent strategies.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BFR3SYK6","webpage","2023","","ChatGPT poses an existential threat, and the window for gaining control over it is small","The Irish Times","","","","https://www.irishtimes.com/opinion/2023/04/19/chat-gpt-poses-an-existential-threat-and-the-window-for-gaining-control-over-it-is-small/","How is an AI, with no notion of right or wrong, expected to make moral decisions without some explicit guidance? It can't","2023","2023-06-28 02:06:15","2023-06-29 06:02:18","2023-06-28 02:06:15","","","","","","","","","","","","","","en","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\IV9M9QVD\chat-gpt-poses-an-existential-threat-and-the-window-for-gaining-control-over-it-is-small.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AZIRREDD","webpage","","","(3) (PDF) Evaluating Post-hoc Interpretability with Intrinsic Interpretability","","","","","https://www.researchgate.net/publication/370524188_Evaluating_Post-hoc_Interpretability_with_Intrinsic_Interpretability","","","2023-06-29 04:19:43","2023-06-29 04:19:43","2023-06-29 04:19:43","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\J6NKE3HM\370524188_Evaluating_Post-hoc_Interpretability_with_Intrinsic_Interpretability.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4EKI2WUJ","preprint","2017","Bau, David; Zhou, Bolei; Khosla, Aditya; Oliva, Aude; Torralba, Antonio","Network Dissection: Quantifying Interpretability of Deep Visual Representations","","","","10.48550/arXiv.1704.05796","http://arxiv.org/abs/1704.05796","We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.","2017-04-19","2023-06-29 02:57:23","2023-06-29 02:57:34","2023-06-29 02:57:23","","","","","","","Network Dissection","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1704.05796 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\5RY53NCF\1704.html; C:\Users\ambreen.hanif\Zotero\storage\M5XYNZXJ\Bau et al_2017_Network Dissection.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1704.05796","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KA97V6RA","conferencePaper","2020","Thomson, Robert; Schoenherr, Jordan Richard","Knowledge-to-Information Translation Training (KITT): An Adaptive Approach to Explainable Artificial Intelligence","Adaptive Instructional Systems","978-3-030-50788-6","","10.1007/978-3-030-50788-6_14","","Modern black-box artificial intelligence algorithms are computationally powerful yet fallible in unpredictable ways. While much research has gone into developing techniques to interpret these algorithms, less have also integrated the requirement to understand the algorithm as a function of their training data. In addition, few have examined the human requirements for explainability, so these interpretations provide the right quantity and quality of information to each user. We argue that Explainable Artificial Intelligence (XAI) frameworks need to account the expertise and goals of the user in order to gain widespread adoptance. We describe the Knowledge-to-Information Translation Training (KITT) framework, an approach to XAI that considers a number of possible explanatory models that can be used to facilitate users’ understanding of artificial intelligence. Following a review of algorithms, we provide a taxonomy of explanation types and outline how adaptive instructional systems can facilitate knowledge translation between developers and users. Finally, we describe limitations of our approach and paths for future research opportunities.","2020","2023-06-29 01:08:56","2023-06-29 01:09:03","","187-204","","","","","","Knowledge-to-Information Translation Training (KITT)","Lecture Notes in Computer Science","","","","Springer International Publishing","Cham","en","","","","","Springer Link","","5 citations (Crossref) [2023-06-29]","","C:\Users\ambreen.hanif\Zotero\storage\CP8QG25A\Thomson_Schoenherr_2020_Knowledge-to-Information Translation Training (KITT).pdf","","","","Sottilare, Robert A.; Schwarz, Jessica","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZI4ZZY9E","journalArticle","2023","Haque, Bahalul AKM","NOTION OF EXPLAINABLE ARTIFICIAL INTELLIGENCE - AN EMPIRICAL INVESTIGATION FROM A USER'S PERSPECTIVE","ECIS 2023 Research Papers","","","","https://aisel.aisnet.org/ecis2023_rp/404","The growing attention to artificial intelligence-based applications has led to research interest in explainability issues. This emerging research attention on explainable AI (XAI) advocates the need to investigate end user-centric explainable AI. Thus, this study aims to investigate user-centric explainable AI and considered recommendation systems as the study context. We conducted focus group interviews to collect qualitative data on the recommendation system. We asked participants about the end users' comprehension of a recommended item, its probable explanation, and their opinion of making a recommendation explainable. Our findings reveal that end users want a non-technical and tailor-made explanation with on-demand supplementary information. Moreover, we also observed users requiring an explanation about personal data usage, detailed user feedback, and authentic and reliable explanations. Finally, we propose a synthesized framework that aims at involving the end user in the development process for requirements collection and validation.","2023-05-11","2023-06-19 18:30:52","2023-06-28 04:45:02","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\ZGAKIDID\Haque - 2023 - NOTION OF EXPLAINABLE ARTIFICIAL INTELLIGENCE - AN.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NMRAAMAQ","preprint","2017","Zhang, Quanshi; Cao, Ruiming; Zhang, Shengming; Redmonds, Mark; Wu, Ying Nian; Zhu, Song-Chun","Interactively Transferring CNN Patterns for Part Localization","","","","10.48550/arXiv.1708.01783","http://arxiv.org/abs/1708.01783","In the scenario of one/multi-shot learning, conventional end-to-end learning strategies without sufficient supervision are usually not powerful enough to learn correct patterns from noisy signals. Thus, given a CNN pre-trained for object classification, this paper proposes a method that first summarizes the knowledge hidden inside the CNN into a dictionary of latent activation patterns, and then builds a new model for part localization by manually assembling latent patterns related to the target part via human interactions. We use very few (e.g., three) annotations of a semantic object part to retrieve certain latent patterns from conv-layers to represent the target part. We then visualize these latent patterns and ask users to further remove incorrect patterns, in order to refine part representation. With the guidance of human interactions, our method exhibited superior performance of part localization in experiments.","2017-11-21","2023-06-27 06:59:24","2023-06-27 06:59:24","2023-06-27 06:59:24","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1708.01783 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\IJEFKWSA\1708.html; C:\Users\ambreen.hanif\Zotero\storage\CQZLIAWM\Zhang et al_2017_Interactively Transferring CNN Patterns for Part Localization.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1708.01783","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CVAEH5I9","webpage","","","[1708.01783] Interactively Transferring CNN Patterns for Part Localization","","","","","https://arxiv.org/abs/1708.01783","","","2023-06-27 06:58:37","2023-06-27 06:58:37","2023-06-27 06:58:37","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\NK8UBYJT\1708.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZXJKYKPU","journalArticle","2022","Li, Xiao-Hui; Cao, Caleb Chen; Shi, Yuhan; Bai, Wei; Gao, Han; Qiu, Luyu; Wang, Cong; Gao, Yuanyuan; Zhang, Shenjia; Xue, Xun; Chen, Lei","A Survey of Data-Driven and Knowledge-Aware eXplainable AI","IEEE Transactions on Knowledge and Data Engineering","","1558-2191","10.1109/TKDE.2020.2983930","","We are witnessing a fast development of Artificial Intelligence (AI), but it becomes dramatically challenging to explain AI models in the past decade. “Explanation” has a flexible philosophical concept of “satisfying the subjective curiosity for causal information”, driving a wide spectrum of methods being invented and/or adapted from many aspects and communities, including machine learning, visual analytics, human-computer interaction and so on. Nevertheless, from the view-point of data and knowledge engineering (DKE), a best explaining practice that is cost-effective in terms of extra intelligence acquisition should exploit the causal information and explaining scenarios which is hidden richly in the data itself. In the past several years, there are plenty of works contributing in this line but there is a lack of a clear taxonomy and systematic review of the current effort. To this end, we propose this survey, reviewing and taxonomizing existing efforts from the view-point of DKE, summarizing their contribution, technical essence and comparative characteristics. Specifically, we categorize methods into data-driven methods where explanation comes from the task-related data, and knowledge-aware methods where extraneous knowledge is incorporated. Furthermore, in the light of practice, we provide survey of state-of-art evaluation metrics and deployed explanation applications in industrial practice.","2022-01","2023-06-27 06:37:13","2023-06-27 06:37:19","","29-49","","1","34","","","","","","","","","","","","","","","IEEE Xplore","","49 citations (Crossref) [2023-06-27] Conference Name: IEEE Transactions on Knowledge and Data Engineering","","C:\Users\ambreen.hanif\Zotero\storage\C3B9GR84\9050829.html; C:\Users\ambreen.hanif\Zotero\storage\TXHNYUH7\Li et al_2022_A Survey of Data-Driven and Knowledge-Aware eXplainable AI.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PE8MI4D7","conferencePaper","2003","Fe-Fei, Li; Fergus; Perona","A Bayesian approach to unsupervised one-shot learning of object categories","Proceedings Ninth IEEE International Conference on Computer Vision","","","10.1109/ICCV.2003.1238476","","Learning visual models of object categories notoriously requires thousands of training examples; this is due to the diversity and richness of object appearance which requires models containing hundreds of parameters. We present a method for learning object categories from just a few images (1 /spl sim/ 5). It is based on incorporating ""generic"" knowledge which may be obtained from previously learnt models of unrelated categories. We operate in a variational Bayesian framework: object categories are represented by probabilistic models, and ""prior"" knowledge is represented as a probability density function on the parameters of these models. The ""posterior"" model for an object category is obtained by updating the prior in the light of one or more observations. Our ideas are demonstrated on four diverse categories (human faces, airplanes, motorcycles, spotted cats). Initially three categories are learnt from hundreds of training examples, and a ""prior"" is estimated from these. Then the model of the fourth category is learnt from 1 to 5 training examples, and is used for detecting new exemplars a set of test images.","2003-10","2023-06-27 06:37:06","2023-06-27 06:37:09","","1134-1141 vol.2","","","","","","","","","","","","","","","","","","IEEE Xplore","","132 citations (Crossref) [2023-06-27]","","C:\Users\ambreen.hanif\Zotero\storage\T7J7EIW8\Fe-Fei et al_2003_A Bayesian approach to unsupervised one-shot learning of object categories.pdf; C:\Users\ambreen.hanif\Zotero\storage\JN8Y8Q6I\1238476.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings Ninth IEEE International Conference on Computer Vision","","","","","","","","","","","","","","",""
"VCABTW7G","journalArticle","2021","Gaur, Manas; Faldu, Keyur; Sheth, Amit","Semantics of the Black-Box: Can Knowledge Graphs Help Make Deep Learning Systems More Interpretable and Explainable?","IEEE Internet Computing","","1941-0131","10.1109/MIC.2020.3031769","","The recent series of innovations in deep learning (DL) have shown enormous potential to impact individuals and society, both positively and negatively. DL models utilizing massive computing power and enormous datasets have significantly outperformed prior historical benchmarks on increasingly difficult, well-defined research tasks across technology domains such as computer vision, natural language processing, and human-computer interactions. However, DL's black-box nature and over-reliance on massive amounts of data condensed into labels and dense representations pose challenges for interpretability and explainability. Furthermore, DLs have not proven their ability to effectively utilize relevant domain knowledge critical to human understanding. This aspect was missing in early data-focused approaches and necessitated knowledge-infused learning (K-iL) to incorporate computational knowledge. This article demonstrates how knowledge, provided as a knowledge graph, is incorporated into DL using K-iL. Through examples from natural language processing applications in healthcare and education, we discuss the utility of K-iL towards interpretability and explainability.","2021-01","2023-06-27 06:36:58","2023-06-27 06:37:02","","51-59","","1","25","","","Semantics of the Black-Box","","","","","","","","","","","","IEEE Xplore","","44 citations (Crossref) [2023-06-27] Conference Name: IEEE Internet Computing","","C:\Users\ambreen.hanif\Zotero\storage\9WURNK8V\Gaur et al_2021_Semantics of the Black-Box.pdf; C:\Users\ambreen.hanif\Zotero\storage\8GSZQQB2\9357868.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GCXC2BL6","webpage","","","A Bayesian approach to unsupervised one-shot learning of object categories | IEEE Conference Publication | IEEE Xplore","","","","","https://ieeexplore-ieee-org.simsrad.net.ocs.mq.edu.au/document/1238476","","","2023-06-27 06:30:45","2023-06-27 06:30:45","2023-06-27 06:30:45","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\YRZM4S3G\1238476.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6TNQNUSW","webpage","","","Semantics of the Black-Box: Can Knowledge Graphs Help Make Deep Learning Systems More Interpretable and Explainable? | IEEE Journals & Magazine | IEEE Xplore","","","","","https://ieeexplore-ieee-org.simsrad.net.ocs.mq.edu.au/document/9357868","","","2023-06-27 06:30:37","2023-06-27 06:30:37","2023-06-27 06:30:37","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\UZKUTAXM\9357868.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W9TXWUM6","conferencePaper","2018","Chen, Jiaoyan; Lecue, Freddy; Pan, Jeff Z; Horrocks, Ian; Chen, Huajun","Knowledge-Based Transfer Learning Explanation","Sixteenth International Conference on Principles of Knowledge Representation and Reasoning","","","","","Machine learning explanation can signiﬁcantly boost machine learning’s application, but the usability of current methods is limited in human-centric explanation, especially for transfer learning, an important machine learning branch that aims at utilizing knowledge from one learning domain (i.e., a pair of dataset and prediction task) to enhance prediction model training in another learning domain. In this paper, we propose an ontology-based approach for human-centric explanation of transfer learning. Three kinds of knowledgebased explanatory evidence, with different granularities, including general factors, particular narrators and core contexts are ﬁrst proposed and then inferred with both local ontologies and external knowledge bases. The evaluation with US ﬂight data and DBpedia has presented their conﬁdence and availability in explaining the transferability of feature representation in ﬂight departure delay forecasting.","2018","2023-06-27 04:04:50","2023-06-27 04:13:07","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\HG4EDVS5\Chen et al. - Knowledge-Based Transfer Learning Explanation.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"M7EW969X","preprint","2023","Kuhl, Ulrike; Artelt, André; Hammer, Barbara","For Better or Worse: The Impact of Counterfactual Explanations' Directionality on User Behavior in xAI","","","","","http://arxiv.org/abs/2306.07637","Counterfactual explanations (CFEs) are a popular approach in explainable artificial intelligence (xAI), highlighting changes to input data necessary for altering a model's output. A CFE can either describe a scenario that is better than the factual state (upward CFE), or a scenario that is worse than the factual state (downward CFE). However, potential benefits and drawbacks of the directionality of CFEs for user behavior in xAI remain unclear. The current user study (N=161) compares the impact of CFE directionality on behavior and experience of participants tasked to extract new knowledge from an automated system based on model predictions and CFEs. Results suggest that upward CFEs provide a significant performance advantage over other forms of counterfactual feedback. Moreover, the study highlights potential benefits of mixed CFEs improving user performance compared to downward CFEs or no explanations. In line with the performance results, users' explicit knowledge of the system is statistically higher after receiving upward CFEs compared to downward comparisons. These findings imply that the alignment between explanation and task at hand, the so-called regulatory fit, may play a crucial role in determining the effectiveness of model explanations, informing future research directions in xAI. To ensure reproducible research, the entire code, underlying models and user data of this study is openly available: https://github.com/ukuhl/DirectionalAlienZoo","2023-06-13","2023-06-27 04:00:00","2023-06-27 04:00:00","2023-06-27 03:59:55","","","","","","","For Better or Worse","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2306.07637 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\S5KGNW98\2306.html; C:\Users\ambreen.hanif\Zotero\storage\2TFVMD3Y\Kuhl et al_2023_For Better or Worse.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2306.07637","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B46NHXYV","preprint","2023","Rodis, Nikolaos; Sardianos, Christos; Papadopoulos, Georgios Th; Radoglou-Grammatikis, Panagiotis; Sarigiannidis, Panagiotis; Varlamis, Iraklis","Multimodal Explainable Artificial Intelligence: A Comprehensive Review of Methodological Advances and Future Research Directions","","","","","http://arxiv.org/abs/2306.05731","The current study focuses on systematically analyzing the recent advances in the field of Multimodal eXplainable Artificial Intelligence (MXAI). In particular, the relevant primary prediction tasks and publicly available datasets are initially described. Subsequently, a structured presentation of the MXAI methods of the literature is provided, taking into account the following criteria: a) The number of the involved modalities, b) The stage at which explanations are produced, and c) The type of the adopted methodology (i.e. mathematical formalism). Then, the metrics used for MXAI evaluation are discussed. Finally, a comprehensive analysis of current challenges and future research directions is provided.","2023-06-09","2023-06-27 03:59:38","2023-06-27 03:59:38","2023-06-27 03:59:38","","","","","","","Multimodal Explainable Artificial Intelligence","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2306.05731 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\PT5QWYLE\2306.html; C:\Users\ambreen.hanif\Zotero\storage\ENSA8474\Rodis et al_2023_Multimodal Explainable Artificial Intelligence.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2306.05731","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9N6MM3FF","preprint","2015","Kim, Been; Rudin, Cynthia; Shah, Julie","The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification","","","","10.48550/arXiv.1503.01161","http://arxiv.org/abs/1503.01161","We present the Bayesian Case Model (BCM), a general framework for Bayesian case-based reasoning (CBR) and prototype classification and clustering. BCM brings the intuitive power of CBR to a Bayesian generative framework. The BCM learns prototypes, the ""quintessential"" observations that best represent clusters in a dataset, by performing joint inference on cluster labels, prototypes and important features. Simultaneously, BCM pursues sparsity by learning subspaces, the sets of features that play important roles in the characterization of the prototypes. The prototype and subspace representation provides quantitative benefits in interpretability while preserving classification accuracy. Human subject experiments verify statistically significant improvements to participants' understanding when using explanations produced by BCM, compared to those given by prior art.","2015-03-03","2023-06-27 03:40:52","2023-06-27 03:40:52","2023-06-27 03:40:52","","","","","","","The Bayesian Case Model","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1503.01161 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\77LGMAXN\1503.html; C:\Users\ambreen.hanif\Zotero\storage\NMAXMTG9\Kim et al_2015_The Bayesian Case Model.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1503.01161","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T2U9V9ME","preprint","2020","Abnar, Samira; Zuidema, Willem","Quantifying Attention Flow in Transformers","","","","10.48550/arXiv.2005.00928","http://arxiv.org/abs/2005.00928","In the Transformer model, ""self-attention"" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.","2020-05-31","2023-06-27 01:50:10","2023-06-27 01:50:10","2023-06-27 01:50:04","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2005.00928 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\9MN47WIW\Abnar_Zuidema_2020_Quantifying Attention Flow in Transformers.pdf; C:\Users\ambreen.hanif\Zotero\storage\FLS5ZEEC\2005.html","","","","","","","","","","","","","","","","","","","","","","arXiv:2005.00928","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XCGPGXDG","preprint","2021","Chefer, Hila; Gur, Shir; Wolf, Lior","Transformer Interpretability Beyond Attention Visualization","","","","10.48550/arXiv.2012.09838","http://arxiv.org/abs/2012.09838","Self-attention techniques, and specifically Transformers, are dominating the field of text processing and are becoming increasingly popular in computer vision classification tasks. In order to visualize the parts of the image that led to a certain classification, existing methods either rely on the obtained attention maps or employ heuristic propagation along the attention graph. In this work, we propose a novel way to compute relevancy for Transformer networks. The method assigns local relevance based on the Deep Taylor Decomposition principle and then propagates these relevancy scores through the layers. This propagation involves attention layers and skip connections, which challenge existing methods. Our solution is based on a specific formulation that is shown to maintain the total relevancy across layers. We benchmark our method on very recent visual Transformer networks, as well as on a text classification problem, and demonstrate a clear advantage over the existing explainability methods.","2021-04-05","2023-06-27 01:50:02","2023-06-27 01:50:02","2023-06-27 01:49:55","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2012.09838 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\VVTN5CWT\2012.html; C:\Users\ambreen.hanif\Zotero\storage\22DP4E2Y\Chefer et al_2021_Transformer Interpretability Beyond Attention Visualization.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2012.09838","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8CXH98DU","preprint","2021","Wu, Zhengxuan; Ong, Desmond C.","On Explaining Your Explanations of BERT: An Empirical Study with Sequence Classification","","","","10.48550/arXiv.2101.00196","http://arxiv.org/abs/2101.00196","BERT, as one of the pretrianed language models, attracts the most attention in recent years for creating new benchmarks across GLUE tasks via fine-tuning. One pressing issue is to open up the blackbox and explain the decision makings of BERT. A number of attribution techniques have been proposed to explain BERT models, but are often limited to sequence to sequence tasks. In this paper, we adapt existing attribution methods on explaining decision makings of BERT in sequence classification tasks. We conduct extensive analyses of four existing attribution methods by applying them to four different datasets in sentiment analysis. We compare the reliability and robustness of each method via various ablation studies. Furthermore, we test whether attribution methods explain generalized semantics across semantically similar tasks. Our work provides solid guidance for using attribution methods to explain decision makings of BERT for downstream classification tasks.","2021-01-01","2023-06-27 01:47:37","2023-06-27 01:47:44","2023-06-27 01:47:34","","","","","","","On Explaining Your Explanations of BERT","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2101.00196 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\Y5YLX65H\2101.html; C:\Users\ambreen.hanif\Zotero\storage\V4QA2UWW\Wu_Ong_2021_On Explaining Your Explanations of BERT.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2101.00196","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L5SNZ7AF","conferencePaper","2019","Voita, Elena; Talbot, David; Moiseev, Fedor; Sennrich, Rico; Titov, Ivan","Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned","Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics","","","10.18653/v1/P19-1580","https://aclanthology.org/P19-1580","Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.","2019-07","2023-06-27 01:47:31","2023-06-27 01:47:39","2023-06-27 01:47:19","5797–5808","","","","","","Analyzing Multi-Head Self-Attention","","","","","Association for Computational Linguistics","Florence, Italy","","","","","","ACLWeb","","147 citations (Crossref) [2023-06-27]","","C:\Users\ambreen.hanif\Zotero\storage\H4FHY2EB\Voita et al_2019_Analyzing Multi-Head Self-Attention.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","ACL 2019","","","","","","","","","","","","","","",""
"R4U5JZ3S","webpage","","","Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned - ACL Anthology","","","","","https://aclanthology.org/P19-1580/","","","2023-06-27 01:47:11","2023-06-27 01:47:11","2023-06-27 01:47:11","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\HP44WZ7D\P19-1580.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QZIQBLNS","journalArticle","2014","Simonyan, Karen; Simonyan, Karen; Vedaldi, Andrea; Zisserman, Andrew","Deep inside convolutional networks: Visualising image classification models and saliency maps","in workshop at international conference on learning representations","","","","http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.746.3713","","2014","2021-10-28 00:55:29","2023-06-26 23:16:24","2021-10-28","","","","","","","","","","","","","","","","","","","","","","","","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RJPEWY4Y","preprint","2023","OpenAI","GPT-4 Technical Report","","","","","http://arxiv.org/abs/2303.08774","We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.","2023-03-27","2023-06-26 03:50:27","2023-06-26 03:50:27","2023-06-26 03:50:21","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2303.08774 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\JL56GDNR\2303.html; C:\Users\ambreen.hanif\Zotero\storage\3RR5RI4L\OpenAI_2023_GPT-4 Technical Report.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2303.08774","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3892RVVP","journalArticle","2023","Vellido, A.; Lisboa, P.J.G.; Fernández-Domenech, R.; Saralajew, S.; Villmann, T.","The coming of age of interpretable and explainable machine learning models","Neurocomputing","","","10.1016/j.neucom.2023.02.040","https://discovery.researcher.life/article/the-coming-of-age-of-interpretable-and-explainable-machine-learning-models/024751cafd3d34dcac03b5a738aadf9f","Machine-learning-based systems are now part of a wide array of real-world applications seamlessly embedded in the social realm. In the wake of this realization, strict legal regulations for these systems are currently being developed, addressing some of the risks they may pose. This is the coming of age of the concepts of interpretability and explainability in machine-learning-based data analysis, which can no longer be seen just as an academic research problem. In this paper, we discuss explainable and interpretable machine learning as post-hoc and ante-hoc strategies to address regulatory restrictions and highlight several aspects related to them, including their evaluation and assessment and the legal boundaries of application.","2023-05-01","2023-06-20 21:05:23","2023-06-21 01:35:13","2023-06-21 02:35:22","","","","535","","","","","","","","","","en","","","","","discovery.researcher.life","","1 citations (Crossref) [2023-06-21]","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4AFDN78S","journalArticle","2020","","Interpretability and Explainability: A Machine Learning Zoo Mini-tour","arXiv: Learning","","","10.3929/ethz-b-000454597","https://discovery.researcher.life/article/interpretability-and-explainability-a-machine-learning-zoo-mini-tour/8aeda4dfb8353dbcaa761a8662693962","In this review, we examine the problem of designing interpretable and explainable machine learning models. Interpretability and explainability lie at the core of many machine learning and statistical applications in medicine, economics, law, and natural sciences. Although interpretability and explainability have escaped a clear universal definition, many techniques motivated by these properties have been developed over the recent 30 years with the focus currently shifting towards deep learning methods. In this review, we emphasise the divide between interpretability and explainability and illustrate these two different research directions with concrete examples of the state-of-the-art. The review is intended for a general machine learning audience with interest in exploring the problems of interpretation and explanation beyond logistic regression or random forest variable importance. This work is not an exhaustive literature survey, but rather a primer focusing selectively on certain lines of research which the authors found interesting or informative.","2020-12-03","2023-06-20 21:13:42","2023-06-20 21:13:42","2023-06-21 02:43:41","","","","","","","","","","","","","","en","","","","","discovery.researcher.life","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ADFSBU6K","webpage","","","O'Reilly Machine Learning for High-Risk Applications: Techniques for Responsible AI","Dataiku","","","","https://content.dataiku.com/oreilly-responsible-ai","Machine Learning for High-Risk Applications will arm practitioners with a solid understanding of model governance processes and a new way to use common Python tools for training interpretable models and debugging them for performance, safety, fairness, security and privacy issues.","","2023-06-19 19:19:10","2023-06-19 19:19:22","2023-06-19 19:19:10","","","","","","","O'Reilly Machine Learning for High-Risk Applications","","","","","","","en","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\X8DC8DYB\oreilly-responsible-ai.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GQ4HWXMR","preprint","2022","Ouyang, Long; Wu, Jeff; Jiang, Xu; Almeida, Diogo; Wainwright, Carroll L.; Mishkin, Pamela; Zhang, Chong; Agarwal, Sandhini; Slama, Katarina; Ray, Alex; Schulman, John; Hilton, Jacob; Kelton, Fraser; Miller, Luke; Simens, Maddie; Askell, Amanda; Welinder, Peter; Christiano, Paul; Leike, Jan; Lowe, Ryan","Training language models to follow instructions with human feedback","","","","10.48550/arXiv.2203.02155","http://arxiv.org/abs/2203.02155","Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.","2022-03-04","2023-06-19 19:09:37","2023-06-19 19:09:43","2023-06-19 19:09:37","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2203.02155 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\YRZ5CIKM\2203.html; C:\Users\ambreen.hanif\Zotero\storage\LI5Y3AH3\Ouyang et al_2022_Training language models to follow instructions with human feedback.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2203.02155","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8QFI6LAD","journalArticle","2022","Biswas, Neepa; Mondal, Anindita Sarkar; Kusumastuti, Ari; Saha, Swati; Mondal, Kartick Chandra","Automated credit assessment framework using ETL process and machine learning","Innovations in Systems and Software Engineering","","1614-5046","10.1007/s11334-022-00522-x","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9803598/","In the current business scenario, real-time analysis of enterprise data through Business Intelligence (BI) is crucial for supporting operational activities and taking any strategic decision. The automated ETL (extraction, transformation, and load) process ensures data ingestion into the data warehouse in near real-time, and insights are generated through the BI process based on real-time data. In this paper, we have concentrated on automated credit risk assessment in the financial domain based on the machine learning approach. The machine learning-based classification techniques can furnish a self-regulating process to categorize data. Establishing an automated credit decision-making system helps the lending institution to manage the risks, increase operational efficiency and comply with regulators. In this paper, an empirical approach is taken for credit risk assessment using logistic regression and neural network classification method in compliance with Basel II standards. Here, Basel II standards are adopted to calculate the expected loss. The required data integration for building machine learning models is done through an automated ETL process. We have concluded this research work by evaluating this new methodology for credit risk assessment.","2022-12-31","2023-06-19 19:09:15","2023-06-19 19:09:19","2023-06-19 19:09:15","1-14","","","","","Innov Syst Softw Eng","","","","","","","","","","","","","PubMed Central","","1 citations (Crossref) [2023-06-20] PMID: 36619240 PMCID: PMC9803598","","C:\Users\ambreen.hanif\Zotero\storage\Q9TNBR9D\Biswas et al_2022_Automated credit assessment framework using ETL process and machine learning.pdf; ","https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9803598/","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TK2QP2LZ","webpage","","","[2212.10560] Self-Instruct: Aligning Language Models with Self-Generated Instructions","","","","","https://arxiv.org/abs/2212.10560","","","2023-06-19 19:06:09","2023-06-19 19:06:09","2023-06-19 19:06:09","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\22Y84MIC\2212.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DFV7KRLW","webpage","","","Maithra Raghu | Does One Large Model Rule Them All?","","","","","https://maithraraghu.com/blog/2023/does-one-model-rule-them-all/","","","2023-06-19 19:05:43","2023-06-19 19:05:45","2023-06-19 19:05:43","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\35Z6W2LE\does-one-model-rule-them-all.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NNV8YUZD","webpage","","","PROV Model Primer","","","","","https://www.w3.org/TR/prov-primer/","","","2023-06-19 18:56:41","2023-06-19 18:56:41","2023-06-19 18:56:41","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\2FNXEAC6\prov-primer.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HCFKLL8N","journalArticle","2008","Weitzner, Daniel J.; Abelson, Harold; Berners-Lee, Tim; Feigenbaum, Joan; Hendler, James; Sussman, Gerald Jay","Information accountability","Communications of the ACM","","0001-0782, 1557-7317","10.1145/1349026.1349043","https://dl.acm.org/doi/10.1145/1349026.1349043","With access control and encryption no longer capable of protecting privacy, laws and systems are needed that hold people accountable for the misuse of personal information, whether public or secret.","2008-06","2023-06-19 18:56:19","2023-06-19 18:56:24","2023-06-19 18:56:19","82-87","","6","51","","Commun. ACM","","","","","","","","en","","","","","DOI.org (Crossref)","","226 citations (Crossref) [2023-06-20]","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PTS9AHP3","journalArticle","2019","Belinkov, Yonatan; Glass, James","Analysis Methods in Neural Language Processing: A Survey","Transactions of the Association for Computational Linguistics","","2307-387X","10.1162/tacl_a_00254","https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00254/43503/Analysis-Methods-in-Neural-Language-Processing-A","The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their featurerich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.","2019-04-01","2023-06-19 18:47:43","2023-06-19 18:47:45","2023-06-19 18:47:43","49-72","","","7","","","Analysis Methods in Neural Language Processing","","","","","","","en","","","","","DOI.org (Crossref)","","92 citations (Crossref) [2023-06-20]","","C:\Users\ambreen.hanif\Zotero\storage\5PV2RE7N\Belinkov and Glass - 2019 - Analysis Methods in Neural Language Processing A .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AJI6FAK2","webpage","","","Paper page - PaLM 2 Technical Report","","","","","https://huggingface.co/papers/2305.10403","","","2023-06-19 18:39:05","2023-06-19 18:39:30","2023-06-19 18:39:05","","","","","","","","","","","","","","","","","","","","","","","; C:\Users\ambreen.hanif\Zotero\storage\BFF6XTLW\2305.html","notion://www.notion.so/cite-noauthor_paper_nodate-1-7f5c2284d5b64eb6804f4e8a79226c41","notion","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PZX4WKX7","webpage","","","Paper page - PaLM 2 Technical Report","","","","","https://huggingface.co/papers/2305.10403","","","2023-06-19 18:39:01","2023-06-19 18:39:01","2023-06-19 18:39:01","","","","","","","","","","","","","","","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\MXQJZF83\2305.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JD83F7KH","preprint","2023","Beheshti, Amin; Yang, Jian; Sheng, Quan Z.; Benatallah, Boualem; Casati, Fabio; Dustdar, Schahram; Nezhad, Hamid Reza Motahari; Zhang, Xuyun; Xue, Shan","ProcessGPT: Transforming Business Process Management with Generative Artificial Intelligence","","","","10.48550/arXiv.2306.01771","http://arxiv.org/abs/2306.01771","Generative Pre-trained Transformer (GPT) is a state-of-the-art machine learning model capable of generating human-like text through natural language processing (NLP). GPT is trained on massive amounts of text data and uses deep learning techniques to learn patterns and relationships within the data, enabling it to generate coherent and contextually appropriate text. This position paper proposes using GPT technology to generate new process models when/if needed. We introduce ProcessGPT as a new technology that has the potential to enhance decision-making in data-centric and knowledge-intensive processes. ProcessGPT can be designed by training a generative pre-trained transformer model on a large dataset of business process data. This model can then be fine-tuned on specific process domains and trained to generate process flows and make decisions based on context and user input. The model can be integrated with NLP and machine learning techniques to provide insights and recommendations for process improvement. Furthermore, the model can automate repetitive tasks and improve process efficiency while enabling knowledge workers to communicate analysis findings, supporting evidence, and make decisions. ProcessGPT can revolutionize business process management (BPM) by offering a powerful tool for process augmentation, automation and improvement. Finally, we demonstrate how ProcessGPT can be a powerful tool for augmenting data engineers in maintaining data ecosystem processes within large bank organizations. Our scenario highlights the potential of this approach to improve efficiency, reduce costs, and enhance the quality of business operations through the automation of data-centric and knowledge-intensive processes. These results underscore the promise of ProcessGPT as a transformative technology for organizations looking to improve their process workflows.","2023-05-28","2023-06-19 18:27:17","2023-06-19 18:27:17","2023-06-19 18:27:17","","","","","","","ProcessGPT","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2306.01771 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\JGDFK2QX\2306.html; C:\Users\ambreen.hanif\Zotero\storage\QDAWF3CA\Beheshti et al_2023_ProcessGPT.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2306.01771","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y2SY6D6N","preprint","2022","Nagisetty, Vineel; Graves, Laura; Scott, Joseph; Ganesh, Vijay","xAI-GAN: Enhancing Generative Adversarial Networks via Explainable AI Systems","","","","10.48550/arXiv.2002.10438","http://arxiv.org/abs/2002.10438","Generative Adversarial Networks (GANs) are a revolutionary class of Deep Neural Networks (DNNs) that have been successfully used to generate realistic images, music, text, and other data. However, GAN training presents many challenges, notably it can be very resource-intensive. A potential weakness in GANs is that it requires a lot of data for successful training and data collection can be an expensive process. Typically, the corrective feedback from discriminator DNNs to generator DNNs (namely, the discriminator's assessment of the generated example) is calculated using only one real-numbered value (loss). By contrast, we propose a new class of GAN we refer to as xAI-GAN that leverages recent advances in explainable AI (xAI) systems to provide a ""richer"" form of corrective feedback from discriminators to generators. Specifically, we modify the gradient descent process using xAI systems that specify the reason as to why the discriminator made the classification it did, thus providing the ""richer"" corrective feedback that helps the generator to better fool the discriminator. Using our approach, we observe xAI-GANs provide an improvement of up to 23.18% in the quality of generated images on both MNIST and FMNIST datasets over standard GANs as measured by Frechet Inception Distance (FID). We further compare xAI-GAN trained on 20% of the data with standard GAN trained on 100% of data on the CIFAR10 dataset and find that xAI-GAN still shows an improvement in FID score. Further, we compare our work with Differentiable Augmentation - which has been shown to make GANs data-efficient - and show that xAI-GANs outperform GANs trained on Differentiable Augmentation. Moreover, both techniques can be combined to produce even better results. Finally, we argue that xAI-GAN enables users greater control over how models learn than standard GANs.","2022-03-29","2023-06-19 18:25:41","2023-06-19 18:25:41","2023-06-19 18:25:41","","","","","","","xAI-GAN","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2002.10438 [cs, stat]","","C:\Users\ambreen.hanif\Zotero\storage\QZAMCK5A\2002.html; C:\Users\ambreen.hanif\Zotero\storage\LESBX696\Nagisetty et al_2022_xAI-GAN.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:2002.10438","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DHQYPD5H","journalArticle","","Bryan-Kinns, Nick; Banar, Berker; Ford, Corey; Reed, Courtney N; Zhang, Yixiao; Colton, Simon; Armitage, Jack","Exploring XAI for the Arts: Explaining Latent Space in Generative Music","","","","","","Explainable AI has the potential to support more interactive and ﬂuid co-creative AI systems which can creatively collaborate with people. To do this, creative AI models need to be amenable to debugging by offering eXplainable AI (XAI) features which are inspectable, understandable, and modiﬁable. However, currently there is very little XAI for the arts. In this work, we demonstrate how a latent variable model for music generation can be made more explainable; speciﬁcally we extend MeasureVAE which generates measures of music. We increase the explainability of the model by: i) using latent space regularisation to force some speciﬁc dimensions of the latent space to map to meaningful musical attributes, ii) providing a user interface feedback loop to allow people to adjust dimensions of the latent space and observe the results of these changes in real-time, iii) providing a visualisation of the musical attributes in the latent space to help people understand and predict the effect of changes to latent space dimensions. We suggest that in doing so we bridge the gap between the latent space and the generated musical outcomes in a meaningful way which makes the model and its outputs more explainable and more debuggable.","","2023-06-19 18:21:50","2023-06-19 18:21:54","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\W9B7HRJ9\Bryan-Kinns et al. - Exploring XAI for the Arts Explaining Latent Spac.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"584SFQI4","blogPost","2019","Bieliauskas, Stefan","Getting started with Provenance and Neo4j","Neo4j Developer Blog","","","","https://medium.com/neo4j/getting-started-with-provenance-and-neo4j-b50f666d8656","We all want to know “Where does our meat come from?” or “Is this a reliable information or fake news?”. If we ask this kind of questions…","2019-02-12","2023-06-19 18:14:40","2023-06-19 18:14:40","2023-06-19 18:14:40","","","","","","","","","","","","","","en","","","","","","","","","C:\Users\ambreen.hanif\Zotero\storage\CE7ACMV5\getting-started-with-provenance-and-neo4j-b50f666d8656.html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8AKDI9PT","webpage","","","ProvLake - IBM","","","","","https://researcher.watson.ibm.com/researcher/researcher.watson.ibm.com/researcher/view_group_subpage.php","IBM Research","REPLACE","2023-06-19 18:14:19","2023-06-19 18:14:19","2023-06-19 18:14:19","","","","","","","","","","","","","","en-US","© Copyright IBM Corp. 2016","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QRU6FPJR","preprint","2019","Souza, Renan; Azevedo, Leonardo; Lourenço, Vítor; Soares, Elton; Thiago, Raphael; Brandão, Rafael; Civitarese, Daniel; Brazil, Emilio Vital; Moreno, Marcio; Valduriez, Patrick; Mattoso, Marta; Cerqueira, Renato; Netto, Marco A. S.","Provenance Data in the Machine Learning Lifecycle in Computational Science and Engineering","","","","","http://arxiv.org/abs/1910.04223","Machine Learning (ML) has become essential in several industries. In Computational Science and Engineering (CSE), the complexity of the ML lifecycle comes from the large variety of data, scientists' expertise, tools, and workflows. If data are not tracked properly during the lifecycle, it becomes unfeasible to recreate a ML model from scratch or to explain to stakeholders how it was created. The main limitation of provenance tracking solutions is that they cannot cope with provenance capture and integration of domain and ML data processed in the multiple workflows in the lifecycle while keeping the provenance capture overhead low. To handle this problem, in this paper we contribute with a detailed characterization of provenance data in the ML lifecycle in CSE; a new provenance data representation, called PROV-ML, built on top of W3C PROV and ML Schema; and extensions to a system that tracks provenance from multiple workflows to address the characteristics of ML and CSE, and to allow for provenance queries with a standard vocabulary. We show a practical use in a real case in the Oil and Gas industry, along with its evaluation using 48 GPUs in parallel.","2019-10-21","2023-06-19 18:13:33","2023-06-19 18:13:33","2023-06-19 18:13:33","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1910.04223 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\VQEY9V7W\1910.html; C:\Users\ambreen.hanif\Zotero\storage\7WDBIUJZ\Souza et al_2019_Provenance Data in the Machine Learning Lifecycle in Computational Science and.pdf","","","","","","","","","","","","","","","","","","","","","","arXiv:1910.04223","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FRB7G62X","conferencePaper","2023","Capel, Tara; Brereton, Margot","What is Human-Centered about Human-Centered AI? A Map of the Research Landscape","Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems","978-1-4503-9421-5","","10.1145/3544548.3580959","https://dl.acm.org/doi/10.1145/3544548.3580959","","2023-04-19","2023-06-19 08:30:53","2023-06-19 08:31:00","2023-06-19 08:30:53","1-23","","","","","","What is Human-Centered about Human-Centered AI?","","","","","ACM","Hamburg Germany","en","","","","","DOI.org (Crossref)","","0 citations (Crossref) [2023-06-19]","","C:\Users\ambreen.hanif\Zotero\storage\4WT92QEG\Capel_Brereton_2023_What is Human-Centered about Human-Centered AI.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","CHI '23: CHI Conference on Human Factors in Computing Systems","","","","","","","","","","","","","","",""
"JJYUB624","journalArticle","","Helgstrand, Carl Johan; Hultin, Niklas","Comparing Human Reasoning and Explainable AI","","","","","","","","2023-06-19 08:27:03","2023-06-19 08:27:05","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\INBD342U\Helgstrand and Hultin - Comparing Human Reasoning and Explainable AI.pdf","","","⛔ No DOI found","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RQMXVB83","book","2019","El-Assady, Mennatallah; Jentner, Wolfgang; Kehlbeck, Rebecca; Schlegel, Udo; Sevastjanova, Rita; Sperrle, Fabian; Spinner, Thilo; Keim, Daniel","Towards XAI: Structuring the Processes of Explanations","","","","","","Explainable Artificial Intelligence describes a process to reveal the logical propagation of operations that transform a given input to a certain output. In this paper, we investigate the design space of explanation processes based on factors gathered from six research areas, namely, Pedagogy, Story-telling, Argumentation, Programming, Trust-Building, and Gamification. We contribute a conceptual model describing the building blocks of explanation processes, including a comprehensive overview of explanation and verification phases, pathways, mediums, and strategies. We further argue for the importance of studying effective methods of explainable machine learning, and discuss open research challenges and opportunities. Figure 1: The proposed explanation process model. On the highest level, each explanation consists of different phases that structure the whole process into defined elements. Each phase contains explanation blocks, i.e., self-contained units to explain one phenomenon based on a selected strategy and medium. At the end of each explanation phase, an optional verification block ensures the understanding of the explained aspects. Lastly, to transition between phases and building blocks, different pathways are utilized.","2019-05-04","2023-06-19 08:26:09","2023-06-19 08:26:09","","","","","","","","Towards XAI","","","","","","","","","","","","ResearchGate","","","","C:\Users\ambreen.hanif\Zotero\storage\2M7XS2UR\El-Assady et al_2019_Towards XAI.pdf; ","https://www.researchgate.net/profile/Mennatallah-El-Assady/publication/332802468_Towards_XAI_Structuring_the_Processes_of_Explanations/links/5ccad56b92851c8d22146613/Towards-XAI-Structuring-the-Processes-of-Explanations.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JHGVPQKJ","book","2021","Liao, Vera; Varshney, Kush","Human-Centered Explainable AI (XAI): From Algorithms to User Experiences","","","","","","In recent years, the field of explainable AI (XAI) has produced a vast collection of algorithms, providing a useful toolbox for researchers and practitioners to build XAI applications. With the rich application opportunities, explainability is believed to have moved beyond a demand by data scientists or researchers to comprehend the models they develop, to an essential requirement for people to trust and adopt AI deployed in numerous domains. However, explainability is an inherently human-centric property and the field is starting to embrace human-centered approaches. Human-computer interaction (HCI) research and user experience (UX) design in this area are becoming increasingly important. In this chapter, we begin with a high-level overview of the technical landscape of XAI algorithms, then selectively survey our own and other recent HCI works that take human-centered approaches to design, evaluate, and provide conceptual and methodological tools for XAI. We ask the question ""what are human-centered approaches doing for XAI"" and highlight three roles that they play in shaping XAI technologies by helping navigate, assess and expand the XAI toolbox: to drive technical choices by users' explainability needs, to uncover pitfalls of existing XAI methods and inform new methods, and to provide conceptual frameworks for human-compatible XAI.","2021-10-20","2023-06-19 08:22:48","2023-06-19 08:22:59","","","","","","","","Human-Centered Explainable AI (XAI)","","","","","","","","","","","","ResearchGate","","","","C:\Users\ambreen.hanif\Zotero\storage\TJS9LF2C\Liao_Varshney_2021_Human-Centered Explainable AI (XAI).pdf; ; ","notion://www.notion.so/cite-liao_human-centered_2021-059464fa4caf4fb397cb55e674a05373; https://www.researchgate.net/publication/355495618_Human-Centered_Explainable_AI_XAI_From_Algorithms_to_User_Experiences","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9Q9TKEGJ","journalArticle","","Touvron, Hugo; Martin, Louis; Stone, Kevin","Llama 2: Open Foundation and Fine-Tuned Chat Models","","","","","","In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.","","2023-07-19 05:48:26","2023-07-19 05:48:26","","","","","","","","","","","","","","","en","","","","","Zotero","","","","C:\Users\ambreen.hanif\Zotero\storage\2YRT47YB\Touvron et al. - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6MBGM3P3","preprint","2022","Romero, Raphaël; Kang, Bo; De Bie, Tijl","Graph-Survival: A Survival Analysis Framework for Machine Learning on Temporal Networks","","","","","http://arxiv.org/abs/2203.07260","Continuous time temporal networks are attracting increasing attention due their omnipresence in real-world datasets and they manifold applications. While static network models have been successful in capturing static topological regularities, they often fail to model effects coming from the causal nature that explain the generation of networks. Exploiting the temporal aspect of networks has thus been the focus of various studies in the last decades. We propose a framework for designing generative models for continuous time temporal networks. Assuming a first order Markov assumption on the edge-specific temporal point processes enables us to flexibly apply survival analysis models directly on the waiting time between events, while using time-varying history-based features as covariates for these predictions. This approach links the well-documented field of temporal networks analysis through multivariate point processes, with methodological tools adapted from survival analysis. We propose a fitting method for models within this framework, and an algorithm for simulating new temporal networks having desired properties. We evaluate our method on a downstream future link prediction task, and provide a qualitative assessment of the network simulations.","2022-03-15","2023-07-20 04:47:30","2023-07-20 04:47:30","2023-07-20 04:47:30","","","","","","","Graph-Survival","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2203.07260 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\CFJ369LX\2203.html; C:\Users\ambreen.hanif\Zotero\storage\UR9Q5YRD\Romero et al. - 2022 - Graph-Survival A Survival Analysis Framework for .pdf","","","Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2203.07260","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9WHB4HN9","journalArticle","2009","Kostakos, Vassilis","Temporal graphs","Physica A: Statistical Mechanics and its Applications","","03784371","10.1016/j.physa.2008.11.021","https://linkinghub.elsevier.com/retrieve/pii/S0378437108009485","We introduce the idea of temporal graphs, a representation that encodes temporal data into graphs while fully retaining the temporal information of the original data. This representation lets us explore the dynamic temporal properties of data by using existing graph algorithms (such as shortest-path), with no need for data-driven simulations. We also present a number of metrics that can be used to study and explore temporal graphs. Finally, we use temporal graphs to analyse real-world data and present the results of our analysis.","2009-03","2023-07-20 04:47:44","2023-07-20 04:47:45","2023-07-20 04:47:44","1007-1023","","6","388","","Physica A: Statistical Mechanics and its Applications","","","","","","","","en","","","","","DOI.org (Crossref)","","","","C:\Users\ambreen.hanif\Zotero\storage\T6T2GU3C\Kostakos - 2009 - Temporal graphs.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D4YC2GCB","conferencePaper","2022","Brugman, Simon; Sostak, Tomas; Patil, Pradyot; Baak, Max","popmon: Analysis Package for Dataset Shift Detection","","","","10.25080/majora-212e5952-01d","https://conference.scipy.org/proceedings/scipy2022/popmon.html","","2022","2023-07-20 23:58:39","2023-07-20 23:58:39","2023-07-20 23:58:39","194-201","","","","","","popmon","","","","","","Austin, Texas","en","","","","","DOI.org (Crossref)","","","","C:\Users\ambreen.hanif\Zotero\storage\E9RL6LFY\popmon.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Python in Science Conference","","","","","","","","","","","","","","",""
"P46PGAP6","conferencePaper","2020","Yenicelik, David; Schmidt, Florian; Kilcher, Yannic","How does BERT capture semantics? A closer look at polysemous words","Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP","","","10.18653/v1/2020.blackboxnlp-1.15","https://aclanthology.org/2020.blackboxnlp-1.15","The recent paradigm shift to contextual word embeddings has seen tremendous success across a wide range of down-stream tasks. However, little is known on how the emergent relation of context and semantics manifests geometrically. We investigate polysemous words as one particularly prominent instance of semantic organization. Our rigorous quantitative analysis of linear separability and cluster organization in embedding vectors produced by BERT shows that semantics do not surface as isolated clusters but form seamless structures, tightly coupled with sentiment and syntax.","2020-11","2023-07-21 00:34:24","2023-07-21 00:34:24","2023-07-21 00:34:24","156–162","","","","","","How does BERT capture semantics?","","","","","Association for Computational Linguistics","Online","","","","","","ACLWeb","","","","C:\Users\ambreen.hanif\Zotero\storage\YTDQW98P\Yenicelik et al. - 2020 - How does BERT capture semantics A closer look at .pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","BlackboxNLP 2020","","","","","","","","","","","","","","",""
"TA8X6QHB","journalArticle","2023","Kolajo, Taiwo; Daramola, Olawande","Human-centric and semantics-based explainable event detection: a survey","Artificial Intelligence Review","","","10.1007/s10462-023-10525-0","https://discovery.researcher.life/article/human-centric-and-semantics-based-explainable-event-detection-a-survey/06470ab3de7038378efc932878873ae7","AbstractIn recent years, there has been a surge of interest in Artificial Intelligence (AI) systems that can provide human-centric explanations for decisions or predictions. No matter how good and efficient an AI model is, users or practitioners find it difficult to trust it if they cannot understand the AI model or its behaviours. Incorporating explainability that is human-centric in event detection systems is significant for building a decision-making process that is more trustworthy and sustainable. Human-centric and semantics-based explainable event detection will achieve trustworthiness, explainability, and reliability, which are currently lacking in AI systems. This paper provides a survey on human-centric explainable AI, explainable event detection, and semantics-based explainable event detection by answering some research questions that bother on the characteristics of human-centric explanations, the state of explainable AI, methods for human-centric explanations, the essence of human-centricity in explainable event detection, research efforts in explainable event solutions, and the benefits of integrating semantics into explainable event detection. The findings from the survey show the current state of human-centric explainability, the potential of integrating semantics into explainable AI, the open problems, and the future directions which can guide researchers in the explainable AI domain.","2023-06-22","2023-07-21 01:33:05","2023-07-21 01:33:05","2023-07-21 07:03:04","","","","","","","","","","","","","","en","","","","","discovery.researcher.life","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"V7U7FFMR","preprint","2020","Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario","Language Models are Few-Shot Learners","","","","10.48550/arXiv.2005.14165","http://arxiv.org/abs/2005.14165","Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.","2020-07-22","2023-07-21 02:34:14","2023-07-21 02:34:14","2023-07-21 02:34:14","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2005.14165 [cs]","","C:\Users\ambreen.hanif\Zotero\storage\SASN5H5Y\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf; C:\Users\ambreen.hanif\Zotero\storage\TG4DRRXU\2005.html","","","Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2005.14165","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YBZETUSH","preprint","2021","Matulionyte, Rita; Hanif, Ambreen","A call for more explainable AI in law enforcement","","","","10.2139/ssrn.3974243","https://papers.ssrn.com/abstract=3974243","The use of AI in law enforcement raises several significant ethical and legal concerns. One of them is AI explainability principle, which is mentioned in numerous national and international AI ethical guidelines. This paper firstly analyses what AI explainability principle could mean with relation toAI use in law enforcement, namely, to whom, why and how the explanation about the functioning of AI and its outcomes needs to be provided. Secondly, it explores some legal obstacles in ensuring the desired explainability of AI technologies, namely, the trade secret protection that often applies to AI modules and prevents access to proprietary elements of the algorithm. Finally, the paper outlines and discusses three ways to mitigate this conflict between the AI explainability principle and tradesecret protection. It encourages law enforcement authorities to be more proactive in ensuring that Face Recognition Technology (FRT) outputs are explainable to different stakeholder groups,especially those directly affected.","2021-11-30","2023-07-24 01:30:51","2023-07-24 01:30:51","2023-07-24 01:30:51","","","","","","","","","","","","","Rochester, NY","en","","SSRN Scholarly Paper","","","Social Science Research Network","","","","C:\Users\ambreen.hanif\Zotero\storage\J8KGKXUQ\Matulionyte and Hanif - 2021 - A call for more explainable AI in law enforcement.pdf","","","artificial intelligence; explainability; face recognition technology; law enforcement; machine learning; transparency","","","","","","","","","","","","","","","","","","","3974243","","","","","","","","","","","","","","","","","","","","","","","","","","",""